{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "# Assignment Questions\n",
        "# Theoretical Questions:"
      ],
      "metadata": {
        "id": "TWpMLvtvXjqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "    - Logistic Regression is a statistical method used for binary classification problems. It predicts the probability of an outcome belonging to a particular category, typically represented as 0 or 1.\n",
        "\n",
        "    - Differences from Linear Regression:\n",
        "    -1. Output Type:\n",
        "    - Linear Regression predicts continuous values.\n",
        "    - Logistic Regression predicts probabilities and classifies data into discrete categories (0 or 1).\n",
        "    -2. Function Used:\n",
        "    - Linear Regression uses a linear function: ( y = \\beta_0 + \\beta_1x )\n",
        "    - Logistic Regression applies a Sigmoid function to convert the output into a probability.\n",
        "    -3. Interpretation:\n",
        "    - Linear Regression finds the best fit line.\n",
        "    - Logistic Regression finds a decision boundary to separate classes.\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "    - The equation of Logistic Regression is:\n",
        "    - [ P(Y=1 | X) = \\frac{1}{1 + e^{- (\\beta_0 + \\beta_1X)}} ]\n",
        "\n",
        "    - Where:\n",
        "\n",
        "    - ( P(Y=1 | X) ) is the probability that output is 1.\n",
        "    - ( \\beta_0, \\beta_1 ) are the model coefficients.\n",
        "    - ( X ) is the input variable.\n",
        "    - The Sigmoid function is used to map the output between 0 and 1.\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "    - The Sigmoid function converts any real-valued number into a probability between 0 and 1:\n",
        "\n",
        "             [ sigmoid(z) = \\frac{1}{1 + e^{-z}} ]\n",
        "\n",
        "    - Reasons for using Sigmoid:\n",
        "    -1. Probability Interpretation: The output is always between 0 and 1,    making it suitable for classification.\n",
        "    -2. Non-linearity: Helps model complex relationships.\n",
        "    -3. Decision Boundary: Classifies data based on a probability threshold (e.g., 0.5).\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "    - The log loss function (Binary Cross-Entropy Loss) is used:\n",
        "\n",
        "           [ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right] ]\n",
        "\n",
        "Where:\n",
        "\n",
        "    - ( y_i ) is the actual label (0 or 1).\n",
        "    - ( h_\\theta(x_i) ) is the predicted probability.\n",
        "    - ( m ) is the number of observations.\n",
        "    - This function penalizes incorrect predictions, ensuring that the model minimizes error.\n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "    - Regularization is used to prevent overfitting by adding a penalty term to the cost function.\n",
        "\n",
        "    - Types of Regularization:\n",
        "    - 1. L1 Regularization (Lasso): Adds the absolute value of coefficients.\n",
        "    - 2. L2 Regularization (Ridge): Adds the squared value of coefficients.\n",
        "    - Why is it needed?\n",
        "    - Prevents the model from memorizing noise.\n",
        "    - Reduces model complexity.\n",
        "    - Improves generalization on unseen data.\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "   - Here’s a clear comparison of Lasso, Ridge, and Elastic Net regression — all are types of regularized linear models used to prevent overfitting by penalizing large coefficients.\n",
        "\n",
        "🔧 1. Ridge Regression (L2 Regularization)\n",
        "Penalty term added to cost function:\n",
        "\n",
        "Loss\n",
        "=\n",
        "MSE\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=MSE+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Penalizes the square of coefficients\n",
        "\n",
        "Shrinks coefficients, but does not set them to zero\n",
        "\n",
        "Keeps all features, just with smaller weights\n",
        "\n",
        "✅ Best for:\n",
        "\n",
        "Multicollinearity (correlated features)\n",
        "\n",
        "When you want to retain all features\n",
        "\n",
        "🔧 2. Lasso Regression (L1 Regularization)\n",
        "Penalty term added to cost function:\n",
        "\n",
        "Loss\n",
        "=\n",
        "MSE\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "Loss=MSE+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Penalizes the absolute value of coefficients\n",
        "\n",
        "Can shrink some coefficients exactly to zero\n",
        "\n",
        "Performs feature selection (removes irrelevant features)\n",
        "\n",
        "✅ Best for:\n",
        "\n",
        "Sparse models\n",
        "\n",
        "When you suspect many features are irrelevant\n",
        "\n",
        "🔧 3. Elastic Net Regression (L1 + L2 Regularization)\n",
        "Penalty term added to cost function:\n",
        "\n",
        "Loss\n",
        "=\n",
        "MSE\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=MSE+λ\n",
        "1\n",
        "​\n",
        " ∑∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        " ∑β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Combines benefits of Ridge and Lasso\n",
        "\n",
        "Keeps groups of correlated features together\n",
        "\n",
        "Controls balance with l1_ratio (e.g., 0.5 means equal weight)\n",
        "\n",
        "✅ Best for:\n",
        "\n",
        "High-dimensional data (more features than samples)\n",
        "\n",
        "When you want a balance between shrinkage and feature selection\n",
        "   - | Regularization | Penalty Term | Effect | |--------------|-------------|--------| | Lasso (L1) | ( \\lambda \\sum |\\beta_i| ) | Shrinks some coefficients to zero, useful for feature selection. | | Ridge (L2) | ( \\lambda \\sum \\beta_i^2 ) | Reduces coefficient values but doesn’t make them zero. | | Elastic Net | ( \\lambda_1 \\sum |\\beta_i| + \\lambda_2 \\sum \\beta_i^2 ) | A mix of L1 and L2 regularization. |\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "   - You should consider using Elastic Net when you want the benefits of both Lasso and Ridge, especially in the following situations:\n",
        "\n",
        "✅ Use Elastic Net When:\n",
        "1. You have many features, and some are correlated\n",
        "Lasso tends to select only one feature from a group of correlated features and ignore the rest.\n",
        "\n",
        "Elastic Net can keep groups of correlated features together by blending L1 (feature selection) and L2 (shrinkage).\n",
        "\n",
        "2. You have more features than observations (high-dimensional data)\n",
        "Lasso may struggle or behave unstably in this case.\n",
        "\n",
        "Elastic Net can stabilize the model with its L2 component.\n",
        "\n",
        "3. Lasso is too aggressive, and Ridge is too mild\n",
        "Lasso might zero out too many important features.\n",
        "\n",
        "Ridge might keep everything, even irrelevant features.\n",
        "\n",
        "Elastic Net offers a balanced trade-off via the l1_ratio.\n",
        "\n",
        "4. You want sparsity, but not total exclusion\n",
        "Elastic Net allows partial sparsity: it reduces many coefficients and zeros out some, depending on the data and regularization settings.\n",
        "\n",
        "5. Model selection is unstable with Lasso\n",
        "In cases of high multicollinearity, Elastic Net often produces more stable and interpretable results than Lasso.\n",
        "\n",
        "\n",
        "    - Elastic Net is preferred when:\n",
        "\n",
        "    -1. There are correlated features – Lasso may randomly select one and ignore others, while Elastic Net keeps them together.\n",
        "    -2. Lasso selects too few features – Elastic Net balances selection and shrinkage.\n",
        "\n",
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "    - The regularization parameter λ (lambda) in Logistic Regression has a significant impact on the model's complexity, performance, and generalization ability.\n",
        "\n",
        "In scikit-learn, it's represented as C = 1 / λ, so:\n",
        "\n",
        "Small C = strong regularization (large λ)\n",
        "\n",
        "Large C = weak regularization (small λ)\n",
        "\n",
        "✅ What Regularization Does:\n",
        "Regularization adds a penalty to the loss function to prevent overfitting by shrinking large coefficients.\n",
        "\n",
        "Logistic Regression Loss (with L2 regularization):\n",
        "Loss\n",
        "=\n",
        "Log Loss\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=Log Loss+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "🎯 Impact of λ on Logistic Regression:\n",
        "λ (or C = 1/λ)\tEffect on Coefficients\tModel Behavior\n",
        "Small λ (large C)\tLittle penalty on weights\tModel fits training data more closely; risk of overfitting\n",
        "Large λ (small C)\tHeavy penalty on weights (shrinkage)\tCoefficients become smaller or even zero; risk of underfitting\n",
        "Optimal λ\tBalanced penalty\tGood generalization to unseen data\n",
        "\n",
        "📘 Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Strong regularization\n",
        "model = LogisticRegression(C=0.01)\n",
        "\n",
        "# Weak regularization\n",
        "model = LogisticRegression(C=100)\n",
        "📊 Visual Intuition:\n",
        "High λ (low C): Suppresses weights → simpler model → may miss important patterns\n",
        "\n",
        "Low λ (high C): Allows large weights → complex model → may memorize noise\n",
        "\n",
        "✅ When to Tune λ:\n",
        "Always! Use cross-validation to find the best λ (or C) that balances bias and variance.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "🧠 Summary:\n",
        "λ controls regularization strength\n",
        "\n",
        "Too small → overfitting\n",
        "\n",
        "Too large → underfitting\n",
        "\n",
        "Tune it to find the sweet spot for best performance\n",
        "    - Higher λ → More regularization → Smaller coefficients → Simpler model.\n",
        "    - Lower λ → Less regularization → More complex model.\n",
        "    - Too high λ → Underfitting.\n",
        "    - Too low λ → Overfitting.\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "   - Logistic Regression is a powerful and commonly used classification algorithm, but like all models, it relies on certain assumptions to perform optimally.\n",
        "\n",
        "Here are the key assumptions of Logistic Regression:\n",
        "\n",
        "✅ 1. Binary or Multiclass Dependent Variable\n",
        "The outcome variable must be categorical:\n",
        "\n",
        "Binary (e.g., 0 or 1) for basic logistic regression.\n",
        "\n",
        "Multiclass for multinomial or softmax extensions.\n",
        "\n",
        "✅ 2. Linearity of Log-Odds\n",
        "Assumes a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "\n",
        "✅ 3. Independence of Observations\n",
        "Observations (rows) should be independent of each other.\n",
        "\n",
        "Violations (like time-series or clustered data) can bias estimates and require other methods (e.g., GEE, mixed models).\n",
        "\n",
        "✅ 4. Little to No Multicollinearity\n",
        "Independent variables should not be too highly correlated with each other.\n",
        "\n",
        "High multicollinearity:\n",
        "\n",
        "Inflates standard errors\n",
        "\n",
        "Makes coefficient interpretation unreliable\n",
        "\n",
        "Use Variance Inflation Factor (VIF) to detect and manage multicollinearity.\n",
        "\n",
        "✅ 5. Large Sample Size\n",
        "Logistic Regression requires a moderately large sample size for stable and reliable estimates.\n",
        "\n",
        "Especially important when:\n",
        "\n",
        "The number of predictors is large\n",
        "\n",
        "The data is imbalanced\n",
        "\n",
        "✅ 6. No Outliers or Influential Points (Ideally)\n",
        "Logistic Regression can be sensitive to extreme values, especially in smaller datasets.\n",
        "\n",
        "Use influence diagnostics like Cook’s Distance or Leverage plots.\n",
        "\n",
        "✅ 7. Independent Variables Can Be Continuous or Categorical\n",
        "No assumption about the distribution of predictors (e.g., normality is not required).\n",
        "\n",
        "However, dummy encoding is needed for categorical features.\n",
        "\n",
        "\n",
        "    1.  Independent observations – No multicollinearity among predictors.\n",
        "    2.  Linearity in log-odds – Relationship between features and log-odds is linear.\n",
        "    3.  No missing values – Data should be complete.\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "    - Here are several strong alternatives to Logistic Regression for classification tasks, each with their own strengths and best-use cases:\n",
        "\n",
        "✅ 1. Decision Trees\n",
        "How it works: Splits data by feature thresholds\n",
        "\n",
        "Pros: Easy to interpret, handles non-linear relationships\n",
        "\n",
        "Cons: Can overfit without pruning\n",
        "\n",
        "Best for: When interpretability and handling of mixed feature types are important\n",
        "\n",
        "✅ 2. Random Forest\n",
        "How it works: Ensemble of decision trees using bagging\n",
        "\n",
        "Pros: High accuracy, handles missing data, reduces overfitting\n",
        "\n",
        "Cons: Slower to train, less interpretable\n",
        "\n",
        "Best for: General-purpose classification tasks with complex data\n",
        "\n",
        "✅ 3. Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost)\n",
        "How it works: Builds trees sequentially to correct previous errors\n",
        "\n",
        "Pros: Excellent performance, handles missing values, robust to overfitting\n",
        "\n",
        "Cons: Sensitive to hyperparameters, more complex\n",
        "\n",
        "Best for: High-stakes predictive modeling (e.g., finance, competitions)\n",
        "\n",
        "✅ 4. Support Vector Machines (SVM)\n",
        "How it works: Finds the optimal hyperplane that separates classes\n",
        "\n",
        "Pros: Effective in high-dimensional spaces, robust to overfitting\n",
        "\n",
        "Cons: Slow on large datasets, hard to interpret, needs scaling\n",
        "\n",
        "Best for: Text classification, image recognition with small-to-medium datasets\n",
        "\n",
        "✅ 5. k-Nearest Neighbors (k-NN)\n",
        "How it works: Classifies based on majority class of nearest neighbors\n",
        "\n",
        "Pros: Simple, no training phase, works well for small datasets\n",
        "\n",
        "Cons: Slow prediction, sensitive to irrelevant features\n",
        "\n",
        "Best for: Problems with well-separated and structured data\n",
        "\n",
        "✅ 6. Naive Bayes\n",
        "How it works: Applies Bayes' theorem assuming feature independence\n",
        "\n",
        "Pros: Fast, simple, works well with text (e.g., spam filtering)\n",
        "\n",
        "Cons: Strong independence assumption, less accurate in complex settings\n",
        "\n",
        "Best for: Text classification, real-time predictions\n",
        "\n",
        "✅ 7. Neural Networks\n",
        "How it works: Learns non-linear functions using layers of nodes\n",
        "\n",
        "Pros: Powerful with large, complex datasets\n",
        "\n",
        "Cons: Requires tuning, computationally expensive, harder to interpret\n",
        "\n",
        "Best for: Image, audio, and natural language classification\n",
        "\n",
        "\n",
        "    - Decision Trees\n",
        "    - Random Forest\n",
        "    - Support Vector Machines (SVM)\n",
        "    - Neural Networks\n",
        "    - Naïve Bayes\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "    - ✅ Classification Evaluation Metrics\n",
        "Classification evaluation metrics help you measure how well your model predicts class labels. The choice of metric depends on your problem type (binary vs. multiclass), class imbalance, and business goals (e.g., accuracy vs. recall in medical diagnosis).\n",
        "\n",
        "🔹 1. Accuracy\n",
        "Accuracy\n",
        "=\n",
        "TP + TN\n",
        "TP + TN + FP + FN\n",
        "Accuracy=\n",
        "TP + TN + FP + FN\n",
        "TP + TN\n",
        "​\n",
        "\n",
        "Proportion of correct predictions.\n",
        "\n",
        "✅ Easy to interpret\n",
        "\n",
        "🔹 2. Precision\n",
        "Precision\n",
        "=\n",
        "TP\n",
        "TP + FP\n",
        "Precision=\n",
        "TP + FP\n",
        "TP\n",
        "​\n",
        "\n",
        "Out of predicted positives, how many were actually correct.\n",
        "\n",
        "✅ Important when false positives are costly (e.g., spam filters).\n",
        "\n",
        "🔹 3. Recall (Sensitivity, True Positive Rate)\n",
        "Recall\n",
        "=\n",
        "TP\n",
        "TP + FN\n",
        "Recall=\n",
        "TP + FN\n",
        "TP\n",
        "​\n",
        "\n",
        "Out of actual positives, how many did the model find.\n",
        "\n",
        "✅ Important when false negatives are costly (e.g., medical diagnosis).\n",
        "\n",
        "🔹 4. F1-Score\n",
        "F1\n",
        "=\n",
        "2\n",
        "⋅\n",
        "Precision\n",
        "⋅\n",
        "Recall\n",
        "Precision + Recall\n",
        "F1=2⋅\n",
        "Precision + Recall\n",
        "Precision⋅Recall\n",
        "​\n",
        "\n",
        "Harmonic mean of precision and recall.\n",
        "\n",
        "✅ Best when you want a balance between precision and recall.\n",
        "\n",
        "🔹 5. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
        "Measures the trade-off between true positive rate and false positive rate.\n",
        "\n",
        "✅ Useful for binary classification\n",
        "\n",
        "✅ Works well with imbalanced datasets\n",
        "\n",
        "🔹 6. PR-AUC (Precision-Recall AUC)\n",
        "Area under the Precision-Recall curve.\n",
        "\n",
        "✅ Better than ROC-AUC for highly imbalanced problems.\n",
        "\n",
        "🔹 7. Confusion Matrix\n",
        "Shows TP, FP, FN, TN in a table.\n",
        "\n",
        "✅ Visual tool to understand model errors.\n",
        "\n",
        "🔹 8. Specificity (True Negative Rate)\n",
        "Specificity\n",
        "=\n",
        "TN\n",
        "TN + FP\n",
        "Specificity=\n",
        "TN + FP\n",
        "TN\n",
        "​\n",
        "\n",
        "Out of actual negatives, how many were correctly identified.\n",
        "\n",
        "🔹 9. Cohen's Kappa\n",
        "Measures agreement between predicted and actual labels, adjusting for chance.\n",
        "\n",
        "✅ Robust alternative to accuracy for imbalanced classes.\n",
        "\n",
        "🔹 10. Matthews Correlation Coefficient (MCC)\n",
        "MCC\n",
        "=\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "⋅\n",
        "𝑇\n",
        "𝑁\n",
        ")\n",
        "−\n",
        "(\n",
        "𝐹\n",
        "𝑃\n",
        "⋅\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        ")\n",
        "(\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "MCC=\n",
        "(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
        "​\n",
        "\n",
        "(TP⋅TN)−(FP⋅FN)\n",
        "​\n",
        "\n",
        "✅ Balanced metric even with class imbalance.\n",
        "\n",
        "Value ranges from -1 (total disagreement) to 1 (perfect agreement).\n",
        "\n",
        "\n",
        "    - Accuracy\n",
        "    - Precision, Recall, F1-score\n",
        "    - ROC-AUC Curve\n",
        "    - Confusion Matrix\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "    - Class imbalance can negatively affect Logistic Regression, just like many other classifiers. Here's how and why:\n",
        "\n",
        "When one class (e.g., \"No\") significantly outweighs the other (e.g., \"Yes\"), Logistic Regression may:\n",
        "\n",
        "Learn to favor the majority class\n",
        "\n",
        "Appear to have high accuracy, but actually perform poorly on the minority class\n",
        "\n",
        "Produce biased probability estimates\n",
        "\n",
        "Mislead evaluation metrics (e.g., 95% accuracy may just mean it's predicting the majority class)\n",
        "\n",
        "📘 Example:\n",
        "Suppose:\n",
        "\n",
        "Class 0 = 95% of data\n",
        "\n",
        "Class 1 = 5% of data\n",
        "\n",
        "A model that always predicts class 0 will be 95% accurate, but it will completely fail to detect class 1.\n",
        "\n",
        "🧠 Why This Happens in Logistic Regression:\n",
        "Logistic Regression minimizes log-loss, which is dominated by the majority class if it's not balanced.\n",
        "\n",
        "As a result, it can neglect the minority class, making it almost irrelevant in decision-making.\n",
        "\n",
        "✅ Solutions to Handle Class Imbalance:\n",
        "1. Use class_weight='balanced'\n",
        "Automatically adjusts weights inversely proportional to class frequencies.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "2. Manually assign class_weight\n",
        "Give more importance to the minority class:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model = LogisticRegression(class_weight={0:1, 1:5})\n",
        "3. Use resampling techniques\n",
        "Oversample the minority class (e.g., SMOTE)\n",
        "\n",
        "Undersample the majority class\n",
        "\n",
        "4. Use better evaluation metrics\n",
        "Precision, Recall, F1-score\n",
        "\n",
        "ROC-AUC, PR-AUC\n",
        "\n",
        "Confusion matrix (check both TP and FN)\n",
        "    - Leads to biased predictions towards the majority class.\n",
        "    - Solutions: SMOTE, Class weights adjustment, Threshold tuning.\n",
        "\n",
        "13.  What is Hyperparameter Tuning in Logistic Regression.\n",
        "    - Hyperparameter tuning in Logistic Regression is the process of optimizing the external settings of the model — such as regularization strength or penalty type — to achieve the best performance on unseen data.\n",
        "\n",
        "Unlike model parameters (like coefficients), hyperparameters are set before training and directly influence how the model learns.\n",
        "\n",
        "✅ Why is it Important?\n",
        "Boosts accuracy, precision, recall, F1, etc.\n",
        "\n",
        "Helps avoid overfitting or underfitting\n",
        "\n",
        "Improves model generalization to new data\n",
        "\n",
        "🔍 Common Hyperparameters in Logistic Regression:\n",
        "Hyperparameter\tDescription\n",
        "C\tInverse of regularization strength (smaller C → more regularization)\n",
        "penalty\tType of regularization: 'l1', 'l2', 'elasticnet', 'none'\n",
        "solver\tOptimization algorithm: 'liblinear', 'lbfgs', 'saga', 'newton-cg'\n",
        "max_iter\tMaximum number of iterations to converge\n",
        "class_weight\tHandle imbalanced classes (e.g., 'balanced')\n",
        "\n",
        "⚙️ How to Perform Tuning (using GridSearchCV):\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n",
        "\n",
        "    - Hyperparameter tuning in Logistic Regression refers to the process of systematically searching for the best combination of hyperparameter values that result in the most accurate and generalizable model.\n",
        "    - Adjusting parameters like regularization strength (λ) and solver to improve performance.\n",
        "    - Done using GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "    - 🔧 Solvers in Logistic Regression (Scikit-learn)\n",
        "In scikit-learn, the solver parameter in LogisticRegression specifies the algorithm used to optimize the model. Each solver has strengths, limitations, and compatibility with penalties.\n",
        "\n",
        "✅ List of Solvers:\n",
        "Solver\tType\tSupports L1\tSupports L2\tSupports ElasticNet\tMulticlass Support\tBest For\n",
        "liblinear\tCoordinate Descent\t✅ Yes\t✅ Yes\t❌ No\t✅ One-vs-Rest\tSmall datasets, L1\n",
        "lbfgs\tQuasi-Newton\t❌ No\t✅ Yes\t❌ No\t✅ Multinomial\tDefault for most cases\n",
        "newton-cg\tNewton-Raphson\t❌ No\t✅ Yes\t❌ No\t✅ Multinomial\tHigh-dimensional data\n",
        "sag\tStochastic Gradient\t❌ No\t✅ Yes\t❌ No\t✅ Multinomial\tLarge datasets (linear models)\n",
        "saga\tStochastic Average Gradient\t✅ Yes\t✅ Yes\t✅ Yes\t✅ Multinomial\tLarge datasets, ElasticNet, L1\n",
        "\n",
        "📌 Solver vs Penalty Compatibility:\n",
        "Penalty\tliblinear\tlbfgs\tnewton-cg\tsag\tsaga\n",
        "l2\t✅\t✅\t✅\t✅\t✅\n",
        "l1\t✅\t❌\t❌\t❌\t✅\n",
        "elasticnet\t❌\t❌\t❌\t❌\t✅\n",
        "none\t❌\t✅\t✅\t✅\t✅\n",
        "\n",
        "✅ Which Solver Should You Use?\n",
        "Use Case\tRecommended Solver\n",
        "Small dataset + L1 regularization\tliblinear\n",
        "Default (general-purpose)\tlbfgs\n",
        "Very large dataset\tsag or saga\n",
        "Need ElasticNet regularization\tsaga\n",
        "Multiclass classification\tlbfgs, newton-cg, or saga\n",
        "\n",
        "💡 Default in scikit-learn:\n",
        "As of scikit-learn 0.22+, the default solver is lbfgs, which is good for most use cases.\n",
        "\n",
        "✅ Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use saga for L1 or ElasticNet\n",
        "model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, C=1.0)\n",
        "\n",
        "    - Solver\t|         Type               |     When to Use\n",
        "    - lbfgs \t |     Quasi-Newton\t        |  Best for small datasets\n",
        "    - liblinear\t|    Coordinate Descent\t  | Good for L1 regularization\n",
        "    - saga\t  |   Stochastic Gradient Descent\t |  Large datasets\n",
        "    - newton-cg |\t    Newton’s Method\t      |   L2 regularization\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "    - Logistic Regression is inherently a binary classifier, but it can be extended to handle multiclass classification using two main strategies:\n",
        "\n",
        "✅ 1. One-vs-Rest (OvR) Strategy\n",
        "Also known as One-vs-All\n",
        "\n",
        "Fits one binary classifier per class:\n",
        "\n",
        "For class k, it predicts: Is this class k or not?\n",
        "\n",
        "The class with the highest predicted probability is selected.\n",
        "\n",
        "📘 Example:\n",
        "For 3 classes A, B, and C:\n",
        "\n",
        "Train 3 models:\n",
        "\n",
        "A vs (B+C),\n",
        "\n",
        "B vs (A+C),\n",
        "\n",
        "C vs (A+B)\n",
        "\n",
        "Scikit-learn uses OvR by default for LogisticRegression(multi_class='ovr')\n",
        "\n",
        "✅ 2. Multinomial (Softmax) Strategy\n",
        "Uses the softmax function to predict probabilities for all classes simultaneously\n",
        "\n",
        "Trains a single model with multiple output weights\n",
        "\n",
        "Optimizes cross-entropy loss over all classes\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "β\n",
        "j\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "e\n",
        "β\n",
        "k\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "​\n",
        "\n",
        "Use LogisticRegression(multi_class='multinomial', solver='lbfgs' or 'saga') for this\n",
        "\n",
        "✅ Comparison:\n",
        "Feature\tOne-vs-Rest (OvR)\tMultinomial (Softmax)\n",
        "Number of Models\tK binary classifiers\tOne single model\n",
        "Speed\tSlower to train\tFaster (single model)\n",
        "Accuracy\tMay be lower\tGenerally better\n",
        "Interpretability\tEasier (binary output)\tHarder (combined logic)\n",
        "Supported Solvers\tliblinear, saga\tlbfgs, saga, newton-cg\n",
        "\n",
        "✅ When to Use Which?\n",
        "Scenario\tStrategy\n",
        "Small dataset, high interpretability needed\tOvR\n",
        "Large dataset, better accuracy desired\tMultinomial\n",
        "Regularization with L1 or ElasticNet\tOvR or Softmax via saga\n",
        "\n",
        "✅ Example in Scikit-learn:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Multinomial strategy (softmax)\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "    - One-vs-Rest (OvR) – Trains one model per class.\n",
        "    - Softmax Regression (Multinomial) – Generalizes Logistic Regression for multiple classes.\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "    - The advantages and disadvantages of Logistic Regression are:\n",
        "    -  Advantages of Logistic Regression:\n",
        "    - Simple and Interpretable:\n",
        "\n",
        "    - Easy to implement and understand.\n",
        "\n",
        "    - You can interpret coefficients to understand the effect of each feature.\n",
        "\n",
        "    - Computationally Efficient:\n",
        "\n",
        "    - Fast to train, even on large datasets.\n",
        "\n",
        "    - Probability Output:\n",
        "\n",
        "    - Predicts probabilities, not just class labels. This is useful in risk-based or ranking scenarios.\n",
        "\n",
        "    - Works Well with Linearly Separable Data:\n",
        "\n",
        "    - Performs very well when the classes are linearly separable.\n",
        "\n",
        "    - Regularization Support:\n",
        "\n",
        "    - scikit-learn supports L1, L2, and ElasticNet to avoid overfitting.\n",
        "\n",
        "    - Baseline for Classification Tasks:\n",
        "\n",
        "    - Good first model to try for binary (or multinomial) classification problems.\n",
        "\n",
        "    -  Disadvantages of Logistic Regression:\n",
        "    - Assumes Linear Decision Boundary:\n",
        "\n",
        "    - Cannot capture complex relationships unless features are transformed.\n",
        "\n",
        "    - Not Great with Highly Correlated Features:\n",
        "\n",
        "    - Multicollinearity (correlation between features) can reduce model reliability.\n",
        "\n",
        "    - Sensitive to Outliers:\n",
        "\n",
        "    - Logistic Regression is not robust to extreme values in the data.\n",
        "\n",
        "    - Limited to Binary or Multinomial Classification:\n",
        "\n",
        "    - Not suitable for regression tasks or ordinal classification without special adaptations.\n",
        "\n",
        "    - Requires Feature Scaling:\n",
        "\n",
        "    - Especially when using regularization or solvers like saga, sag.\n",
        "\n",
        "    - Performance Drops on Non-linearly Separable Data:\n",
        "\n",
        "    - May perform poorly if data has complex patterns that require nonlinear models like decision trees or neural networks.\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        "    - Logistic Regression is widely used for classification problems where the output is categorical (binary or multiclass). Here are some real-world use cases:\n",
        "\n",
        "    Common Use Cases of Logistic Regression:\n",
        "1 Medical Diagnosis\n",
        "Predict if a patient has a disease (e.g., diabetes, cancer) based on symptoms, lab results, etc.\n",
        "\n",
        "Example: 0 = No cancer, 1 = Has cancer\n",
        "\n",
        "2. Credit Scoring / Loan Approval\n",
        "Determine whether a loan applicant is likely to default or not.\n",
        "\n",
        "Example: 0 = No Default, 1 = Default\n",
        "\n",
        "3. Email Spam Detection\n",
        "Classify emails as spam or not spam.\n",
        "\n",
        "Example: 0 = Not Spam, 1 = Spam\n",
        "\n",
        "4. Customer Churn Prediction\n",
        "Predict whether a customer will leave a service (churn) or stay.\n",
        "\n",
        "Example: 0 = Stay, 1 = Churn\n",
        "\n",
        "5. Marketing Campaigns\n",
        "Predict whether a customer will respond to a promotional offer.\n",
        "\n",
        "Example: 0 = No Response, 1 = Responded\n",
        "\n",
        "6. Image Recognition (Basic)\n",
        "Identify simple objects, such as handwritten digits (e.g., in MNIST dataset using one-vs-rest multiclass logistic regression).\n",
        "\n",
        "7. Fraud Detection\n",
        "Detect fraudulent transactions or behaviors based on transaction patterns.\n",
        "\n",
        "8. Ad Click Prediction\n",
        "Predict whether a user will click on an ad based on their behavior and demographics.\n",
        "\n",
        "9. HR Analytics\n",
        "Predict if a candidate will accept a job offer or if an employee is likely to resign.\n",
        "\n",
        "10. Political Predictions\n",
        "Predict voting outcomes: whether a person will vote for a candidate or not, based on demographics and opinions.\n",
        "\n",
        "Q18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "    - The key difference between Softmax Regression and Logistic Regression lies in the type of classification they handle:\n",
        "\n",
        "✅ 1. Logistic Regression:\n",
        "Used for binary classification\n",
        "\n",
        "Predicts the probability of one class (0 or 1) using the sigmoid function\n",
        "\n",
        "🔢 Formula:\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "🔄 Output:\n",
        "One probability (e.g., probability of class 1), and\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "1−p is for class 0\n",
        "\n",
        "🧠 Use Case:\n",
        "Spam vs Not Spam\n",
        "\n",
        "Disease vs No Disease\n",
        "\n",
        "✅ 2. Softmax Regression (a.k.a. Multinomial Logistic Regression):\n",
        "Used for multiclass classification (i.e., more than two categories)\n",
        "\n",
        "Generalization of logistic regression for 3 or more classes\n",
        "\n",
        "Uses the softmax function to assign probabilities to each class\n",
        "\n",
        "🔢 Formula:\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝜃\n",
        "𝑘\n",
        "𝑇\n",
        "𝑋\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝜃\n",
        "𝑗\n",
        "𝑇\n",
        "𝑋\n",
        "for each class\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "…\n",
        "𝐾\n",
        "P(y=k∣X)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "θ\n",
        "j\n",
        "T\n",
        "​\n",
        " X\n",
        "\n",
        "e\n",
        "θ\n",
        "k\n",
        "T\n",
        "​\n",
        " X\n",
        "\n",
        "​\n",
        " for each class k=1…K\n",
        "🔄 Output:\n",
        "A vector of probabilities (one for each class)\n",
        "\n",
        "The class with the highest probability is the prediction\n",
        "\n",
        "🧠 Use Case:\n",
        "Classifying an image as cat, dog, or bird\n",
        "\n",
        "Predicting customer segment: Silver, Gold, Platinum\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "    -  Choosing between One-vs-Rest (OvR) and Softmax (Multinomial) for multiclass classification depends on your data, model, and performance needs.\n",
        "\n",
        "✅ Quick Summary:\n",
        "Strategy\tOne-vs-Rest (OvR)\tSoftmax (Multinomial Logistic Regression)\n",
        "How it works\tTrains one binary classifier per class\tTrains one model for all classes together\n",
        "Number of models\tK (one per class)\t1 model with K outputs\n",
        "Output\tHighest probability from binary models\tNormalized probabilities across all classes\n",
        "Speed\tSlower to train, faster to predict\tFaster training (for small K), slower prediction\n",
        "Accuracy\tMay be less accurate for correlated classes\tUsually more accurate with inter-class context\n",
        "\n",
        "✅ Use Softmax (Multinomial) when:\n",
        "You’re using Logistic Regression with many classes\n",
        "\n",
        "You want probability distributions across all classes\n",
        "\n",
        "You have mutually exclusive classes (e.g., \"cat\", \"dog\", \"bird\")\n",
        "\n",
        "You care about global optimization across all classes\n",
        "\n",
        "Use LogisticRegression(multi_class='multinomial', solver='lbfgs') in scikit-learn.\n",
        "\n",
        "✅ Use One-vs-Rest (OvR) when:\n",
        "Your classifier doesn’t support multiclass natively (e.g., SVM, Perceptron)\n",
        "\n",
        "You want interpretable binary classifiers per class\n",
        "\n",
        "Your classes are not mutually exclusive or heavily imbalanced\n",
        "\n",
        "You need custom thresholds per class\n",
        "\n",
        "Use LogisticRegression(multi_class='ovr') or OneVsRestClassifier wrapper in scikit-learn.\n",
        "\n",
        "🧠 Practical Example:\n",
        "Problem: Classify email types: \"Promotional\", \"Social\", \"Updates\"\n",
        "\n",
        "You Want...\tRecommended\n",
        "High probability accuracy per email category\tSoftmax\n",
        "Custom control over each class (e.g., tune thresholds for \"Spam\")\tOvR\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "    - Interpreting coefficients in Logistic Regression is slightly different from linear regression because the model predicts log-odds rather than a direct output. Here's a breakdown:\n",
        "\n",
        "✅ 1. Basic Formula of Logistic Regression:\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "1−p\n",
        "p\n",
        "​\n",
        "  is the odds of the event happening (e.g., success)\n",
        "\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        "  is the coefficient for feature\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "✅ 2. Interpretation of a Coefficient (\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        " ):\n",
        "In terms of log-odds:\n",
        "Each unit increase in\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  changes the log-odds of the outcome by\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        " .\n",
        "\n",
        "In terms of odds ratio:\n",
        "Take the exponential of the coefficient:\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "Odds Ratio=e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " >1: the odds increase\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " <1: the odds decrease\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " =1: no effect on the outcome\n",
        "\n",
        "📘 Example:\n",
        "Suppose a logistic regression model for predicting the chance of buying a product has:\n",
        "\n",
        "Intercept\n",
        "𝛽\n",
        "0\n",
        "=\n",
        "−\n",
        "1.2\n",
        "β\n",
        "0\n",
        "​\n",
        " =−1.2\n",
        "\n",
        "Coefficient for \"Ad Clicked\" =\n",
        "𝛽\n",
        "1\n",
        "=\n",
        "0.8\n",
        "β\n",
        "1\n",
        "​\n",
        " =0.8\n",
        "\n",
        "➤ Interpretation:\n",
        "Log-odds: If someone clicks an ad, the log-odds of buying increases by 0.8\n",
        "\n",
        "Odds ratio:\n",
        "\n",
        "𝑒\n",
        "0.8\n",
        "≈\n",
        "2.23\n",
        "e\n",
        "0.8\n",
        " ≈2.23\n",
        "So, a person who clicked the ad is 2.23 times more likely to buy the product compared to someone who didn’t click.\n",
        "\n",
        "✅ 3. For Categorical Variables:\n",
        "If a categorical variable is encoded (e.g., via one-hot encoding), each coefficient shows the effect of that category compared to the baseline.\n",
        "\n",
        "✅ 4. Sign of Coefficients:\n",
        "Positive β → increases the probability of the outcome\n",
        "\n",
        "Negative β → decreases the probability of the outcome\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t4piJXX2aCRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions:"
      ],
      "metadata": {
        "id": "fKrZ7oQpqhAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjdLiWhGqmsG",
        "outputId": "d548e923-cd9e-4612-e683-1116a23db6b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with L1 regularization (Lasso): {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkdkTR7lrAM5",
        "outputId": "c03935f5-17eb-4259-8531-7304dfd701d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 regularization (Lasso): 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with L2 regularization (Ridge): {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBjIw_6irdy2",
        "outputId": "555ea0e6-e2ba-4061-a5f4-d21a3760041f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 regularization (Ridge): 0.97\n",
            "Model Coefficients:\n",
            "[[-4.70615403e-01  6.52347878e-01  1.15785115e+00 -5.93993010e-01\n",
            "  -2.40022883e-02  1.22893881e-01  1.34792635e+00  1.21129324e-01\n",
            "  -3.06785207e-01 -7.98211199e-02 -1.51671077e-01  7.16849518e-01\n",
            "   1.41153991e-02]\n",
            " [ 8.05008463e-01 -1.09848709e+00 -8.53071213e-01  2.77643511e-01\n",
            "   2.95994782e-03  6.31536994e-02  4.73150900e-01  2.72080075e-01\n",
            "   6.91066585e-01 -1.78399154e+00  6.66507852e-01  3.35794615e-01\n",
            "  -1.20384315e-02]\n",
            " [-2.59133933e-01  6.52310178e-01  1.20611740e-01  8.04321893e-02\n",
            "   1.92407496e-02 -5.19182381e-01 -1.70239118e+00 -1.24123949e-01\n",
            "  -7.08593522e-01  1.03220422e+00 -4.64782174e-01 -1.23780486e+00\n",
            "  -5.56613331e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USgekTwUsGr-",
        "outputId": "7db9fc71-9b8c-4231-b3d7-f6f9e7ac4ace"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 0.75\n",
            "Model Coefficients:\n",
            "[[-6.21515088e-03 -1.94327610e-03 -1.10017715e-03 -1.59640443e-02\n",
            "  -3.81690428e-02 -1.74415294e-04  8.81517180e-04 -3.21393714e-04\n",
            "  -2.01476899e-04 -3.12608347e-03 -2.84537550e-04 -1.45940938e-04\n",
            "   5.68095790e-03]\n",
            " [ 3.98350092e-03 -1.14451478e-03  5.91957314e-04  9.79857674e-03\n",
            "   2.99405131e-02  1.52829334e-03  2.23332853e-03  3.34801897e-05\n",
            "   1.25322368e-03 -5.40616440e-03  1.00263986e-03  2.67618488e-03\n",
            "  -4.43127549e-03]\n",
            " [ 2.19013682e-03  3.11621057e-03  4.66706691e-04  6.12394274e-03\n",
            "   8.18700486e-03 -1.31234146e-03 -3.15635885e-03  2.46417952e-04\n",
            "  -1.01021606e-03  8.57376102e-03 -6.76577450e-04 -2.48868979e-03\n",
            "  -1.20866136e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression for multiclass classification using one-vs-rest (ovr)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with multi_class=\"ovr\": {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Q6h9mysYaw",
        "outputId": "253aedd5-5d9b-4714-8522-704338130cda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with multi_class=\"ovr\": 0.97\n",
            "Model Coefficients:\n",
            "[[-4.70615403e-01  6.52347878e-01  1.15785115e+00 -5.93993010e-01\n",
            "  -2.40022883e-02  1.22893881e-01  1.34792635e+00  1.21129324e-01\n",
            "  -3.06785207e-01 -7.98211199e-02 -1.51671077e-01  7.16849518e-01\n",
            "   1.41153991e-02]\n",
            " [ 8.05008463e-01 -1.09848709e+00 -8.53071213e-01  2.77643511e-01\n",
            "   2.95994782e-03  6.31536994e-02  4.73150900e-01  2.72080075e-01\n",
            "   6.91066585e-01 -1.78399154e+00  6.66507852e-01  3.35794615e-01\n",
            "  -1.20384315e-02]\n",
            " [-2.59133933e-01  6.52310178e-01  1.20611740e-01  8.04321893e-02\n",
            "   1.92407496e-02 -5.19182381e-01 -1.70239118e+00 -1.24123949e-01\n",
            "  -7.08593522e-01  1.03220422e+00 -4.64782174e-01 -1.23780486e+00\n",
            "  -5.56613331e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create pipeline (scaling + logistic regression)\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='saga', max_iter=5000))  # saga supports both l1 and l2\n",
        "])\n",
        "\n",
        "# Step 4: Define parameter grid\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10],\n",
        "    'logreg__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Step 5: Apply GridSearchCV\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on Test Set:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaKJzwlIuE5F",
        "outputId": "c35af9eb-c68b-48db-df94-38c2a52e4e9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logreg__C': 1, 'logreg__penalty': 'l1'}\n",
            "Accuracy on Test Set: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Define the Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using Stratified K-Fold Cross-Validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f'Average Accuracy with Stratified K-Fold Cross-Validation: {average_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4DLlw-rwVru",
        "outputId": "20a35fff-b039-498e-f4f9-4633a8dcda61"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy with Stratified K-Fold Cross-Validation: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Convert categorical to numerical\n",
        "\n",
        "df = df[features + ['Survived']].dropna()  # Drop rows with missing values\n",
        "\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dZ9KosfwpFD",
        "outputId": "3660313c-513f-49b7-8612-9dc424319016"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Preprocessing\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# Drop columns that are not needed for the model\n",
        "df.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
        "\n",
        "# Convert categorical 'Sex' to numerical\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Define features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 40),  # Increased number of values for C\n",
        "    'penalty': ['l1', 'l2'],       # Regularization type\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support l1 and l2 penalties\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "# Evaluate the model with the best parameters on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Cross-Validation Score: {best_score:.4f}')\n",
        "print(f'Test Set Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3y4_UT8w4Y4",
        "outputId": "3b495819-47d9-480f-ebcb-f1c24bab7401"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': np.float64(0.046415888336127774)}\n",
            "Best Cross-Validation Score: 0.7991\n",
            "Test Set Accuracy: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply One-vs-One (Ovo) Multiclass Logistic Regression\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(solver='liblinear'))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with One-vs-One (Ovo) Multiclass Logistic Regression: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v51loz7OxTDu",
        "outputId": "46d9f905-2a41-4181-fa29-64ef51959af8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-One (Ovo) Multiclass Logistic Regression: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "w_DaVR8Wxruu",
        "outputId": "5b9952c8-f954-4aae-dbeb-4c61f0e352ec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAIjCAYAAACTRapjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAR81JREFUeJzt3Xt8z/X///H7e7OTnTdsUzbHhlBCjHJKSSqaHDow0qfSHEcHfVKorI9iDoXEBx0oJJ9KkrMIiSmpnGuVHUQ2c9hme/3+8PP+9jZq0957v7xft2uX1+Wy9/P1er+ej/e7i3l4PA8vm2EYhgAAAGAZHq4OAAAAAOWLBBAAAMBiSAABAAAshgQQAADAYkgAAQAALIYEEAAAwGJIAAEAACyGBBAAAMBiSAABAAAshgQQwF/at2+fbrvtNgUHB8tms2np0qVlev+ffvpJNptNc+fOLdP7Xsnatm2rtm3bujoMAG6MBBC4Ahw4cECPPvqoatasKV9fXwUFBalVq1aaPHmyTp8+7dS+ExIStGvXLr300kt6++231bRpU6f2V5769u0rm82moKCgi36P+/btk81mk81m06uvvlrq+x8+fFijR4/Wzp07yyBaACg7FVwdAIC/tmzZMnXv3l0+Pj7q06ePGjRooPz8fG3cuFFPPPGEdu/erZkzZzql79OnT2vz5s3697//rYEDBzqlj5iYGJ0+fVpeXl5Ouf/fqVChgk6dOqWPP/5YPXr0cDj37rvvytfXV2fOnLmsex8+fFhjxoxR9erVdf3115f4fZ9//vll9QcAJUUCCJjYoUOH1KtXL8XExGjNmjWKioqyn0tMTNT+/fu1bNkyp/V/5MgRSVJISIjT+rDZbPL19XXa/f+Oj4+PWrVqpQULFhRLAOfPn6/OnTvrgw8+KJdYTp06pYoVK8rb27tc+gNgXQwBAyY2fvx45ebmavbs2Q7J33m1a9fWkCFD7K/Pnj2rF154QbVq1ZKPj4+qV6+uZ555Rnl5eQ7vq169uu68805t3LhRN954o3x9fVWzZk299dZb9mtGjx6tmJgYSdITTzwhm82m6tWrSzo3dHr+5z8bPXq0bDabQ9vKlSt10003KSQkRAEBAYqNjdUzzzxjP3+pOYBr1qzRzTffLH9/f4WEhKhLly764YcfLtrf/v371bdvX4WEhCg4OFj9+vXTqVOnLv3FXuD+++/X8uXLdfz4cXvbtm3btG/fPt1///3Frj927JhGjBihhg0bKiAgQEFBQerUqZO++eYb+zXr1q1Ts2bNJEn9+vWzDyWf/5xt27ZVgwYNtH37drVu3VoVK1a0fy8XzgFMSEiQr69vsc/fsWNHhYaG6vDhwyX+rAAgkQACpvbxxx+rZs2aatmyZYmuf/jhh/Xcc8/phhtuUEpKitq0aaPk5GT16tWr2LX79+/Xvffeq1tvvVUTJkxQaGio+vbtq927d0uS4uPjlZKSIkm677779Pbbb2vSpEmlin/37t268847lZeXp7Fjx2rChAm6++67tWnTpr9836pVq9SxY0dlZWVp9OjRSkpK0pdffqlWrVrpp59+KnZ9jx49dOLECSUnJ6tHjx6aO3euxowZU+I44+PjZbPZtGTJEnvb/PnzVbduXd1www3Frj948KCWLl2qO++8UxMnTtQTTzyhXbt2qU2bNvZkrF69eho7dqwk6ZFHHtHbb7+tt99+W61bt7bf5+jRo+rUqZOuv/56TZo0Se3atbtofJMnT1blypWVkJCgwsJCSdIbb7yhzz//XFOnTlXVqlVL/FkBQJJkADCl7OxsQ5LRpUuXEl2/c+dOQ5Lx8MMPO7SPGDHCkGSsWbPG3hYTE2NIMjZs2GBvy8rKMnx8fIzhw4fb2w4dOmRIMl555RWHeyYkJBgxMTHFYnj++eeNP/9aSUlJMSQZR44cuWTc5/uYM2eOve366683qlSpYhw9etTe9s033xgeHh5Gnz59ivX30EMPOdzznnvuMcLDwy/Z558/h7+/v2EYhnHvvfcat9xyi2EYhlFYWGhERkYaY8aMueh3cObMGaOwsLDY5/Dx8THGjh1rb9u2bVuxz3ZemzZtDEnGjBkzLnquTZs2Dm0rVqwwJBkvvviicfDgQSMgIMDo2rXr335GALgYKoCASeXk5EiSAgMDS3T9p59+KklKSkpyaB8+fLgkFZsrWL9+fd18883215UrV1ZsbKwOHjx42TFf6Pzcwf/9738qKioq0XvS09O1c+dO9e3bV2FhYfb2Ro0a6dZbb7V/zj977LHHHF7ffPPNOnr0qP07LIn7779f69atU0ZGhtasWaOMjIyLDv9K5+YNenic+/VZWFioo0eP2oe3d+zYUeI+fXx81K9fvxJde9ttt+nRRx/V2LFjFR8fL19fX73xxhsl7gsA/owEEDCpoKAgSdKJEydKdP3PP/8sDw8P1a5d26E9MjJSISEh+vnnnx3ao6Oji90jNDRUf/zxx2VGXFzPnj3VqlUrPfzww4qIiFCvXr20cOHCv0wGz8cZGxtb7Fy9evX0+++/6+TJkw7tF36W0NBQSSrVZ7njjjsUGBio999/X++++66aNWtW7Ls8r6ioSCkpKapTp458fHxUqVIlVa5cWd9++62ys7NL3OdVV11VqgUfr776qsLCwrRz505NmTJFVapUKfF7AeDPSAABkwoKClLVqlX13Xfflep9Fy7CuBRPT8+LthuGcdl9nJ+fdp6fn582bNigVatWqXfv3vr222/Vs2dP3XrrrcWu/Sf+yWc5z8fHR/Hx8Zo3b54+/PDDS1b/JGncuHFKSkpS69at9c4772jFihVauXKlrr322hJXOqVz309ppKamKisrS5K0a9euUr0XAP6MBBAwsTvvvFMHDhzQ5s2b//bamJgYFRUVad++fQ7tmZmZOn78uH1Fb1kIDQ11WDF73oVVRkny8PDQLbfcookTJ+r777/XSy+9pDVr1mjt2rUXvff5OPfs2VPs3I8//qhKlSrJ39//n32AS7j//vuVmpqqEydOXHThzHmLFy9Wu3btNHv2bPXq1Uu33XabOnToUOw7KWkyXhInT55Uv379VL9+fT3yyCMaP368tm3bVmb3B2AtJICAiT355JPy9/fXww8/rMzMzGLnDxw4oMmTJ0s6N4QpqdhK3YkTJ0qSOnfuXGZx1apVS9nZ2fr222/tbenp6frwww8drjt27Fix957fEPnCrWnOi4qK0vXXX6958+Y5JFTfffedPv/8c/vndIZ27drphRde0GuvvabIyMhLXufp6Vmsurho0SL99ttvDm3nE9WLJcul9dRTTyktLU3z5s3TxIkTVb16dSUkJFzyewSAv8JG0ICJ1apVS/Pnz1fPnj1Vr149hyeBfPnll1q0aJH69u0rSbruuuuUkJCgmTNn6vjx42rTpo2++uorzZs3T127dr3kFiOXo1evXnrqqad0zz33aPDgwTp16pSmT5+ua665xmERxNixY7VhwwZ17txZMTExysrK0rRp03T11VfrpptuuuT9X3nlFXXq1ElxcXHq37+/Tp8+ralTpyo4OFijR48us89xIQ8PDz377LN/e92dd96psWPHql+/fmrZsqV27dqld999VzVr1nS4rlatWgoJCdGMGTMUGBgof39/NW/eXDVq1ChVXGvWrNG0adP0/PPP27elmTNnjtq2batRo0Zp/PjxpbofALANDHAF2Lt3r/Gvf/3LqF69uuHt7W0EBgYarVq1MqZOnWqcOXPGfl1BQYExZswYo0aNGoaXl5dRrVo1Y+TIkQ7XGMa5bWA6d+5crJ8Ltx+51DYwhmEYn3/+udGgQQPD29vbiI2NNd55551i28CsXr3a6NKli1G1alXD29vbqFq1qnHfffcZe/fuLdbHhVulrFq1ymjVqpXh5+dnBAUFGXfddZfx/fffO1xzvr8Lt5mZM2eOIck4dOjQJb9Tw3DcBuZSLrUNzPDhw42oqCjDz8/PaNWqlbF58+aLbt/yv//9z6hfv75RoUIFh8/Zpk0b49prr71on3++T05OjhETE2PccMMNRkFBgcN1w4YNMzw8PIzNmzf/5WcAgAvZDKMUs6QBAABwxWMOIAAAgMWQAAIAAFgMCSAAAIDFkAACAACYRPXq1WWz2YodiYmJkqQzZ84oMTFR4eHhCggIULdu3S66TdjfYREIAACASRw5csThSUnfffedbr31Vq1du1Zt27bVgAEDtGzZMs2dO1fBwcEaOHCgPDw8tGnTplL1QwIIAABgUkOHDtUnn3yiffv2KScnR5UrV9b8+fN17733Sjr3hKR69epp8+bNatGiRYnvyxAwAACAE+Xl5SknJ8fhKMlTfPLz8/XOO+/ooYceks1m0/bt21VQUKAOHTrYr6lbt66io6NL9MjQP3PLJ4E8+M43rg4BgJNMjW/g6hAAOEloRU+X9e3XeKDT7v1Ul0oaM2aMQ9vzzz//t082Wrp0qY4fP25/4lNGRoa8vb0VEhLicF1ERIQyMjJKFZNbJoAAAABmMXLkSCUlJTm0+fj4/O37Zs+erU6dOqlq1aplHhMJIAAAgM15s+J8fHxKlPD92c8//6xVq1ZpyZIl9rbIyEjl5+fr+PHjDlXAzMxMRUZGlur+zAEEAACw2Zx3XIY5c+aoSpUq6ty5s72tSZMm8vLy0urVq+1te/bsUVpamuLi4kp1fyqAAAAAJlJUVKQ5c+YoISFBFSr8X6oWHBys/v37KykpSWFhYQoKCtKgQYMUFxdXqhXAEgkgAACAU4eAS2vVqlVKS0vTQw89VOxcSkqKPDw81K1bN+Xl5aljx46aNm1aqftwy30AWQUMuC9WAQPuy6WrgJsOc9q9T3+d4rR7Xy4qgAAAAJc5V+9KZZ56JwAAAMoFFUAAAAATzQEsD9b6tAAAAKACCAAAYLU5gCSAAAAADAEDAADAnVEBBAAAsNgQMBVAAAAAi6ECCAAAwBxAAAAAuDMqgAAAAMwBBAAAgDujAggAAGCxOYAkgAAAAAwBAwAAwJ1RAQQAALDYELC1Pi0AAACoAAIAAFABBAAAgFujAggAAODBKmAAAAC4MSqAAAAAFpsDSAIIAADARtAAAABwZ1QAAQAALDYEbK1PCwAAACqAAAAAzAEEAACAW6MCCAAAwBxAAAAAuDMqgAAAABabA0gCCAAAwBAwAAAA3BkVQAAAAIsNAVMBBAAAsBgqgAAAAMwBBAAAgDujAggAAMAcQAAAALgzKoAAAAAWmwNIAggAAGCxBNBanxYAAABUAAEAAFgEAgAAALdGBRAAAIA5gAAAAHBnVAABAACYAwgAAAB3RgUQAADAYnMASQABAAAYAgYAAIA7owIIAAAsz0YFEAAAAO6MCiAAALA8KoAAAABwaySAAAAANicepfTbb7/pwQcfVHh4uPz8/NSwYUN9/fXX9vOGYei5555TVFSU/Pz81KFDB+3bt69UfZAAAgAAmMQff/yhVq1aycvLS8uXL9f333+vCRMmKDQ01H7N+PHjNWXKFM2YMUNbt26Vv7+/OnbsqDNnzpS4H+YAAgAAyzPLHMD//Oc/qlatmubMmWNvq1Gjhv1nwzA0adIkPfvss+rSpYsk6a233lJERISWLl2qXr16lagfKoAAAMDybDab0468vDzl5OQ4HHl5eReN46OPPlLTpk3VvXt3ValSRY0bN9abb75pP3/o0CFlZGSoQ4cO9rbg4GA1b95cmzdvLvHnJQEEAABwouTkZAUHBzscycnJF7324MGDmj59uurUqaMVK1ZowIABGjx4sObNmydJysjIkCRFREQ4vC8iIsJ+riQYAgYAAJbnzCHgkSNHKikpyaHNx8fnotcWFRWpadOmGjdunCSpcePG+u677zRjxgwlJCSUWUxUAAEAAJzIx8dHQUFBDselEsCoqCjVr1/foa1evXpKS0uTJEVGRkqSMjMzHa7JzMy0nysJEkAAAGB5zpwDWBqtWrXSnj17HNr27t2rmJgYSecWhERGRmr16tX28zk5Odq6davi4uJK3A9DwAAAACYxbNgwtWzZUuPGjVOPHj301VdfaebMmZo5c6akc4nq0KFD9eKLL6pOnTqqUaOGRo0apapVq6pr164l7ocEEAAAwBy7wKhZs2b68MMPNXLkSI0dO1Y1atTQpEmT9MADD9ivefLJJ3Xy5Ek98sgjOn78uG666SZ99tln8vX1LXE/NsMwDGd8AFd68J1vXB0CACeZGt/A1SEAcJLQip4u6zv4/reddu/s+b2ddu/LRQUQAABYnlk2gi4vLAIBAACwGCqAAADA8qxWASQBBAAAlme1BJAhYAAAAIuhAggAACyPCiAAAADcGhVAAAAAaxUAqQACAABYDRVAAABgecwBdAFPT09lZWUVaz969Kg8PV33WBgAAAB3ZIoK4KUeR5yXlydvb+9yjgYAAFiN1SqALk0Ap0yZIunclz5r1iwFBATYzxUWFmrDhg2qW7euq8IDAAAWQQJYjlJSUiSdqwDOmDHDYbjX29tb1atX14wZM1wVHgAAgFtyaQJ46NAhSVK7du20ZMkShYaGujIcAABgVdYqAJpjDuDatWtdHQIAAIBlmCIBLCws1Ny5c7V69WplZWWpqKjI4fyaNWtcFBkAALAC5gC6wJAhQzR37lx17txZDRo0sNz/BAAAgPJkigTwvffe08KFC3XHHXe4OhQAAGBBVis+mWIjaG9vb9WuXdvVYQAAAFiCKRLA4cOHa/LkyZfcEBoAAMCZbDab0w4zMsUQ8MaNG7V27VotX75c1157rby8vBzOL1myxEWRAQAAKzBrouYspkgAQ0JCdM8997g6DAAAAEswRQI4Z84cV4cAAACszFoFQHPMAQQAAED5MUUFUJIWL16shQsXKi0tTfn5+Q7nduzY4aKoAACAFVhtDqApKoBTpkxRv379FBERodTUVN14440KDw/XwYMH1alTJ1eHBwAA4FZMkQBOmzZNM2fO1NSpU+Xt7a0nn3xSK1eu1ODBg5Wdne3q8AAAgJuz2jYwpkgA09LS1LJlS0mSn5+fTpw4IUnq3bu3FixY4MrQAAAA3I4pEsDIyEgdO3ZMkhQdHa0tW7ZIkg4dOsTm0AAAwOmoALpA+/bt9dFHH0mS+vXrp2HDhunWW29Vz5492R8QAAA4n82JhwmZYhXwzJkzVVRUJElKTExUeHi4vvzyS91999169NFHXRwdAACAezFFAujh4SEPj/8rRvbq1Uu9evVyYUQAAMBKzDpU6yymSAAl6fjx4/rqq6+UlZVlrwae16dPHxdFBQAA4H5MkQB+/PHHeuCBB5Sbm6ugoCCHLNxms5EAAgAAp7JaBdAUi0CGDx+uhx56SLm5uTp+/Lj++OMP+3F+dTAAAADKhikqgL/99psGDx6sihUrujoUmNQtdcJ1yzXhquzvLUn6NfuMPtyVqW8Pn9szskqAt+6/oaquqeIvLw+bvk0/oXnbflPOmbOuDBtAGXjrv29q2tQU9by/t4Y9MdLV4cBNUQF0gY4dO+rrr792dRgwsWOnCvR+arqeXb5Xo5bv1fcZuUpqU11XBfvIx9NDT91SU4YMjVt1QGM+3y9PD5uGt61h1tX3AEro+9279OEHC1W7TqyrQwHciikqgJ07d9YTTzyh77//Xg0bNpSXl5fD+bvvvttFkcEsUn/LcXi96JsM3XJNuGpX8ldoxXxV9vfWs5/u1emCcwuI3vgyTW/0aKD6kQHanZHripAB/EOnTp3U8888qZGjxmjOrDdcHQ7cnNUqgKZIAP/1r39JksaOHVvsnM1mU2FhYXmHBBOz2aTm0SHyqeChfb+fVESAjwxJBYX/99SYgkJDhiHFVvEnAQSuUK8mv6hWN7fRjS1akgDC+ayV/5kjAbxw25fSyMvLU15enkNbYUG+PL28/2lYMJmrQ3w1umNteXl66MzZIk1a/5MOZ+fpxJmzyjtbpF6No7RwZ7pssqln4yh5etgU4uf19zcGYDorP/tUe378Xv99Z6GrQwHckinmAP4TycnJCg4Odjh2fzzb1WHBCdJz8vTvZXv1/Gf7tHrv73q0ZbSqBvvoRF6hpnzxkxpfHaRZvRpqZs8GqujtoUNHT6mIZ0kDV5zMjHRNfCVZo18aLx8fH1eHA4uw2rOAbYbh+r8hp0yZctF2m80mX19f1a5dW61bt5anp2exay5WAXz0gz1UAC3g6VtqKis3X//d+qu9LcDHU0VFhk4VFOm1bvW1/IcjWvb9ERdGibI2Nb6Bq0OAk61fu0pPJQ12+J1fWFgom80mDw8Pbdi686J/H+DKF1rRdf9fayZ96rR7H5x4h9PufblMMQSckpKiI0eO6NSpUwoNDZUk/fHHH6pYsaICAgKUlZWlmjVrau3atapWrZrDe318fIr9C5HkzxpsNqmCh+O/rHLzzs0XrR8RoCDfCtrxa87F3grAxJreGKd3F/3Poe3F5/+tmBo11LvvwyR/cAqzVuqcxRRDwOPGjVOzZs20b98+HT16VEePHtXevXvVvHlzTZ48WWlpaYqMjNSwYcNcHSpcpMf1kYqt4q9K/l66OsRXPa6PVL2IAH156A9JUuuaoapVqaKqBHirVY0QDWodo89+OKL0nLy/uTMAs/H391et2nUcDl8/PwUHh6hW7TquDg9wC6aoAD777LP64IMPVKtWLXtb7dq19eqrr6pbt246ePCgxo8fr27durkwSrhSkG8FPdYyWiF+FXSqoFC//HFG41cf1Hf/f4VvVJCvejSOUoC3p46cLNBH32Vq+Q+/uzhqAMCVwmIFQHMkgOnp6Tp7tvgTG86ePauMjAxJUtWqVXXixInyDg0mMWvLr395/v2d6Xp/Z3o5RQOgvE2fNc/VIQBuxRRDwO3atdOjjz6q1NRUe1tqaqoGDBig9u3bS5J27dqlGjVquCpEAADgxqy2CtgUCeDs2bMVFhamJk2a2Bd1NG3aVGFhYZo9+9yWLgEBAZowYYKLIwUAAO7IZnPeYUamGAKOjIzUypUr9eOPP2rv3r2SpNjYWMXG/t+zH9u1a+eq8AAAANyKKRLA8+rWrau6deu6OgwAAGAxZh2qdRaXJYBJSUl64YUX5O/vr6SkpL+8duLEieUUFQAAgPtzWQKYmpqqgoIC+8+XYrWMHAAAlD+rpRsuSwDXrl170Z8BAADgXKaaAwgAAOAKHh7WKgG6LAGMj48v8bVLlixxYiQAAADW4rJ9AIODg0t8AAAAOJNZ9gEcPXp0sY2k/7xDypkzZ5SYmKjw8HAFBASoW7duyszMLPXndVkFcM6cOa7qGgAAwIGZFp1ee+21WrVqlf11hQr/l64NGzZMy5Yt06JFixQcHKyBAwcqPj5emzZtKlUfzAEEAAAwkQoVKigyMrJYe3Z2tmbPnq358+fbH5U7Z84c1atXT1u2bFGLFi1K3keZRfsPLV68WAsXLlRaWpry8/Mdzu3YscNFUQEAACtwZgEwLy9PeXl5Dm3nH317Mfv27VPVqlXl6+uruLg4JScnKzo6Wtu3b1dBQYE6dOhgv7Zu3bqKjo7W5s2bS5UAmuJZwFOmTFG/fv0UERGh1NRU3XjjjQoPD9fBgwfVqVMnV4cHAABw2ZKTk4utb0hOTr7otc2bN9fcuXP12Wefafr06Tp06JBuvvlmnThxQhkZGfL29lZISIjDeyIiIpSRkVGqmExRAZw2bZpmzpyp++67T3PnztWTTz6pmjVr6rnnntOxY8dcHR4AAHBzzpwDOHLkyGJPPbtU9e/Pha9GjRqpefPmiomJ0cKFC+Xn51dmMZmiApiWlqaWLVtKkvz8/HTixAlJUu/evbVgwQJXhgYAAPCP+Pj4KCgoyOG4VAJ4oZCQEF1zzTXav3+/IiMjlZ+fr+PHjztck5mZedE5g3/FFAlgZGSkvdIXHR2tLVu2SJIOHTokwzBcGRoAALCAC7deKcvjn8jNzdWBAwcUFRWlJk2ayMvLS6tXr7af37Nnj9LS0hQXF1eq+5piCLh9+/b66KOP1LhxY/Xr10/Dhg3T4sWL9fXXX5dqw2gAAIAr2YgRI3TXXXcpJiZGhw8f1vPPPy9PT0/dd999Cg4OVv/+/ZWUlKSwsDAFBQVp0KBBiouLK9UCEMkkCeDMmTNVVFQkSUpMTFSlSpW0adMm3X333XrsscdcHB0AAHB3ZtkG8Ndff9V9992no0ePqnLlyrrpppu0ZcsWVa5cWZKUkpIiDw8PdevWTXl5eerYsaOmTZtW6n5shknGWM+cOaNvv/1WWVlZ9mRQOleSveuuu0p1rwff+aaswwNgElPjG7g6BABOElrR02V9Nx6zxmn3Tn2+vdPufblMUQH87LPP1Lt3bx09erTYOZvNpsLCQhdEBQAA4J5MsQhk0KBB6tGjh9LT01VUVORwkPwBAABnM8uzgMuLKRLAzMxMJSUlKSIiwtWhAAAAuD1TJID33nuv1q1b5+owAACARZl1GxhnMcUcwNdee03du3fXF198oYYNG8rLy8vh/ODBg10UGQAAgPsxRQK4YMECff755/L19dW6descsmWbzUYCCAAAnMqkhTqnMUUC+O9//1tjxozR008/LQ8PU4xKAwAAuC1TJID5+fnq2bMnyR8AAHAJs87VcxZTZFwJCQl6//33XR0GAACAJZiiAlhYWKjx48drxYoVatSoUbFFIBMnTnRRZAAAwAosVgA0RwK4a9cuNW7cWJL03XffOZyzWkkWAACUP6vlG6ZIANeuXevqEAAAACzDFAkgAACAK1msAGiORSAAAAAoP1QAAQCA5VltDiAVQAAAAIuhAggAACzPYgVAKoAAAABWQwUQAABYntXmAJIAAgAAy7NY/scQMAAAgNVQAQQAAJZntSFgKoAAAAAWQwUQAABYHhVAAAAAuDUqgAAAwPIsVgCkAggAAGA1VAABAIDlWW0OIAkgAACwPIvlfwwBAwAAWA0VQAAAYHlWGwKmAggAAGAxVAABAIDlWawASAUQAADAaqgAAgAAy/OwWAmQCiAAAIDFUAEEAACWZ7ECIAkgAAAA28AAAADArVEBBAAAludhrQIgFUAAAACroQIIAAAsjzmAAAAAcGtUAAEAgOVZrABIBRAAAMBqqAACAADLs8laJUASQAAAYHlsAwMAAAC3RgUQAABYHtvAAAAAwK1RAQQAAJZnsQIgFUAAAACroQIIAAAsz8NiJUAqgAAAABZDBRAAAFiexQqAJIAAAABsAwMAAABTePnll2Wz2TR06FB725kzZ5SYmKjw8HAFBASoW7duyszMLNV9SQABAIDl2WzOOy7Xtm3b9MYbb6hRo0YO7cOGDdPHH3+sRYsWaf369Tp8+LDi4+NLdW8SQAAAAJPJzc3VAw88oDfffFOhoaH29uzsbM2ePVsTJ05U+/bt1aRJE82ZM0dffvmltmzZUuL7kwACAADL87DZnHbk5eUpJyfH4cjLy/vLeBITE9W5c2d16NDBoX379u0qKChwaK9bt66io6O1efPmkn/e0n09AAAAKI3k5GQFBwc7HMnJyZe8/r333tOOHTsuek1GRoa8vb0VEhLi0B4REaGMjIwSx8QqYAAAYHnOXAM8cuRIJSUlObT5+Phc9NpffvlFQ4YM0cqVK+Xr6+u0mEgAAQAAnMjHx+eSCd+Ftm/frqysLN1www32tsLCQm3YsEGvvfaaVqxYofz8fB0/ftyhCpiZmanIyMgSx0QCCAAALM8s+wDecsst2rVrl0Nbv379VLduXT311FOqVq2avLy8tHr1anXr1k2StGfPHqWlpSkuLq7E/ZAAAgAAy/MwR/6nwMBANWjQwKHN399f4eHh9vb+/fsrKSlJYWFhCgoK0qBBgxQXF6cWLVqUuB8SQAAAgCtISkqKPDw81K1bN+Xl5aljx46aNm1aqe5BAggAACzPLEPAF7Nu3TqH176+vnr99df1+uuvX/Y92QYGAADAYqgAAgAAyzNxAdApqAACAABYDBVAAABgeWaeA+gMJUoAP/rooxLf8O67777sYAAAAOB8JUoAu3btWqKb2Ww2FRYW/pN4AAAAyp1Z9gEsLyVKAIuKipwdBwAAgMtYbQiYRSAAAAAWc1mLQE6ePKn169crLS1N+fn5DucGDx5cJoEBAACUF2vV/y4jAUxNTdUdd9yhU6dO6eTJkwoLC9Pvv/+uihUrqkqVKiSAAAAAJlfqIeBhw4bprrvu0h9//CE/Pz9t2bJFP//8s5o0aaJXX33VGTECAAA4lYfN5rTDjEqdAO7cuVPDhw+Xh4eHPD09lZeXp2rVqmn8+PF65plnnBEjAAAAylCpE0AvLy95eJx7W5UqVZSWliZJCg4O1i+//FK20QEAAJQDm815hxmVeg5g48aNtW3bNtWpU0dt2rTRc889p99//11vv/22GjRo4IwYAQAAUIZKXQEcN26coqKiJEkvvfSSQkNDNWDAAB05ckQzZ84s8wABAACczWazOe0wo1JXAJs2bWr/uUqVKvrss8/KNCAAAAA412XtAwgAAOBOTFqoc5pSJ4A1atT4y3LmwYMH/1FAAAAA5c2s27U4S6kTwKFDhzq8LigoUGpqqj777DM98cQTZRUXAAAAnKTUCeCQIUMu2v7666/r66+//scBAQAAlDeLFQBLvwr4Ujp16qQPPvigrG4HAAAAJymzRSCLFy9WWFhYWd0OAACg3Jh1uxZnuayNoP/8JRmGoYyMDB05ckTTpk0r0+AAAABQ9kqdAHbp0sUhAfTw8FDlypXVtm1b1a1bt0yDu1yzel3n6hAAOElos4GuDgGAk5xOfc1lfZfZnLgrRKkTwNGjRzshDAAAAJSXUie8np6eysrKKtZ+9OhReXp6lklQAAAA5YlHwf0NwzAu2p6Xlydvb+9/HBAAAEB58zBnnuY0JU4Ap0yZIulchjxr1iwFBATYzxUWFmrDhg2mmQMIAACASytxApiSkiLpXAVwxowZDsO93t7eql69umbMmFH2EQIAADgZFcBLOHTokCSpXbt2WrJkiUJDQ50WFAAAAJyn1HMA165d64w4AAAAXMasizWcpdSrgLt166b//Oc/xdrHjx+v7t27l0lQAAAAcJ5SJ4AbNmzQHXfcUay9U6dO2rBhQ5kEBQAAUJ48bM47zKjUCWBubu5Ft3vx8vJSTk5OmQQFAAAA5yl1AtiwYUO9//77xdrfe+891a9fv0yCAgAAKE82m/MOMyr1IpBRo0YpPj5eBw4cUPv27SVJq1ev1vz587V48eIyDxAAAMDZPMyaqTlJqRPAu+66S0uXLtW4ceO0ePFi+fn56brrrtOaNWsUFhbmjBgBAABQhkqdAEpS586d1blzZ0lSTk6OFixYoBEjRmj79u0qLCws0wABAACcrdRz4q5wl/15N2zYoISEBFWtWlUTJkxQ+/bttWXLlrKMDQAAAE5QqgpgRkaG5s6dq9mzZysnJ0c9evRQXl6eli5dygIQAABwxbLYFMCSVwDvuusuxcbG6ttvv9WkSZN0+PBhTZ061ZmxAQAAwAlKXAFcvny5Bg8erAEDBqhOnTrOjAkAAKBcWW0VcIkrgBs3btSJEyfUpEkTNW/eXK+99pp+//13Z8YGAAAAJyhxAtiiRQu9+eabSk9P16OPPqr33ntPVatWVVFRkVauXKkTJ044M04AAACnsdpG0KVeBezv76+HHnpIGzdu1K5duzR8+HC9/PLLqlKliu6++25nxAgAAOBUPAu4FGJjYzV+/Hj9+uuvWrBgQVnFBAAAACe6rI2gL+Tp6amuXbuqa9euZXE7AACAcsUiEAAAALi1MqkAAgAAXMksVgCkAggAAGA1VAABAIDlmXW1rrNQAQQAALAYKoAAAMDybLJWCZAEEAAAWB5DwAAAAHBrVAABAIDlUQEEAACAS0yfPl2NGjVSUFCQgoKCFBcXp+XLl9vPnzlzRomJiQoPD1dAQIC6deumzMzMUvdDAggAACzPZrM57SiNq6++Wi+//LK2b9+ur7/+Wu3bt1eXLl20e/duSdKwYcP08ccfa9GiRVq/fr0OHz6s+Pj40n9ewzCMUr/L5M6cdXUEAJwltNlAV4cAwElOp77msr5fWXfQafd+om3Nf/T+sLAwvfLKK7r33ntVuXJlzZ8/X/fee68k6ccff1S9evW0efNmtWjRosT3ZA4gAACwPGfOAczLy1NeXp5Dm4+Pj3x8fP7yfYWFhVq0aJFOnjypuLg4bd++XQUFBerQoYP9mrp16yo6OrrUCSBDwAAAAE6UnJys4OBghyM5OfmS1+/atUsBAQHy8fHRY489pg8//FD169dXRkaGvL29FRIS4nB9RESEMjIyShUTFUAAAGB5pZyqVyojR45UUlKSQ9tfVf9iY2O1c+dOZWdna/HixUpISND69evLNCYSQAAAYHkeTswASzLc+2fe3t6qXbu2JKlJkybatm2bJk+erJ49eyo/P1/Hjx93qAJmZmYqMjKyVDExBAwAAGBiRUVFysvLU5MmTeTl5aXVq1fbz+3Zs0dpaWmKi4sr1T2pAAIAAMszy0bQI0eOVKdOnRQdHa0TJ05o/vz5WrdunVasWKHg4GD1799fSUlJCgsLU1BQkAYNGqS4uLhSLQCRSAABAABMIysrS3369FF6erqCg4PVqFEjrVixQrfeeqskKSUlRR4eHurWrZvy8vLUsWNHTZs2rdT9sA8ggCsK+wAC7suV+wBO3XTIafce1KqG0+59uZgDCAAAYDEMAQMAAMvzkEkmAZYTKoAAAAAWQwUQAABYnjM3gjYjEkAAAGB5ZtkGprwwBAwAAGAxVAABAIDlOfNRcGZEBRAAAMBiqAACAADLs1gBkAogAACA1VABBAAAlsccQAAAALg1KoAAAMDyLFYAJAEEAACw2pCo1T4vAACA5VEBBAAAlmez2BgwFUAAAACLoQIIAAAsz1r1PyqAAAAAlkMFEAAAWB4bQQMAAMCtUQEEAACWZ636HwkgAACA5Z4EwhAwAACAxVABBAAAlsdG0AAAAHBrVAABAIDlWa0iZrXPCwAAYHlUAAEAgOUxBxAAAABujQogAACwPGvV/6gAAgAAWA4VQAAAYHlWmwNIAggAACzPakOiVvu8AAAAlkcFEAAAWJ7VhoCpAAIAAFgMFUAAAGB51qr/UQEEAACwHCqAAADA8iw2BZAKIAAAgNVQAQQAAJbnYbFZgCSAAADA8hgCBgAAgFujAggAACzPZrEhYCqAAAAAFkMFEAAAWB5zAAEAAODWqAACAADLs9o2MFQAAQAALIYKIAAAsDyrzQEkAQQAAJZHAugi+/bt09q1a5WVlaWioiKHc88995yLogIAAHA/pkgA33zzTQ0YMECVKlVSZGSkbH9Kw202GwkgAABwKqttBG2KBPDFF1/USy+9pKeeesrVoQAAALg9UySAf/zxh7p37+7qMAAAgEV5WKsAaI5tYLp3767PP//c1WEAAAC4VHJyspo1a6bAwEBVqVJFXbt21Z49exyuOXPmjBITExUeHq6AgAB169ZNmZmZperHFBXA2rVra9SoUdqyZYsaNmwoLy8vh/ODBw92UWQAAMAKzDIHcP369UpMTFSzZs109uxZPfPMM7rtttv0/fffy9/fX5I0bNgwLVu2TIsWLVJwcLAGDhyo+Ph4bdq0qcT92AzDMJz1IUqqRo0alzxns9l08ODBUt3vzNl/GhEAswptNtDVIQBwktOpr7ms7zU/HnXavdvXDb/s9x45ckRVqlTR+vXr1bp1a2VnZ6ty5cqaP3++7r33XknSjz/+qHr16mnz5s1q0aJFie5rigrgoUOHXB0CAACwMGfuA5iXl6e8vDyHNh8fH/n4+Pzte7OzsyVJYWFhkqTt27eroKBAHTp0sF9Tt25dRUdHlyoBNMUcQAAAAFeyOfG/5ORkBQcHOxzJycl/G1NRUZGGDh2qVq1aqUGDBpKkjIwMeXt7KyQkxOHaiIgIZWRklPjzmqICmJSUdNF2m80mX19f1a5dW126dLFnvwAAAFeKkSNHFst1SlL9S0xM1HfffaeNGzeWeUymSABTU1O1Y8cOFRYWKjY2VpK0d+9eeXp6qm7dupo2bZqGDx+ujRs3qn79+i6OFgAAuBtnbgNT0uHePxs4cKA++eQTbdiwQVdffbW9PTIyUvn5+Tp+/LhDFTAzM1ORkZElvr8phoC7dOmiDh066PDhw9q+fbu2b9+uX3/9Vbfeeqvuu+8+/fbbb2rdurWGDRvm6lABAACcxjAMDRw4UB9++KHWrFlTbKFskyZN5OXlpdWrV9vb9uzZo7S0NMXFxZW4H1OsAr7qqqu0cuXKYtW93bt367bbbtNvv/2mHTt26LbbbtPvv//+t/djFTDgvlgFDLgvV64C/mLvH067983XhJb42scff1zz58/X//73P/uoqCQFBwfLz89PkjRgwAB9+umnmjt3roKCgjRo0CBJ0pdfflnifkxRAczOzlZWVlax9iNHjignJ0eSFBISovz8/PIODQAAoNxMnz5d2dnZatu2raKiouzH+++/b78mJSVFd955p7p166bWrVsrMjJSS5YsKVU/ppgD2KVLFz300EOaMGGCmjVrJknatm2bRowYoa5du0qSvvrqK11zzTUujBJmsv3rbZr739n64fvvdOTIEaVMeV3tb+nw928EYDo/LhujmKrF90mb8f4GDXt5oXy8K+jlpHh179hEPt4VtGrzDxoy7n1lHTvhgmjhrpy5DUxplGRg1tfXV6+//rpef/31y+7HFAngG2+8oWHDhqlXr146e/bc+G2FChWUkJCglJQUSef2uJk1a5Yrw4SJnD59SrGxseoa301JQxgSBK5kNz34ijz/NAO/fu2q+nTGIC1ZmSpJGj+imzrddK0eeHK2cnJPK+XpHnpvwsNq3y/FVSEDVzxTJIABAQF68803lZKSYn/qR82aNRUQEGC/5vrrr3dRdDCjm25uo5tubuPqMACUgd//yHV4PaJfAx1IO6Ivtu9TUICv+naNU99n5mr9tr2SpEeef0fffDhKNzasrq92/eSCiOGOTFIALDemSADPCwgIUKNGjVwdBgDARbwqeKrXHc005Z01kqTG9aLl7VVBa7bssV+z96dMpaUfU/NGNUgAUWY8zDIGXE5clgDGx8fbV6/Ex8f/5bV/NbHxYo9XMTxLv98OAMD17m7XSCGBfnrn462SpMjwIOXlFyg797TDdVlHcxQRHuSKEAG34LJVwMHBwbL9/2z7wsejXHj8lYs9XuWV//z941UAAOaT0LWlVmz6XulHsl0dCizG5sTDjFxWAZwzZ85Ffy6tiz1exfCk+gcAV5roqFC1bx6rXiPetLdlHM2Rj7eXggP8HKqAVcKDlHk0xxVhAm7BFPsA/hM+Pj4KCgpyOBj+BYArT++745R17ISWf7Hb3pb6Q5ryC86qXfP/2xC3TkwVRUeFaeu3h1wRJtyVxUqAplgEkpmZqREjRmj16tXKysoqtgdOYWGhiyKDWZ06eVJpaWn217/9+qt+/OEHBQcHK6pqVRdGBuBy2Gw29enSQu9+slWFhUX29pzcM5q7dLP+Mzxex7JP6sTJM5r4VHdt+eYgC0CAf8AUCWDfvn2VlpamUaNGKSoqyj43ELiU3bu/08P9+thfvzr+3LzPu7vcoxfGveyqsABcpvbNYxUdFaZ5S7cUO/fkqx+oqMjQglcfPrcR9Jc/aEjy+xe5C3D5bGYt1TmJKZ4FHBgYqC+++KLM9vrjWcCA++JZwID7cuWzgLcecN7Co+a1/npBqyuYogJYrVq1Ej36BAAAwBmsNvhoikUgkyZN0tNPP62ffvrJ1aEAAAALstgaEHNUAHv27KlTp06pVq1aqlixory8vBzOHzt2zEWRAQAAuB9TJICTJk1ydQgAAMDKzFqqcxJTJIAJCQmuDgEAAMAyTDEHUJIOHDigZ599Vvfdd5+ysrIkScuXL9fu3bv/5p0AAAD/jM2J/5mRKRLA9evXq2HDhtq6dauWLFmi3NxcSdI333yj559/3sXRAQAAuBdTJIBPP/20XnzxRa1cuVLe3t729vbt22vLluKbggIAAJQlm815hxmZIgHctWuX7rnnnmLtVapU0e+//+6CiAAAANyXKRLAkJAQpaenF2tPTU3VVVdd5YKIAACAlVhtH0BTJIC9evXSU089pYyMDNlsNhUVFWnTpk0aMWKE+vTp8/c3AAAA+CcslgGaIgEcN26c6tatq2rVqik3N1f169fXzTffrJYtW+rZZ591dXgAAABuxWaY6CG8v/zyi3bt2qWTJ0+qcePGql279mXd58zZMg4MgGmENhvo6hAAOMnp1Ndc1nfqzyecdu/GMYFOu/flMsVG0JI0e/ZspaSkaN++fZKkOnXqaOjQoXr44YddHBkAAIB7MUUC+Nxzz2nixIkaNGiQ4uLiJEmbN2/WsGHDlJaWprFjx7o4QgAA4M7Mul2Ls5hiCLhy5cqaMmWK7rvvPof2BQsWaNCgQaXeCoYhYMB9MQQMuC9XDgHvTHPeEPD10QwBX1RBQYGaNm1arL1JkyY6e5ZsDgAAOJfFCoDmWAXcu3dvTZ8+vVj7zJkz9cADD7ggIgAAAPflsgpgUlKS/WebzaZZs2bp888/V4sWLSRJW7duVVpaGvsAAgAA57NYCdBlCWBqaqrD6yZNmkiSDhw4IEmqVKmSKlWqpN27d5d7bAAAwFpsFssAXZYArl271lVdAwAAWJopFoEAAAC4ktW2gTHFIhAAAACUHyqAAADA8ixWAKQCCAAAYDVUAAEAACxWAqQCCAAAYDFUAAEAgOVZbR9AKoAAAAAWQwUQAABYntX2ASQBBAAAlmex/I8hYAAAAKuhAggAAGCxEiAVQAAAAIuhAggAACyPbWAAAADg1qgAAgAAy7PaNjBUAAEAACyGCiAAALA8ixUASQABAACslgEyBAwAAGAxVAABAIDlsQ0MAAAA3BoVQAAAYHlsAwMAAAC3RgUQAABYnsUKgFQAAQAAzGTDhg266667VLVqVdlsNi1dutThvGEYeu655xQVFSU/Pz916NBB+/btK1UfJIAAAAA2Jx6ldPLkSV133XV6/fXXL3p+/PjxmjJlimbMmKGtW7fK399fHTt21JkzZ0rcB0PAAADA8py5DUxeXp7y8vIc2nx8fOTj43PR6zt16qROnTpd9JxhGJo0aZKeffZZdenSRZL01ltvKSIiQkuXLlWvXr1KFBMVQAAAACdKTk5WcHCww5GcnHxZ9zp06JAyMjLUoUMHe1twcLCaN2+uzZs3l/g+VAABAIDlOXMbmJEjRyopKcmh7VLVv7+TkZEhSYqIiHBoj4iIsJ8rCRJAAAAAJ/qr4V5XYQgYAABYnonWgPylyMhISVJmZqZDe2Zmpv1cSZAAAgAAXCFq1KihyMhIrV692t6Wk5OjrVu3Ki4ursT3YQgYAADARDtB5+bmav/+/fbXhw4d0s6dOxUWFqbo6GgNHTpUL774ourUqaMaNWpo1KhRqlq1qrp27VriPkgAAQAATOTrr79Wu3bt7K/PLyBJSEjQ3Llz9eSTT+rkyZN65JFHdPz4cd1000367LPP5OvrW+I+bIZhGGUeuYudOevqCAA4S2izga4OAYCTnE59zWV9/3w07+8vukwx4eZaACJRAQQAAHDqNjBmxCIQAAAAi6ECCAAALM9iBUAqgAAAAFZDBRAAAFgecwABAADg1qgAAgAAWGwWIBVAAAAAi6ECCAAALM9qcwBJAAEAgOVZLP9jCBgAAMBqqAACAADLs9oQMBVAAAAAi6ECCAAALM9msVmAVAABAAAshgogAACAtQqAVAABAACshgogAACwPIsVAEkAAQAA2AYGAAAAbo0KIAAAsDy2gQEAAIBbowIIAABgrQIgFUAAAACroQIIAAAsz2IFQCqAAAAAVkMFEAAAWJ7V9gEkAQQAAJbHNjAAAABwa1QAAQCA5VltCJgKIAAAgMWQAAIAAFgMCSAAAIDFMAcQAABYHnMAAQAA4NaoAAIAAMuz2j6AJIAAAMDyGAIGAACAW6MCCAAALM9iBUAqgAAAAFZDBRAAAMBiJUAqgAAAABZDBRAAAFie1baBoQIIAABgMVQAAQCA5bEPIAAAANwaFUAAAGB5FisAkgACAABYLQNkCBgAAMBiqAACAADLYxsYAAAAuDUqgAAAwPLYBgYAAABuzWYYhuHqIIDLlZeXp+TkZI0cOVI+Pj6uDgdAGeLPN+A8JIC4ouXk5Cg4OFjZ2dkKCgpydTgAyhB/vgHnYQgYAADAYkgAAQAALIYEEAAAwGJIAHFF8/Hx0fPPP88EccAN8ecbcB4WgQAAAFgMFUAAAACLIQEEAACwGBJAAAAAiyEBhKn07dtXXbt2tb9u27athg4d6rJ4AJRMefxZvfD3A4DLV8HVAQB/ZcmSJfLy8nJ1GBdVvXp1DR06lAQVKCeTJ08W6xaBskECCFMLCwtzdQgATCI4ONjVIQBugyFgXLa2bdtq0KBBGjp0qEJDQxUREaE333xTJ0+eVL9+/RQYGKjatWtr+fLlkqTCwkL1799fNWrUkJ+fn2JjYzV58uS/7ePPFbb09HR17txZfn5+qlGjhubPn6/q1atr0qRJ9mtsNptmzZqle+65RxUrVlSdOnX00Ucf2c+XJI7zQ02vvvqqoqKiFB4ersTERBUUFNjj+vnnnzVs2DDZbDbZbLZ/+G0CV76zZ89q4MCBCg4OVqVKlTRq1Ch7xS4vL08jRozQVVddJX9/fzVv3lzr1q2zv3fu3LkKCQnRihUrVK9ePQUEBOj2229Xenq6/ZoLh4BPnDihBx54QP7+/oqKilJKSkqx3xnVq1fXuHHj9NBDDykwMFDR0dGaOXOms78KwPRIAPGPzJs3T5UqVdJXX32lQYMGacCAAerevbtatmypHTt26LbbblPv3r116tQpFRUV6eqrr9aiRYv0/fff67nnntMzzzyjhQsXlri/Pn366PDhw1q3bp0++OADzZw5U1lZWcWuGzNmjHr06KFvv/1Wd9xxhx544AEdO3ZMkkocx9q1a3XgwAGtXbtW8+bN09y5czV37lxJ54amr776ao0dO1bp6ekOf0kBVjVv3jxVqFBBX331lSZPnqyJEydq1qxZkqSBAwdq8+bNeu+99/Ttt9+qe/fuuv3227Vv3z77+0+dOqVXX31Vb7/9tjZs2KC0tDSNGDHikv0lJSVp06ZN+uijj7Ry5Up98cUX2rFjR7HrJkyYoKZNmyo1NVWPP/64BgwYoD179pT9FwBcSQzgMrVp08a46aab7K/Pnj1r+Pv7G71797a3paenG5KMzZs3X/QeiYmJRrdu3eyvExISjC5dujj0MWTIEMMwDOOHH34wJBnbtm2zn9+3b58hyUhJSbG3STKeffZZ++vc3FxDkrF8+fJLfpaLxRETE2OcPXvW3ta9e3ejZ8+e9tcxMTEO/QJW1qZNG6NevXpGUVGRve2pp54y6tWrZ/z888+Gp6en8dtvvzm855ZbbjFGjhxpGIZhzJkzx5Bk7N+/337+9ddfNyIiIuyv//z7IScnx/Dy8jIWLVpkP3/8+HGjYsWK9t8ZhnHuz+mDDz5of11UVGRUqVLFmD59epl8buBKxRxA/CONGjWy/+zp6anw8HA1bNjQ3hYRESFJ9ird66+/rv/+979KS0vT6dOnlZ+fr+uvv75Efe3Zs0cVKlTQDTfcYG+rXbu2QkND/zIuf39/BQUFOVQKSxLHtddeK09PT/vrqKgo7dq1q0SxAlbUokULh+kQcXFxmjBhgnbt2qXCwkJdc801Dtfn5eUpPDzc/rpixYqqVauW/XVUVNRFK/ySdPDgQRUUFOjGG2+0twUHBys2NrbYtX/+fWCz2RQZGXnJ+wJWQQKIf+TCFbo2m82h7fxfBkVFRXrvvfc0YsQITZgwQXFxcQoMDNQrr7yirVu3lktcRUVFklTiOP7qHgBKLjc3V56entq+fbvDP6okKSAgwP7zxf7MGWWw6pc/y0BxJIAoN5s2bVLLli31+OOP29sOHDhQ4vfHxsbq7NmzSk1NVZMmTSRJ+/fv1x9//FGucZzn7e2twsLCUr8PcFcX/iNqy5YtqlOnjho3bqzCwkJlZWXp5ptvLpO+atasKS8vL23btk3R0dGSpOzsbO3du1etW7cukz4Ad8YiEJSbOnXq6Ouvv9aKFSu0d+9ejRo1Stu2bSvx++vWrasOHTrokUce0VdffaXU1FQ98sgj8vPzK9Uq3H8ax3nVq1fXhg0b9Ntvv+n3338v9fsBd5OWlqakpCTt2bNHCxYs0NSpUzVkyBBdc801euCBB9SnTx8tWbJEhw4d0ldffaXk5GQtW7bssvoKDAxUQkKCnnjiCa1du1a7d+9W//795eHhwap8oARIAFFuHn30UcXHx6tnz55q3ry5jh496lCFK4m33npLERERat26te655x7961//UmBgoHx9fcs1DkkaO3asfvrpJ9WqVUuVK1cu9fsBd9OnTx+dPn1aN954oxITEzVkyBA98sgjkqQ5c+aoT58+Gj58uGJjY9W1a1eH6t3lmDhxouLi4nTnnXeqQ4cOatWqlerVq1eq3weAVdmMsphgAbjIr7/+qmrVqmnVqlW65ZZbXB0OABc6efKkrrrqKk2YMEH9+/d3dTiAqTEHEFeUNWvWKDc3Vw0bNlR6erqefPJJVa9enTk/gAWlpqbqxx9/1I033qjs7GyNHTtWktSlSxcXRwaYHwkgrigFBQV65plndPDgQQUGBqply5Z69913Tfu8YADO9eqrr2rPnj3y9vZWkyZN9MUXX6hSpUquDgswPYaAAQAALIZFIAAAABZDAggAAGAxJIAAAAAWQwIIAABgMSSAAAAAFkMCCMC0+vbtq65du9pft23bVkOHDi33ONatWyebzabjx4+Xe98A4AwkgABKrW/fvrLZbLLZbPL29lbt2rU1duxYnT171qn9LlmyRC+88EKJriVpA4BLYyNoAJfl9ttv15w5c5SXl6dPP/1UiYmJ8vLy0siRIx2uy8/Pl7e3d5n0GRYWVib3AQCrowII4LL4+PgoMjJSMTExGjBggDp06KCPPvrIPmz70ksvqWrVqoqNjZUk/fLLL+rRo4dCQkIUFhamLl266KeffrLfr7CwUElJSQoJCVF4eLiefPJJXbhP/YVDwHl5eXrqqadUrVo1+fj4qHbt2po9e7Z++ukntWvXTpIUGhoqm82mvn37SpKKioqUnJysGjVqyM/PT9ddd50WL17s0M+nn36qa665Rn5+fmrXrp1DnADgDkgAAZQJPz8/5efnS5JWr16tPXv2aOXKlfrkk09UUFCgjh07KjAwUF988YU2bdqkgIAA3X777fb3TJgwQXPnztV///tfbdy4UceOHdOHH374l3326dNHCxYs0JQpU/TDDz/ojTfeUEBAgKpVq6YPPvhAkrRnzx6lp6dr8uTJkqTk5GS99dZbmjFjhnbv3q1hw4bpwQcf1Pr16yWdS1Tj4+N11113aefOnXr44Yf19NNPO+trAwCXYAgYwD9iGIZWr16tFStWaNCgQTpy5Ij8/f01a9Ys+9DvO++8o6KiIs2aNUs2m02SNGfOHIWEhGjdunW67bbbNGnSJI0cOVLx8fGSpBkzZmjFihWX7Hfv3r1auHChVq5cqQ4dOkiSatasaT9/fri4SpUqCgkJkXSuYjhu3DitWrVKcXFx9vds3LhRb7zxhtq0aaPp06erVq1amjBhgiQpNjZWu3bt0n/+858y/NYAwLVIAAFclk8++UQBAQEqKChQUVGR7r//fo0ePVqJiYlq2LChw7y/b775Rvv371dgYKDDPc6cOaMDBw4oOztb6enpat68uf1chQoV1LRp02LDwOft3LlTnp6eatOmTYlj3r9/v06dOqVbb73VoT0/P1+NGzeWJP3www8OcUiyJ4sA4C5IAAFclnbt2mn69Ony9vZW1apVVaHC//068ff3d7g2NzdXTZo00bvvvlvsPpUrV76s/v38/Er9ntzcXEnSsmXLdNVVVzmc8/Hxuaw4AOBKRAII4LL4+/urdu3aJbr2hhtu0Pvvv68qVaooKCjootdERUVp69atat26tSTp7Nmz2r59u2644YaLXt+wYUMVFRVp/fr19iHgPztfgSwsLLS31a9fXz4+PkpLS7tk5bBevXr66KOPHNq2bNny9x8SAK4gLAIB4HQPPPCAKlWqpC5duuiLL77QoUOHtG7dOg0ePFi//vqrJGnIkCF6+eWXtXTpUv344496/PHH/3IPv+rVqyshIUEPPfSQli5dar/nwoULJUkxMTGy2Wz65JNPdOTIEeXm5iowMFAjRozQsGHDNG/ePB04cEA7duzQ1KlTNW/ePEnSY489pn379umJJ57Qnj17NH/+fM2dO9fZXxEAlCsSQABOV7FiRW3YsEHR0dGKj49XvXr11L9/f505c8ZeERw+fLh69+6thIQExcXFKTAwUPfcc89f3nf69Om699579fjjj6tu3br617/+pZMnT0qSrrrqKo0ZM0ZPP/20IiIiNHDgQEnSCy+8oFGjRik5OVn16tXT7bffrmXLlqlGjRqSpOjoaH3wwQdaunSprrvuOs2YMUPjxo1z4rcDAOXPZlxqhjUAAADcEhVAAAAAiyEBBAAAsBgSQAAAAIshAQQAALAYEkAAAACLIQEEAACwGBJAAAAAiyEBBAAAsBgSQAAAAIshAQQAALAYEkAAAACL+X8Mb8NcYbO8+AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Calculate and print Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STJb6mX5yF6R",
        "outputId": "b8bb4151-1947-4430-8e3d-c9f38fa46a05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n",
            "Precision: 0.95\n",
            "Recall: 0.99\n",
            "F1-Score: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with class weights to handle imbalanced data\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Calculate and print Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC_1RZMsylO-",
        "outputId": "c5eacb60-347b-4c64-8ec9-043f984588d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.86\n",
            "Precision: 0.41\n",
            "Recall: 0.75\n",
            "F1-Score: 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical features to numeric\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Prepare the data\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Calculate and print Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnLheFF0y7tI",
        "outputId": "61087932-2ea3-4cd1-89fd-0193bbca5568"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.81\n",
            "Precision: 0.79\n",
            "Recall: 0.74\n",
            "F1-Score: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical features to numeric\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Prepare the data\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(solver='liblinear')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f'Accuracy without scaling: {accuracy_no_scaling:.2f}')\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply Logistic Regression with scaling\n",
        "model_with_scaling = LogisticRegression(solver='liblinear')\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f'Accuracy with scaling: {accuracy_with_scaling:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqcHSDpazMhv",
        "outputId": "918d6b4f-dd24-4029-e0ca-1e233a4962d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.79\n",
            "Accuracy with scaling: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')  # good for small datasets\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities\n",
        "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Step 6: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 4))\n",
        "\n",
        "# Step 7: Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # baseline\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "LXjxc9KmznWr",
        "outputId": "fb1ed6ce-ec55-4994-ab6b-eef3d7c69f3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9974\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdqZJREFUeJzt3XlcVNX/P/DXzDAz7IsCsogi7uaCYpgrueKOomhpuVQuqWX5sdJKzRZtMbPFsjKjUsPEJdxFkRL3DXPFDcUFFARF1hlmzu8Pv8yvCVAGZ7gyvJ6PB4+aM3d5zZnBeXPvuefKhBACRERERFZCLnUAIiIiInNicUNERERWhcUNERERWRUWN0RERGRVWNwQERGRVWFxQ0RERFaFxQ0RERFZFRY3REREZFVY3BAREZFVYXFDRFZPJpPhvffeM8u2Ll++DJlMhsjISLNsj4D4+HjIZDLEx8dLHYWsBIsbqvIiIyMhk8kMPzY2NvD19cWYMWNw/fr1UtcRQuC3335Dly5d4OrqCnt7e7Ro0QLvv/8+cnNzy9zXunXr0KdPH7i7u0OlUsHHxwfDhg1DXFxcubIWFBTgiy++QLt27eDi4gJbW1s0atQIU6ZMwblz5yr0+quSMWPGwNHRUeoY5bJy5UosWrTIovsoLpSKf+RyOWrUqIE+ffpg3759Ft03kTWT8d5SVNVFRkZi7NixeP/991GvXj0UFBRg//79iIyMhL+/P06ePAlbW1vD8jqdDiNGjMAff/yBzp07Izw8HPb29ti9ezdWrlyJZs2aYceOHahVq5ZhHSEEXnjhBURGRqJ169YYOnQovLy8kJqainXr1uHIkSPYs2cPOnToUGbOjIwM9O7dG0eOHEH//v3Ro0cPODo6IikpCVFRUUhLS4NGo7FoX0ltzJgxiI6ORk5OTqXut6CgADY2NrCxsSn3Ov3798fJkydx+fJlo3YhBAoLC6FUKqFQKB4p1+XLl1GvXj08++yz6Nu3L3Q6Hc6dO4dvv/0W+fn5OHToEFq0aPFI+6gK9Ho9NBoNVCoV5HL+zU1mIIiquJ9//lkAEIcOHTJqf+uttwQAsWrVKqP2efPmCQBi+vTpJbYVExMj5HK56N27t1H7Z599JgCI1157Tej1+hLr/frrr+LAgQMPzNmvXz8hl8tFdHR0iecKCgrE//73vweuX15arVYUFhaaZVvmNnr0aOHg4CB1jHLp16+fqFu3rkX3kZycLACIzz77zKh9y5YtAoB4+eWXLbr/0uTk5FT6PonMjcUNVXllFTcbN24UAMS8efMMbXl5ecLNzU00atRIaLXaUrc3duxYAUDs27fPsE6NGjVEkyZNRFFRUYUy7t+/XwAQ48aNK9fyISEhIiQkpET76NGjjb5w//3l+MUXX4iAgAAhl8vF/v37hUKhEO+9916JbZw9e1YAEF9//bWhLSsrS0ydOlXUrl1bqFQqUb9+ffHxxx8LnU5n8mt9kPIWN3/88Ydo06aNsLW1FTVr1hQjR44U165dK3W5pk2bCrVaLZ544gmxdu3aEn0khBAAxJw5cwyPs7OzxdSpU0XdunWFSqUSHh4eokePHuLIkSNCiPv9D8Dop3ibxX3+888/G+3jzJkzIiIiQri7uwtbW1vRqFEj8fbbbz/wdZZV3OTk5AgAolevXkbt5X2fMjIyxHPPPSecnJyEi4uLGDVqlEhMTCyRu/j9uHDhgujTp49wdHQUYWFhQgghdDqd+OKLL0SzZs2EWq0Wnp6eYvz48SIzM9NoX4cOHRK9evUSNWvWFLa2tsLf31+MHTvWaJnff/9dtGnTRjg6OgonJyfRvHlzsWjRIsPzu3btEgDErl27jNYrz+eg+DVcu3ZNhIWFCQcHB+Hu7i7+97//Vfj3laq+8h+jJapiik8nuLm5GdoSEhKQlZWFqVOnlnmKYtSoUfj555+xceNGPPXUU0hISEBmZiZee+21Cp+GiImJAQA8//zzFVr/YX7++WcUFBRg/PjxUKvV8Pb2RkhICP744w/MmTPHaNlVq1ZBoVAgIiICAJCXl4eQkBBcv34dEyZMQJ06dbB3717MnDkTqampFh938l/FpxmffPJJzJ8/Hzdv3sSXX36JPXv24NixY3B1dQUAbNq0CcOHD0eLFi0wf/58ZGVl4cUXX4Svr+9D9zFx4kRER0djypQpaNasGW7fvo2EhAScOXMGbdq0wTvvvIO7d+/i2rVr+OKLLwDggWOF/vnnH3Tu3BlKpRLjx4+Hv78/Ll68iA0bNuCjjz4yuQ9K++yW933S6/UYMGAADh48iJdffhlNmjTBn3/+idGjR5e6r6KiIoSGhqJTp05YsGAB7O3tAQATJkwwvBevvvoqkpOT8c033+DYsWPYs2cPlEolbt26hV69esHDwwMzZsyAq6srLl++jLVr1xq2Hxsbi2effRbdu3fHJ598AgA4c+YM9uzZg6lTp5bZB+X9HAD3TzWHhoaiXbt2WLBgAXbs2IHPP/8c9evXx8svv2xy/5MVkLq6InpUxUduduzYIdLT08XVq1dFdHS08PDwEGq1Wly9etWw7KJFiwQAsW7dujK3l5mZKQCI8PBwIYQQX3755UPXeZjBgwcLACIrK6tcy5t65MbZ2VncunXLaNnvv/9eABAnTpwwam/WrJno1q2b4fEHH3wgHBwcxLlz54yWmzFjhlAoFCIlJaVcmcvjYUduNBqN8PT0FM2bNxf5+fmG9uKjcLNnzza0tWjRQtSuXVvcu3fP0BYfH290lKUY/nPkxsXFRUyePPmBWcs6LVXakZsuXboIJycnceXKFaNlSzuFWdq25s6dK9LT00VaWprYvXu3ePLJJwUAsXr1asOy5X2f1qxZIwAYHRnR6XSiW7dupR65ASBmzJhhtM3du3cLAGLFihVG7Vu3bjVqX7duXalHTf9t6tSpwtnZ+YFHUf575MaUz0Hxa3j//feNttm6dWsRFBRU5j7JunHkFlmNHj16wMPDA35+fhg6dCgcHBwQExOD2rVrG5a5d+8eAMDJyanM7RQ/l52dbfTfB63zMObYxoMMGTIEHh4eRm3h4eGwsbHBqlWrDG0nT57E6dOnMXz4cEPb6tWr0blzZ7i5uSEjI8Pw06NHD+h0Ovz9998WyVyaw4cP49atW5g0aZLRIPB+/fqhSZMm2LRpEwDgxo0bOHHiBEaNGmV0RCUkJKRcA3BdXV1x4MAB3Lhx45Ezp6en4++//8YLL7yAOnXqGD0nk8nKtY05c+bAw8MDXl5e6Ny5M86cOYPPP/8cQ4cONSxT3vdp69atUCqVGDdunGFduVyOyZMnl7n//x7dWL16NVxcXNCzZ0+jfQUFBcHR0RG7du0CAMPRk40bN0Kr1Za6bVdXV+Tm5iI2NrZcfQGU/3PwbxMnTjR63LlzZ1y6dKnc+yTrwuKGrMbixYsRGxuL6Oho9O3bFxkZGVCr1UbLFBcXxUVOaf5bADk7Oz90nYcxxzYepF69eiXa3N3d0b17d/zxxx+GtlWrVsHGxgbh4eGGtvPnz2Pr1q3w8PAw+unRowcA4NatW2Xu9+7du0hLSzP8ZGZmPtLruHLlCgCgcePGJZ5r0qSJ4fni/zZo0KDEcqW1/denn36KkydPws/PD8HBwXjvvfcq/EVYvF7z5s0rtD4AjB8/HrGxsdiwYQNef/115OfnQ6fTGS1T3vfpypUr8Pb2NpxeKlZWv9jY2Bj9AVC8r7t378LT07PE/nJycgz7CgkJwZAhQzB37ly4u7sjLCwMP//8MwoLCw3bmjRpEho1aoQ+ffqgdu3aeOGFF7B169YH9kd5PwfFbG1tSxT3bm5uyMrKeuB+yHpxzA1ZjeDgYLRt2xYAMGjQIHTq1AkjRoxAUlKS4a/7pk2bArg/RmLQoEGlbueff/4BADRr1gzA/X9MAeDEiRNlrvMw/95G586dH7q8TCaDKGWWhv9+4RWzs7Mrtf2ZZ57B2LFjkZiYiMDAQPzxxx/o3r073N3dDcvo9Xr07NkTb775ZqnbaNSoUZk5p06dil9++cXwOCQkpEpMxDZs2DB07twZ69atw/bt2/HZZ5/hk08+wdq1a9GnT59Kz9OwYUNDkdK/f38oFArMmDEDXbt2NXymH+V9ehC1Wl3i8mu9Xg9PT0+sWLGi1HWKCwmZTIbo6Gjs378fGzZswLZt2/DCCy/g888/x/79++Ho6AhPT08kJiZi27Zt2LJlC7Zs2YKff/4Zo0aNMvrsPIpHvSSfrA+P3JBVUigUmD9/Pm7cuIFvvvnG0N6pUye4urpi5cqVZRYKv/76K4D7XzLF67i5ueH3338vc52HGTBgAABg+fLl5Vrezc0Nd+7cKdH+379YH2bQoEFQqVRYtWoVEhMTce7cOTzzzDNGy9SvXx85OTno0aNHqT//PdXyb2+++SZiY2MNP59//rlJ+f6rbt26AICkpKQSzyUlJRmeL/7vhQsXSixXWltpvL29MWnSJKxfvx7JycmoWbOm0eDf8p5SCggIAHD/lJ+5vPPOO3BycsK7775raCvv+1S3bl2kpqYiLy/PaJvl7Zfifd2+fRsdO3YsdV+tWrUyWv6pp57CRx99hMOHD2PFihU4deoUoqKiDM+rVCoMGDAA3377LS5evIgJEybg119/LTNTeT8HRGVhcUNW6+mnn0ZwcDAWLVqEgoICAIC9vT2mT5+OpKQkvPPOOyXW2bRpEyIjIxEaGoqnnnrKsM5bb72FM2fO4K233ir1iMry5ctx8ODBMrO0b98evXv3xtKlS7F+/foSz2s0GkyfPt3wuH79+jh79izS09MNbcePH8eePXvK/fqB++MdQkND8ccffyAqKgoqlarE0adhw4Zh37592LZtW4n179y5g6KiojK336xZM6MvvaCgIJPy/Vfbtm3h6emJJUuWGJ3a2LJlC86cOYN+/foBAHx8fNC8eXP8+uuvRhMC/vXXXzhx4sQD96HT6XD37l2jNk9PT/j4+Bjt08HBocRypfHw8ECXLl2wbNkypKSkGD1X2melPFxdXTFhwgRs27YNiYmJAMr/PoWGhkKr1eLHH380PK/X67F48eJy73/YsGHQ6XT44IMPSjxXVFRkKLyzsrJKvMbAwEAAMPTl7du3jZ6Xy+Vo2bKl0TL/Vd7PAVFZeFqKrNobb7yBiIgIREZGGgYczpgxA8eOHcMnn3yCffv2YciQIbCzs0NCQgKWL1+Opk2bljhc/sYbb+DUqVP4/PPPsWvXLsMMxWlpaVi/fj0OHjyIvXv3PjDLr7/+il69eiE8PBwDBgxA9+7d4eDggPPnzyMqKgqpqalYsGABAOCFF17AwoULERoaihdffBG3bt3CkiVL8MQTTxgGJ5fX8OHD8dxzz+Hbb79FaGio0SW0xa8tJiYG/fv3x5gxYxAUFITc3FycOHEC0dHRuHz5stFprEel1Wrx4YcflmivUaMGJk2ahE8++QRjx45FSEgInn32WcMlwP7+/nj99dcNy8+bNw9hYWHo2LEjxo4di6ysLHzzzTdo3rz5A2dAvnfvHmrXro2hQ4eiVatWcHR0xI4dO3Do0CGjI09BQUFYtWoVpk2bhieffBKOjo6GI3D/9dVXX6FTp05o06YNxo8fj3r16uHy5cvYtGmToTgx1dSpU7Fo0SJ8/PHHiIqKKvf7NGjQIAQHB+N///sfLly4gCZNmiAmJsYwHqo8R6RCQkIwYcIEzJ8/H4mJiejVqxeUSiXOnz+P1atX48svv8TQoUPxyy+/4Ntvv8XgwYNRv3593Lt3Dz/++COcnZ3Rt29fAMBLL72EzMxMdOvWDbVr18aVK1fw9ddfIzAw0HCa+L+USmW5PwdEpZL2Yi2iR1fWJH5C3L8Etn79+qJ+/fpGl6LqdDrx888/i44dOwpnZ2dha2srnnjiCTF37twHztAaHR0tevXqJWrUqCFsbGyEt7e3GD58uIiPjy9X1ry8PLFgwQLx5JNPCkdHR6FSqUTDhg3FK6+8Ii5cuGC07PLly0VAQIBQqVQiMDBQbNu27YGT+JUlOztb2NnZCQBi+fLlpS5z7949MXPmTNGgQQOhUqmEu7u76NChg1iwYIHQaDTlem3lUXzZbmk/9evXNyy3atUq0bp1a6FWq0WNGjXKnMQvKipKNGnSRKjVatG8eXMRExMjhgwZIpo0aWK0HP51KXhhYaF44403RKtWrYSTk5NwcHAQrVq1Et9++63ROjk5OWLEiBHC1dW1XJP4nTx5UgwePFi4uroKW1tb0bhxYzFr1qwH9sfD3r8xY8YIhUJh+GyU931KT08XI0aMMEziN2bMGLFnzx4BQERFRRm9Hw+6NP+HH34QQUFBws7OTjg5OYkWLVqIN998U9y4cUMIIcTRo0fFs88+K+rUqWOY6K9///7i8OHDhm0U/854enoKlUol6tSpIyZMmCBSU1MNy5Q1iV95PgdlvYY5c+YIfsVVX7y3FBFZlcDAQHh4eJh06XF1sH79egwePBgJCQno2LGj1HGILIpjboioStJqtSXGA8XHx+P48eN4+umnpQn1mMjPzzd6rNPp8PXXX8PZ2Rlt2rSRKBVR5eGYGyKqkq5fv44ePXrgueeeg4+PD86ePYslS5bAy8urxIRu1c0rr7yC/Px8tG/fHoWFhVi7di327t2LefPmlTltAJE14WkpIqqS7t69i/Hjx2PPnj1IT0+Hg4MDunfvjo8//hj169eXOp6kVq5cic8//xwXLlxAQUEBGjRogJdffhlTpkyROhpRpWBxQ0RERFaFY26IiIjIqrC4ISIiIqtS7QYU6/V63LhxA05OTuWeXp2IiIikJYTAvXv34OPjU+J+aP9V7YqbGzduwM/PT+oYREREVAFXr14tcSf7/6p2xY2TkxOA+53j7Oxs1m1rtVps377dMFU5WQb7uXKwnysH+7nysK8rh6X6OTs7G35+fobv8QepdsVN8akoZ2dnixQ39vb2cHZ25i+OBbGfKwf7uXKwnysP+7pyWLqfyzOkhAOKiYiIyKqwuCEiIiKrwuKGiIiIrAqLGyIiIrIqLG6IiIjIqrC4ISIiIqvC4oaIiIisCosbIiIisiosboiIiMiqsLghIiIiqyJpcfP3339jwIAB8PHxgUwmw/r16x+6Tnx8PNq0aQO1Wo0GDRogMjLS4jmJiIio6pC0uMnNzUWrVq2wePHici2fnJyMfv36oWvXrkhMTMRrr72Gl156Cdu2bbNwUiIiIqoqJL1xZp8+fdCnT59yL79kyRLUq1cPn3/+OQCgadOmSEhIwBdffIHQ0FBLxZSUEAL5Wp3UMR4rWm0RCnVAnqYISvHwG6hRxbCfKwf7ufKwrytHcT8LISTLUKXuCr5v3z706NHDqC00NBSvvfZamesUFhaisLDQ8Dg7OxvA/buWarVas+Yr3p65tiuEwDNLD+Foyh2zbM+62ODNg3FSh6gG2M+Vg/1cedjXlcMG3boVwqUcd/AuL1O+W6tUcZOWloZatWoZtdWqVQvZ2dnIz8+HnZ1diXXmz5+PuXPnlmjfvn077O3tLZIzNjbWLNsp1AFHU6rUW0RERNWQGlrIABRAaWiLi4uDWmG+feTl5ZV7Wav/5pw5cyamTZtmeJydnQ0/Pz/06tULzs7OZt2XVqtFbGwsevbsCaVS+fAVHiJPU2T4C2P/WyGwU5nxU1KFabVFiIuLQ7du3aBUWv1HWDLs58rBfq487GvLuHb1KrZs2oAaNWpi8NAI6HR6xMXFoV9oD6hUKrPtp/jMS3lUqXfXy8sLN2/eNGq7efMmnJ2dSz1qAwBqtRpqtbpEu1KpNEsBUhpzbfvf54SdHWxhr6pSb5fFaLVaqBWAi4Otxd5DYj9XFvZz5WFfm5cQArt370Z8fDyEELCztYUN9HB0sIVaAahUKrP2synbqlLflu3bt8fmzZuN2mJjY9G+fXuJElVMeQcJ52k4kJiIiB4/OTk5WLduHS5dugQAaNWqFfr27QuVSmX28awVIWlxk5OTgwsXLhgeJycnIzExETVq1ECdOnUwc+ZMXL9+Hb/++isAYOLEifjmm2/w5ptv4oUXXkBcXBz++OMPbNq0SaqXYDIhBIYu2YcjV7KkjkJERGSy5ORkrF27Fjk5OVAqlejbty8CAwOljmVE0uLm8OHD6Nq1q+Fx8diY0aNHIzIyEqmpqUhJSTE8X69ePWzatAmvv/46vvzyS9SuXRtLly6tUpeB52t1Jhc2beu6wU7J8TZERCQtvV6PzZs3IycnBx4eHoiIiICHh4fUsUqQtLh5+umnH3gdfGmzDz/99NM4duyYBVNVnsPv9oB9OQYJ2ykVkJnxcjoiIqKKkMvlGDJkCA4fPozQ0NDHduxSlRpzY23sVQoOEiYiosfaxYsXcefOHQQFBQG4f3FP//79JU71YPxmJSIiohL0ej127dqFhIQEyOVy+Pj4wNvbW+pY5cLihoiIiIxkZ2djzZo1hnGvrVu3fizH1pSFxQ0REREZnD9/HuvWrUN+fj5UKhUGDhyIJ554QupYJmFxQ0RERACAnTt3IiEhAQDg7e2NoUOHokaNGhKnMh2LGyIiIgIAw2z/wcHB6NmzJ2xsqmaZUDVTExERkVloNBrDPaDat2+P2rVro06dOhKnejRyqQMQERFR5dPpdNi6dSt+/PFHaDQaAIBMJqvyhQ3AIzdERETVTlZWFqKjo3Hjxg0AQFJSElq0aCFxKvNhcUNERFSNnD59GjExMSgsLIStrS0GDRqExo0bSx3LrFjcEBERVQNFRUXYvn07Dh06BADw8/PDkCFD4OLiInEy82NxQ0REVA38u7Dp2LEjunbtCoXCOm/KzOKGiIioGujSpQuuXLmCnj17okGDBlLHsSheLUVERGSFtFotTpw4YXjs6OiIiRMnWn1hA/DIDRERkdXJyMjA6tWrcevWLcjlcsPtE2QymcTJKgeLGyIiIity/PhxbNq0CVqtFg4ODoZZh6sTFjdERERWQKPRYMuWLUhMTAQA1KtXD4MHD4aTk5O0wSTA4oaIiKiKu3XrFqKjo5Geng6ZTIaQkBB07twZcnn1HFrL4oaIiKiKy8rKQnp6OhwdHTFkyBD4+/tLHUlSLG6IiIiqICGEYYBw48aNMWDAADRu3BgODg4SJ5Ne9TxeRUREVIWlpaXh559/xt27dw1tbdq0YWHzf1jcEBERVRFCCBw+fBhLly7F1atXsX37dqkjPZZ4WoqIiKgKKCwsxIYNG3Dq1CkAQMOGDdGvXz+JUz2eWNwQERE95lJTUxEdHY3MzEzI5XJ0794d7du3rzaT8pmKxY0ZCSFQqAPyNEVQitI/cHkaXSWnIiKiqiw5ORkrVqyATqeDi4sLhg4ditq1a0sd67HG4sZMhBB4ZukhHE2xwZsH46SOQ0REVqJ27dqoWbMm3NzcEBYWVi1nHDYVixszydfqcDTlTrmXb1vXDXZK67zVPBERPZpbt27B3d0dcrkcSqUSo0ePhp2dHU9DlROLGwvY/1YInB1sH7iMnVLBDykRERkRQmD//v3YsWMHQkJC0KVLFwCAvb29xMmqFhY3FmCnUsBexa4lIqLyy8/Px/r163Hu3DkA94/e/HuiPio/fgMTERFJ7OrVq4iOjkZ2djYUCgVCQ0PRtm1bFjYVxOKGiIhIIkII7N27Fzt37oQQAjVq1MDQoUPh7e0tdbQqjcUNERGRRDIzM7Fr1y4IIdC8eXP0798farVa6lhVHosbIiIiidSsWRN9+/aFEAJt2rThaSgzYXFDRERUSYQQSEhIQEBAAHx9fQHcv+ElmRdvnElERFQJcnJysHz5csTFxSE6OhoajUbqSFaLR26IiIgsLDk5GWvXrkVOTg5sbGwQEhIClUoldSyrxeKGiIjIQvR6Pf7++2/89ddfAAAPDw9ERETAw8ND4mTWjcUNERGRBRQWFiIqKgqXL18GAAQGBqJv375QKpXSBqsGWNwQERFZgEqlglKphFKpRP/+/dGyZUupI1UbLG6IiIjMRK/XQ6fTQalUQiaTYdCgQcjLy4O7u7vU0aoVXi1FRERkBtnZ2fjll1+wadMmQ5u9vT0LGwmwuCEiInpE58+fx5IlS5CSkoIzZ87gzp07Ukeq1nhaioiIqIJ0Oh3i4uKwd+9eAIC3tzeGDh0KV1dXaYNVcyxuiIiIKuDu3buIjo7GtWvXAADBwcHo2bMnbGz41So1vgNEREQmEkJg+fLlyMjIgFqtRlhYGJo2bSp1LPo/LG6IiIhMJJPJ0Lt3b8THxyM8PBxubm5SR6J/YXFDRERUDllZWcjMzET9+vUBAPXr10dAQADv5P0YYnFDRET0EKdPn0ZMTAwAYPz48ahRowYAsLB5TLG4ISIiKkNRURG2b9+OQ4cOAQBq164NhUIhcSp6GBY3REREpbh9+zaio6ORlpYGAOjQoQO6devG4qYKYHFDRET0HydPnsSGDRug0WhgZ2eHwYMHo2HDhlLHonJicUNERPQf165dg0ajQZ06dTBkyBA4OztLHYlMwOKGiIgI9+euKR4g3LNnT9SoUQNt27aFXM47FVU1fMeIiKja++eff7By5Uro9XoAgEKhQHBwMAubKopHboiIqNrSaDTYsmULEhMTAQDHjh1DUFCQtKHokbG4ISKiaunWrVuIjo5Geno6ACAkJAStW7eWOBWZg+TH2xYvXgx/f3/Y2tqiXbt2OHjw4AOXX7RoERo3bgw7Ozv4+fnh9ddfR0FBQSWlJSKiqk4IgWPHjuHHH39Eeno6HB0dMWrUKDz99NM8DWUlJD1ys2rVKkybNg1LlixBu3btsGjRIoSGhiIpKQmenp4lll+5ciVmzJiBZcuWoUOHDjh37hzGjBkDmUyGhQsXSvAKiIioqtm9ezcSEhIAAAEBAQgPD4eDg4PEqcicJC1RFy5ciHHjxmHs2LFo1qwZlixZAnt7eyxbtqzU5ffu3YuOHTtixIgR8Pf3R69evfDss88+9GgPERFRsWbNmkGtVqNbt2547rnnWNhYIcmO3Gg0Ghw5cgQzZ840tMnlcvTo0QP79u0rdZ0OHTpg+fLlOHjwIIKDg3Hp0iVs3rwZzz//fJn7KSwsRGFhoeFxdnY2AECr1UKr1Zrp1QBabZHR/5tz22SsuG/Zx5bFfq4c7GfLE0Lg1q1bhvtBubi4YNKkSbCzs0NRUdFD1iZTWeozbcr2JCtuMjIyoNPpUKtWLaP2WrVq4ezZs6WuM2LECGRkZKBTp04QQqCoqAgTJ07E22+/XeZ+5s+fj7lz55Zo3759O+zt7R/tRfxLoQ4o7s64uDioOTu3xcXGxkodoVpgP1cO9rNl6HQ6XL16FXfu3EGDBg3g6OjIvq4k5u7nvLy8ci9bpa6Wio+Px7x58/Dtt9+iXbt2uHDhAqZOnYoPPvgAs2bNKnWdmTNnYtq0aYbH2dnZ8PPzQ69evcw642SepghvHowDAHTr1g0uDrZm2zYZ02q1iI2NRc+ePaFUKqWOY7XYz5WD/Ww5aWlpWLduHe7cuQOZTIaAgADcunWLfW1hlvpMF595KQ/Jiht3d3coFArcvHnTqP3mzZvw8vIqdZ1Zs2bh+eefx0svvQQAaNGiBXJzczF+/Hi88847pY5yV6vVUKvVJdqVSqVZO10p/v9t75VKG/7iVAJzv4dUOvZz5WA/m48QAocOHcL27duh0+ng4uKCIUOGwMvLC5s3b2ZfVxKzf8+asC3JBhSrVCoEBQVh586dhja9Xo+dO3eiffv2pa6Tl5dXooApvjurEMJyYYmIqEooKCjA6tWrsWXLFuh0OjRu3BgTJkyAn5+f1NGoEkl6WmratGkYPXo02rZti+DgYCxatAi5ubkYO3YsAGDUqFHw9fXF/PnzAQADBgzAwoUL0bp1a8NpqVmzZmHAgAG8BT0REeHs2bM4c+YM5HI5evbsiXbt2hnuF0XVh6TFzfDhw5Geno7Zs2cjLS0NgYGB2Lp1q2GQcUpKitGRmnfffRcymQzvvvsurl+/Dg8PDwwYMAAfffSRVC+BiIgeI61atcLNmzfRvHlz+Pr6Sh2HJCL5gOIpU6ZgypQppT4XHx9v9NjGxgZz5szBnDlzKiEZERE97vLz8xEXF4fu3bvD1tYWMpkMoaGhUsciiUle3BAREVXE1atXsWbNGty9exeFhYUIDw+XOhI9JljcEBFRlSKEwN69exEXFwe9Xg83N7cyL0Sh6onFDRERVRl5eXlYv349zp8/DwB44oknMGDAgFKn/KDqi8UNERFVCWlpaVi5ciXu3bsHhUKBPn36oE2bNrwaikpgcUNERFVC8azyNWvWRERERInb9xAVY3FDRESPrcLCQsMpJ3t7ezz33HNwdXWFSqWSOBk9ziSboZiIiOhBkpOT8c033yAxMdHQ5unpycKGHorFDRERPVb0ej3i4+Px22+/IScnB4cOHeItdsgkPC1FRESPjXv37mHdunVITk4GAAQGBqJPnz4cNEwmYXFDRESPhYsXL2LdunXIzc2FUqlEv3790KpVK6ljURXE4oaIiCSXlZWFFStWQAgBT09PREREwN3dXepYVEWxuCEiIsm5ubmhY8eOyM/PR2hoKJRKpdSRqApjcUNERJI4f/483N3d4ebmBgDo1q0bx9aQWfBqKSIiqlQ6nQ6xsbFYuXIloqOjodPpAICFDZkNj9wQEVGluXv3LqKjo3Ht2jUAgK+vLy/zJrNjcUNERJUiKSkJ69evR0FBAdRqNQYOHIhmzZpJHYusEIsbIiKyKJ1Ohx07dmD//v0AAB8fHwwdOtQw1obI3FjcEBGRRQkhcOXKFQBAu3bt0LNnTygUColTkTVjcUNERBYhhIBMJoONjQ0iIiJw8+ZNNGnSROpYVA2wuCEiIrMqKirC9u3bYWtri27dugG4P48NT0NRZWFxQ0REZpOZmYno6GikpqZCJpMhMDAQNWrUkDoWVTMsboiIyCxOnTqFmJgYaDQa2NnZYdCgQSxsSBIsboiI6JFotVps27YNR44cAQDUqVMHQ4YMgbOzs8TJqLpicUNERBUmhMBvv/2Gq1evAgA6deqErl27Qi7nBPgkHRY3RERUYTKZDG3atMHt27cRHh6O+vXrSx2JiMUNERGZRqvV4s6dO/Dw8AAABAYGonHjxrCzs5M4GdF9PG5IRETllp6ejh9//BHLly9HXl6eoZ2FDT1OeOSGiIjKJTExEZs2bUJRUREcHR1x584d2NvbSx2LqAQWN0RE9EAajQabN2/G8ePHAQABAQEYPHgwHB0dJU5GVDoWN0REVKabN28iOjoaGRkZkMlkePrpp9G5c2fIZDKpoxGVicUNERGVac+ePcjIyICTkxOGDBmCunXrSh2J6KFY3BARUZn69u0LGxsbdO/eHQ4ODlLHISoXXi1FREQGqamp2L59O4QQAABbW1sMHDiQhQ1VKY905KagoAC2trbmykJERBIRQuDw4cPYtm0bdDodPDw80Lp1a6ljEVWIyUdu9Ho9PvjgA/j6+sLR0RGXLl0CAMyaNQs//fST2QMSEZFlFRQUIDo6Gps3b4ZOp0OjRo3QpEkTqWMRVZjJxc2HH36IyMhIfPrpp1CpVIb25s2bY+nSpWYNR0RElnX9+nV8//33OH36NORyOXr16oVnnnmGk/JRlWbyaalff/0VP/zwA7p3746JEyca2lu1aoWzZ8+aNRwREVnOsWPHsHHjRuj1eri6umLo0KHw9fWVOhbRIzO5uLl+/ToaNGhQol2v10Or1ZolFBERWV6NGjUghEDTpk0xcOBAjqEkq2FycdOsWTPs3r27xFwH0dHRHHxGRPSY+/eFIHXr1sVLL70Eb29vTspHVsXk4mb27NkYPXo0rl+/Dr1ej7Vr1yIpKQm//vorNm7caImMRET0iIQQ2LdvH3bv3o0XX3wR7u7uAAAfHx+JkxGZn8kDisPCwrBhwwbs2LEDDg4OmD17Ns6cOYMNGzagZ8+elshIRESPIC8vD7///jtiY2NRUFBguEcUkbWq0Dw3nTt3RmxsrLmzEBGRmaWkpGDNmjXIzs6GQqFA7969ERQUJHUsIosy+chNQEAAbt++XaL9zp07CAgIMEsoIiJ6NEII7N69G5GRkcjOzkbNmjXx0ksvoW3bthxfQ1bP5CM3ly9fhk6nK9FeWFiI69evmyUUERE9msTERMTFxQEAWrZsiX79+hnNTUZkzcpd3MTExBj+f9u2bXBxcTE81ul02LlzJ/z9/c0ajoiIKqZVq1Y4efIkmjdvjsDAQB6toWql3MXNoEGDAAAymQyjR482ek6pVMLf3x+ff/65WcMREVH56PV6HDt2DIGBgVAoFJDL5XjuuedY1FC1VO7iRq/XAwDq1auHQ4cOGS4jJCIiaeXk5GDt2rVITk5GRkYGQkNDAYCFDVVbJo+5SU5OtkQOIiKqgEuXLmHt2rXIzc2FUqmEl5eX1JGIJFehS8Fzc3Px119/ISUlBRqNxui5V1991SzBiIiobHq9HvHx8di9ezcAwNPTExERETyqToQKFDfHjh1D3759kZeXh9zcXNSoUQMZGRmwt7eHp6cnixsiIgvLzs7G2rVrceXKFQBAmzZt0Lt3byiVSomTET0eTJ7n5vXXX8eAAQOQlZUFOzs77N+/H1euXEFQUBAWLFhgiYxERPQvRUVFSE1NhUqlQnh4OAYMGMDChuhfTD5yk5iYiO+//x5yuRwKhQKFhYUICAjAp59+itGjRyM8PNwSOYmIqjUhhGGAcI0aNRAREQE3NzfUrFlT4mREjx+Tj9wolUrI5fdX8/T0REpKCgDAxcUFV69eNW86IiLC3bt3ERkZiUuXLhnaGjRowMKGqAwmFzetW7fGoUOHAAAhISGYPXs2VqxYgddeew3Nmzc3e0AiouosKSkJ33//PVJSUrB582bDtBxEVDaTi5t58+bB29sbAPDRRx/Bzc0NL7/8MtLT0/H999+bPSARUXWk0+mwbds2REVFIT8/Hz4+Phg5cqThyDkRlc3kMTdt27Y1/L+npye2bt1q1kBERNXdnTt3EB0dbbhfX7t27dCjRw/Y2FRo9g6iasdsfwIcPXoU/fv3N3m9xYsXw9/fH7a2tmjXrh0OHjz4wOXv3LmDyZMnw9vbG2q1Go0aNcLmzZsrGpuI6LFy9+5dfP/997h+/TpsbW0xfPhw9O7dm4UNkQlM+m3Ztm0bYmNjoVKp8NJLLyEgIABnz57FjBkzsGHDBsOU3+W1atUqTJs2DUuWLEG7du2waNEihIaGIikpCZ6eniWW12g06NmzJzw9PREdHQ1fX19cuXIFrq6uJu2XiOhx5ezsjEaNGiEzMxNDhgzhv29EFVDu4uann37CuHHjUKNGDWRlZWHp0qVYuHAhXnnlFQwfPhwnT55E06ZNTdr5woULMW7cOIwdOxYAsGTJEmzatAnLli3DjBkzSiy/bNkyZGZmYu/evYY5HXgnciKq6goLC5GXlwcXFxfIZDL079/fMN0GEZmu3MXNl19+iU8++QRvvPEG1qxZg4iICHz77bc4ceIEateubfKONRoNjhw5gpkzZxra5HI5evTogX379pW6TkxMDNq3b4/Jkyfjzz//hIeHB0aMGIG33nqrzH8ECgsLUVhYaHicnZ0NANBqtdBqtSbnLotWW2T0/+bcNhkr7lv2sWWxnyvHiRMnkJSUBI1Gg2HDhhnmstHr9bwyysz4ma4clupnU7ZX7uLm4sWLiIiIAACEh4fDxsYGn332WYUKGwDIyMiATqdDrVq1jNpr1aqFs2fPlrrOpUuXEBcXh5EjR2Lz5s24cOECJk2aBK1Wizlz5pS6zvz58zF37twS7du3b4e9vX2FspemUAcUd2dcXBzU/IPL4mJjY6WOUC2wny1Dr9fj+vXruH37NgAgLS0NGzdu5NGaSsDPdOUwdz/n5eWVe9lyFzf5+fmGYkAmk0GtVhsuCa8ser0enp6e+OGHH6BQKBAUFITr16/js88+K7O4mTlzJqZNm2Z4nJ2dDT8/P/Tq1QvOzs5my5anKcKbB+MAAN26dYOLg63Ztk3GtFotYmNj0bNnT045b0HsZ8u5ffs21q1bZyhsPD098fzzz0OtVkuczLrxM105LNXPxWdeysOkAcVLly6Fo6MjgPv3NomMjCxxB9ry3jjT3d0dCoUCN2/eNGq/efMmvLy8Sl3H29sbSqXS6C+bpk2bIi0tDRqNBiqVqsQ6arW61H8wlEqlWTtdKWT/2rYNf3EqgbnfQyod+9m8/vnnH2zcuBFarRb29vYYOHAgzp49C7VazX6uJPxMVw6zf8+asK1yFzd16tTBjz/+aHjs5eWF3377zWgZmUxW7uJGpVIhKCgIO3fuxKBBgwDcPzKzc+dOTJkypdR1OnbsiJUrV0Kv1xsmsjp37hy8vb1LLWyIiB4nWq0Wu3btglarhb+/P8LDw2Fra1vmqXgiqphyFzeXL182+86nTZuG0aNHo23btggODsaiRYuQm5truHpq1KhR8PX1xfz58wEAL7/8Mr755htMnToVr7zyCs6fP4958+aVu6AiIpKSUqnE0KFDcf78eXTp0gVyuZyDW4ksQNJZoYYPH4709HTMnj0baWlpCAwMxNatWw2DjFNSUoymGvfz88O2bdvw+uuvo2XLlvD19cXUqVPx1ltvSfUSiIgeKDExEUIItG7dGgDg6+sLX19fiVMRWTfJp7ycMmVKmaeh4uPjS7S1b98e+/fvt3AqIqJHo9FosHnzZhw/fhwKhQJ16tThXbyJKonkxQ0RkbW5efMmoqOjkZGRAZlMhi5dusDNzU3qWETVBosbIiIzEULg2LFj2LJlC4qKiuDk5ITw8HDOpE5UyVjcEBGZgRAC69evxz///AMAaNCgAQYNGgQHBweJkxFVPxW6K/jFixfx7rvv4tlnn8WtW7cAAFu2bMGpU6fMGo6IqKqQyWSoUaMGZDIZunfvjhEjRrCwIZKIycXNX3/9hRYtWuDAgQNYu3YtcnJyAADHjx8vc5ZgIiJrJIRAfn6+4XHnzp0xfvx4dOrUyXCPKCKqfCYXNzNmzMCHH36I2NhYo4nzunXrxquYiKjaKCgoQHR0NH755RfDXDVyubzMGdaJqPKYPObmxIkTWLlyZYl2T09PZGRkmCUUEdHj7MaNG4iOjkZWVhbkcjmuXr2KgIAAqWMR0f8xubhxdXVFamoq6tWrZ9R+7NgxTkxFRFZNCIGDBw9i+/bt0Ov1cHFxwdChQ1G7dm2poxHRv5hc3DzzzDN46623sHr1ashkMuj1euzZswfTp0/HqFGjLJGRiEhy+fn5iImJMdwHqkmTJhg4cCDs7OwkTkZE/2VycTNv3jxMnjwZfn5+0Ol0aNasGXQ6HUaMGIF3333XEhmJiCS3efNmnD17FgqFAj179kRwcDAHDRM9pkwublQqFX788UfMmjULJ0+eRE5ODlq3bo2GDRtaIh8R0WOhR48eyMzMRL9+/eDj4yN1HCJ6AJOLm4SEBHTq1Al16tRBnTp1LJGJiEhyeXl5OHfuHAIDAwEALi4ueOmll3i0hqgKMLm46datG3x9ffHss8/iueeeQ7NmzSyRi4hIMikpKVizZg2ys7NhZ2eHxo0bAwALG6IqwuR5bm7cuIH//e9/+Ouvv9C8eXMEBgbis88+w7Vr1yyRj4io0gghkJCQgMjISGRnZ6NGjRpwcXGROhYRmcjk4sbd3R1TpkzBnj17cPHiRUREROCXX36Bv78/unXrZomMREQWl5ubixUrVmDnzp0QQqBFixYYP348J+UjqoIe6caZ9erVw4wZM9CqVSvMmjULf/31l7lyERFVmsuXL2PNmjXIycmBjY0N+vTpg9atW/M0FFEVVeHiZs+ePVixYgWio6NRUFCAsLAwzJ8/35zZiIgqRU5ODnJycuDu7o6IiAh4enpKHYmIHoHJxc3MmTMRFRWFGzduoGfPnvjyyy8RFhYGe3t7S+QjIrIIIYThyEzz5s2h0+nQtGlTo3vmEVHVZHJx8/fff+ONN97AsGHD4O7ubolMREQWdenSJcTGxmLkyJFwdHQEALRq1UriVERkLiYXN3v27LFEDiIii9Pr9fjrr7/w999/AwDi4+PRv39/iVMRkbmVq7iJiYlBnz59oFQqERMT88BlBw4caJZgRETmdO/ePaxZswZXrlwBALRu3RqhoaESpyIiSyhXcTNo0CCkpaXB09MTgwYNKnM5mUwGnU5nrmxERGZx4cIFrFu3Dnl5eVCpVOjfvz9atGghdSwispByFTd6vb7U/ycietydOnUK0dHRAIBatWohIiICNWvWlDgVEVmSyZP4/frrrygsLCzRrtFo8Ouvv5olFBGRuTRo0AA1a9ZE27Zt8dJLL7GwIaoGTC5uxo4di7t375Zov3fvHsaOHWuWUEREj+LatWsQQgAA1Go1xo0bh379+sHG5pHmLSWiKsLk4ubfc0P827Vr13gPFiKSlE6nw/bt2/HTTz9h//79hna1Wi1hKiKqbOX+M6Z4KnKZTIbu3bsb/QWk0+mQnJyM3r17WyQkEdHD3LlzB9HR0bh+/TqA+0eTiah6KndxU3yVVGJiIkJDQw0TXwGASqWCv78/hgwZYvaAREQPc/bsWfz5558oKCiAra0twsLC0KRJE6ljEZFEyl3czJkzBwDg7++P4cOHw9bW1mKhiIjKo6ioCLGxsTh48CAAwNfXF0OHDoWrq6u0wYhIUiaPrhs9erQlchARmSw9PR2HDx8GALRv3x7du3eHQqGQOBURSa1cxU2NGjVw7tw5uLu7w83NrdQBxcUyMzPNFo6I6EG8vb3Rp08fODs7o1GjRlLHIaLHRLmKmy+++AJOTk6G/39QcUNEZCnFp6HatGmDWrVqAQDatm0rcSoietyUq7j596moMWPGWCoLEVGZbt++jdWrV+PmzZu4dOkSXn75ZcjlJs9mQUTVgMn/Mhw9ehQnTpwwPP7zzz8xaNAgvP3229BoNGYNR0QEACdOnMAPP/yAmzdvwt7eHqGhoSxsiKhMJv/rMGHCBJw7dw4AcOnSJQwfPhz29vZYvXo13nzzTbMHJKLqS6vVIiYmBmvXroVGo0HdunUxceJENGjQQOpoRPQYM7m4OXfuHAIDAwEAq1evRkhICFauXInIyEisWbPG3PmIqJrKycnB0qVLcezYMQBAly5dMGrUKMP4PyKisph8KbgQwnBn8B07dqB///4AAD8/P2RkZJg3HRFVW/b29nBwcICDgwPCw8MREBAgdSQiqiJMLm7atm2LDz/8ED169MBff/2F7777DgCQnJxsuHqBiKgiNBoN5HI5bGxsIJfLER4eDgBGM6ITET2MyaelFi1ahKNHj2LKlCl45513DOe+o6Oj0aFDB7MHJKLq4datW/jxxx+xdetWQ5ujoyMLGyIymclHblq2bGl0tVSxzz77jDODEpHJhBA4duwYtmzZgqKiIhQWFiIvLw/29vZSRyOiKsrk4qbYkSNHcObMGQBAs2bN0KZNG7OFIqLqobCwEJs2bTL8wVS/fn0MHjyYhQ0RPRKTi5tbt25h+PDh+Ouvvww3p7tz5w66du2KqKgoeHh4mDsjEVmhtLQ0REdH4/bt25DJZOjWrRs6duzIGdCJ6JGZPObmlVdeQU5ODk6dOoXMzExkZmbi5MmTyM7OxquvvmqJjERkZYqKirBy5Urcvn0bzs7OGDNmDDp16sTChojMwuQjN1u3bsWOHTvQtGlTQ1uzZs2wePFi9OrVy6zhiMg62djYoF+/fjh69CjCwsJ4GoqIzMrk4kav10OpVJZoVyqVhvlviIj+68aNGygoKDDMV9O4cWM0atSIR2uIyOxMPi3VrVs3TJ06FTdu3DC0Xb9+Ha+//jq6d+9u1nBEVPUJIXDgwAEsW7YM0dHRuHv3ruE5FjZEZAkmH7n55ptvMHDgQPj7+8PPzw8AcPXqVTRv3hzLly83e0Aiqrry8/MRExODs2fPAgDq1q0LlUolcSoisnYmFzd+fn44evQodu7cabgUvGnTpujRo4fZwxFR1XXt2jWsWbMGd+7cgUKhQM+ePREcHMyjNURkcSYVN6tWrUJMTAw0Gg26d++OV155xVK5iKiKEkJg//792LFjB/R6Pdzc3DB06FD4+PhIHY2IqolyFzffffcdJk+ejIYNG8LOzg5r167FxYsX8dlnn1kyHxFVMTKZDBkZGdDr9WjWrBkGDBgAW1tbqWMRUTVS7gHF33zzDebMmYOkpCQkJibil19+wbfffmvJbERUhQghDP/fu3dvDB48GEOHDmVhQ0SVrtzFzaVLlzB69GjD4xEjRqCoqAipqakWCUZEVYMQAgkJCVi5cqWhwFEqlWjZsiXH1xCRJMp9WqqwsBAODg6Gx3K5HCqVCvn5+RYJRkSPv9zcXKxfvx4XLlwAAJw9e9Zogk8iIimYNKB41qxZRjOJajQafPTRR3BxcTG0LVy40HzpiOixdeXKFaxZswb37t2DjY0N+vTpgyZNmkgdi4io/MVNly5dkJSUZNTWoUMHXLp0yfCYh6CJrJ9er0dCQgLi4+MhhIC7uzsiIiLg6ekpdTQiIgAmFDfx8fEWjEFEVcWmTZtw9OhRAEBgYCD69OnDifmI6LFi8u0XLGHx4sXw9/eHra0t2rVrh4MHD5ZrvaioKMhkMgwaNMiyAYnI4Mknn4SdnR0GDRqEsLAwFjZE9NiRvLhZtWoVpk2bhjlz5uDo0aNo1aoVQkNDcevWrQeud/nyZUyfPh2dO3eupKRE1ZNer8fVq1cNj728vPDaa6+hVatWEqYiIiqb5MXNwoULMW7cOIwdOxbNmjXDkiVLYG9vj2XLlpW5jk6nw8iRIzF37lzDHYaJyPy0Wi1WrlyJyMhIXL9+3dDOozVE9DiTtLjRaDQ4cuSI0X2p5HI5evTogX379pW53vvvvw9PT0+8+OKLlRGTqFq6dOkSkpKSkJKSAhsbG9y7d0/qSERE5WLyjTPNKSMjAzqdDrVq1TJqr1WrluEuwv+VkJCAn376CYmJieXaR2FhIQoLCw2Ps7OzAdz/i1Sr1VYseCm02iKj/zfntslYcd+yjy1Dr9fjr7/+MvyB4eHhgfDwcNSsWZN9bgH8PFce9nXlsFQ/m7K9ChU3u3fvxvfff4+LFy8iOjoavr6++O2331CvXj106tSpIpssl3v37uH555/Hjz/+CHd393KtM3/+fMydO7dE+/bt243m7HlUhTqguDvj4uKgVpht01SG2NhYqSNYHY1GgytXriA3NxcA4O7uDm9vbxw4cEDiZNaPn+fKw76uHObu57y8vHIva3Jxs2bNGjz//PMYOXIkjh07ZjgqcvfuXcybNw+bN28u97bc3d2hUChw8+ZNo/abN2/Cy8urxPIXL17E5cuXMWDAAEObXq+//0JsbJCUlIT69esbrTNz5kxMmzbN8Dg7Oxt+fn7o1asXnJ2dy531YfI0RXjzYBwAoFu3bnBx4P10LEWr1SI2NhY9e/aEUqmUOo5VOXjwIE6fPg21Wo3Q0FCkpKSwny2Mn+fKw76uHJbq5+IzL+VhcnHz4YcfYsmSJRg1ahSioqIM7R07dsSHH35o0rZUKhWCgoKwc+dOw+Xcer0eO3fuxJQpU0os36RJE5w4ccKo7d1338W9e/fw5Zdfws/Pr8Q6arUaarW6RLtSqTRrpyvF/5/AUKm04S9OJTD3e0j3J+bMy8tDUFAQnJyckJKSwn6uJOznysO+rhxm/541YVsmFzdJSUno0qVLiXYXFxfcuXPH1M1h2rRpGD16NNq2bYvg4GAsWrQIubm5GDt2LABg1KhR8PX1xfz582Fra4vmzZsbre/q6goAJdqJ6OHu3LmDXbt2oV+/flCpVJDJZOjZsycAjksgoqrL5OLGy8sLFy5cgL+/v1F7QkJChS7LHj58ONLT0zF79mykpaUhMDAQW7duNQwyTklJgVwu+RXrRFbn7Nmz+PPPP1FQUACVSoV+/fpJHYmIyCxMLm7GjRuHqVOnYtmyZZDJZLhx4wb27duH6dOnY9asWRUKMWXKlFJPQwEPv+1DZGRkhfZJVF3pdDrExsYaBgn7+vqiY8eOEqciIjIfk4ubGTNmQK/Xo3v37sjLy0OXLl2gVqsxffp0vPLKK5bISERmkpWVhejoaNy4cQMA0L59e3Tv3h0KBS/vIyLrYXJxI5PJ8M477+CNN97AhQsXkJOTg2bNmsHR0dES+YjITC5fvoyoqCgUFhYa7g3VqFEjqWMREZldhSfxU6lUaNasmTmzEJEF1axZEzY2NvD09MSQIUPg4uIidSQiIoswubjp2rUrZDJZmc/HxcU9UiAiMp+8vDzDZJVOTk4YM2YM3NzceBqKiKyaycVNYGCg0WOtVovExEScPHkSo0ePNlcuInpEJ06cwMaNGxEWFmY4ylremb2JiKoyk4ubL774otT29957Dzk5OY8ciIgejVarxdatW3H06FEAwPHjx3kKmYiqFbNNIPPcc89h2bJl5tocEVVARkYGli5daihsunTpguHDh0ucioiocpntruD79u2DrS3vp0QklePHj2PTpk3QarVwcHBAeHh4hSbWJCKq6kwubsLDw40eCyGQmpqKw4cPV3gSPyJ6NKmpqVi/fj0AoF69eggPD+f0DERUbZlc3Pz38lG5XI7GjRvj/fffR69evcwWjIjKz9vbG+3bt4darUbnzp15yxIiqtZMKm50Oh3Gjh2LFi1awM3NzVKZiOghhBA4fvw4AgIC4OzsDAD844KI6P+Y9OedQqFAr169KnT3byIyj8LCQqxbtw5//vkn1qxZA71eL3UkIqLHismnpZo3b45Lly6hXr16lshDRA+QlpaG6Oho3L59GzKZDA0bNnzgpJpERNWRycXNhx9+iOnTp+ODDz5AUFAQHBwcjJ4vPkROROYjhMCRI0ewdetW6HQ6ODs7Y8iQIahTp47U0YiIHjvlLm7ef/99/O9//0Pfvn0BAAMHDjT6i1EIAZlMBp1OZ/6URNVYYWEhNmzYgFOnTgEAGjVqhLCwMMNtFYiIyFi5i5u5c+di4sSJ2LVrlyXzENF/yOVypKenQy6Xo3v37mjfvj1PRRERPUC5ixshBAAgJCTEYmGI6L7i3zeZTAalUomhQ4eisLAQtWvXljgZEdHjz6QxN/xrkcjyCgoKEBMTA29vb3Tu3BkA4OHhIXEqIqKqw6TiplGjRg8tcDIzMx8pEFF1dv36dURHR+POnTs4f/48WrduzZmGiYhMZFJxM3fu3BIzFBPRoxNCYP/+/dixYwf0ej3c3NwwdOhQFjZERBVgUnHzzDPPwNPT01JZiKql/Px8rF+/HufOnQMANGvWDAMGDOCNaImIKqjcxQ3H2xCZn06nw9KlS5GZmQmFQoHQ0FC0bduWv29ERI/A5KuliMh8FAoFnnrqKezfvx8RERHw8vKSOhIRUZVX7uKG968hMo+8vDzk5uYaroBq27YtAgMDoVQqJU5GRGQdTL79AhFV3JUrV7BmzRrY2Nhg/PjxsLW1NcxlQ0RE5sHihqgSCCGwe/duxMfHQwgBd3d35OXlcdAwEZEFsLghsrCcnBysW7cOly5dAgC0atUKffv2hUqlkjgZEZF1YnFDZEHJyclYu3YtcnJyoFQq0bdvXwQGBkodi4jIqrG4IbKg/fv3IycnBx4eHoiIiOBtFIiIKgGLGyILCgsLQ0JCArp27cpBw0RElUQudQAia3Lx4kVs377d8Nje3h69evViYUNEVIl45IbIDPR6PXbt2oWEhAQAgJ+fH5o2bSpxKiKi6onFDdEjys7Oxpo1a5CSkgIACAoKQoMGDSRORURUfbG4IXoE58+fx7p165Cfnw+VSoWBAwfiiSeekDoWEVG1xuKGqIJ2796NuLg4AIC3tzeGDh2KGjVqSJyKiIhY3BBVkLe3NwAgODgYPXv2hI0Nf52IiB4H/NeYyAS5ublwcHAAADRo0ACTJk3i3DVERI8ZXgpOVA46nQ5bt27FN998g6ysLEM7CxsioscPixuih8jKysKyZctw4MABFBQU4Pz581JHIiKiB+BpKaIHOH36NGJiYlBYWAg7OzuEhYWhcePGUsciIqIHYHFDVIqioiJs374dhw4dAnB/Ur4hQ4bAxcVF4mRERPQwLG6ISnHgwAFDYdOxY0d07doVCoVC4lRERFQeLG6IStGuXTtcvnwZwcHBaNiwodRxiIjIBBxQTARAq9Vi79690Ov1AAAbGxuMHDmShQ0RURXEIzdU7WVkZGD16tW4desWCgoK0K1bN6kjERHRI2BxQ9Xa8ePHsWnTJmi1Wjg4OMDf31/qSERE9IhY3FC1pNFosGXLFiQmJgIA6tWrh/DwcDg6OkobjIiIHhmLG6p20tPTsXr1aqSnp0MmkyEkJASdO3eGXM4haERE1oDFDVU7QghkZWXB0dERQ4YM4akoIiIrw+KGqgW9Xm84MuPp6Ynhw4fD29vbcBNMIiKyHjwOT1YvLS0NS5YsQUpKiqGtQYMGLGyIiKwUixuyWkIIHD58GEuXLkV6ejpiY2MhhJA6FhERWRhPS5FVKiwsxIYNG3Dq1CkAQMOGDTFo0CDIZDKJkxERkaWxuCGrk5qaiujoaGRmZkIul6N79+5o3749CxsiomqCxQ1ZlVu3buGnn36CTqeDi4sLhgwZAj8/P6ljERFRJWJxQ1bFw8MDjRo1gl6vR1hYGOzs7KSOREREleyxGFC8ePFi+Pv7w9bWFu3atcPBgwfLXPbHH39E586d4ebmBjc3N/To0eOBy5P1u3HjBgoKCgAAMpkMgwcPxvDhw1nYEBFVU5IXN6tWrcK0adMwZ84cHD16FK1atUJoaChu3bpV6vLx8fF49tlnsWvXLuzbtw9+fn7o1asXrl+/XsnJSWpCCOzbtw8//fQTNm7caLgSSqlUcnwNEVE1Jnlxs3DhQowbNw5jx45Fs2bNsGTJEtjb22PZsmWlLr9ixQpMmjQJgYGBaNKkCZYuXQq9Xo+dO3dWcnKSUlFREaKjo7F9+3bo9XoIIaDT6aSORUREjwFJx9xoNBocOXIEM2fONLTJ5XL06NED+/btK9c28vLyoNVqUaNGDUvFpMfMtWvXkJSUBK1WC4VCgdDQULRt25ZHa4iICIDExU1GRgZ0Oh1q1apl1F6rVi2cPXu2XNt466234OPjgx49epT6fGFhIQoLCw2Ps7OzAQBarRZarbaCyUvSaouM/t+c26b7hBDYv38/4uPjIYSAq6srwsPD4eXlhaKioodvgExS/BnmZ9my2M+Vh31dOSzVz6Zsr0pfLfXxxx8jKioK8fHxsLW1LXWZ+fPnY+7cuSXat2/fDnt7e7NlKdQBxd0ZFxcHtcJsm6b/U1RUhKSkJENh4+fnh6NHj0ody+rFxsZKHaFaYD9XHvZ15TB3P+fl5ZV7WUmLG3d3dygUCty8edOo/ebNm/Dy8nrgugsWLMDHH3+MHTt2oGXLlmUuN3PmTEybNs3wODs72zAI2dnZ+dFewL/kaYrw5sE4AEC3bt3g4lB6sUWPJiUlBbdu3cLNmzfRq1cvKJVKqSNZLa1Wi9jYWPTs2ZP9bEHs58rDvq4clurn4jMv5SFpcaNSqRAUFISdO3di0KBBAGAYHDxlypQy1/v000/x0UcfYdu2bWjbtu0D96FWq6FWq0u0K5VKs3a6Uvz/8R5KpQ1/ccxACIHdu3fD1dXVUMDWr18fderUwebNm83+HlLp2M+Vg/1cedjXlcPs37MmbEvy01LTpk3D6NGj0bZtWwQHB2PRokXIzc3F2LFjAQCjRo2Cr68v5s+fDwD45JNPMHv2bKxcuRL+/v5IS0sDADg6OsLR0VGy10HmlZOTg3Xr1uHSpUtQKpXw9/c365E2IiKyXpIXN8OHD0d6ejpmz56NtLQ0BAYGYuvWrYZBxikpKZDL//8V69999x00Gg2GDh1qtJ05c+bgvffeq8zoZCHJyclYu3YtcnJyYGNjgz59+sDJyUnqWEREVEVIXtwAwJQpU8o8DRUfH2/0+PLly5YPRJLQ6/X4+++/8ffff0MIAQ8PD0RERMDDw0PqaEREVIU8FsUNkV6vx/Lly5GcnAwAaN26Nfr06cPz4kREZDIWN/RYkMvl8PHxwbVr19C/f/8HXgFHRET0ICxuSDJ6vR75+flwcHAAAHTt2hVt2rThbNNERPRIJL+3FFVP2dnZ+OWXX7By5UrDPaEUCgULGyIiemQ8ckOV7vz581i3bh3y8/OhUqlw69YteHt7Sx2LiIisBIsbqjQ6nQ5xcXHYu3cvAMDb2xtDhw7l0RoiIjIrFjdUKe7cuYM1a9bg2rVrAIDg4GD07NkTNjb8CBIRkXnxm4UqxYYNG3Dt2jWo1WqEhYWhadOmUkciIiIrxeKGKkW/fv2wadMm9O/fH25ublLHISIiK8arpcgisrKycPToUcPjGjVq4Pnnn2dhQ0REFscjN2R2p0+fRkxMDAoLC+Hq6oqAgACpIxERUTXC4obMpqioCNu3b8ehQ4cAALVr1+aVUEREVOlY3JBZZGZmYvXq1UhLSwMAdOjQAd26dYNCoZA4GRERVTcsbuiRnTp1CjExMdBoNLCzs8PgwYPRsGFDqWMREVE1xeKGHplGo4FGo0GdOnUwZMgQODs7Sx2JiIiqMRY3VCF6vR5y+f2L7QIDA6FSqdC0aVNDGxERkVT4TUQmO378OL777jvk5eUBAGQyGZ544gkWNkRE9FjgtxGVm0ajwZ9//on169cjIyMDBw4ckDoSERFRCTwtReVy69YtREdHIz09HQAQEhKCLl26SJyKiIioJBY39EBCCCQmJmLz5s0oKiqCo6MjwsPDUa9ePamjERERlYrFDT3QoUOHsGXLFgBAQEAABg8eDEdHR4lTERERlY3FDT1Qy5YtceDAAQQGBqJTp06QyWRSRyIiInogFjdkRAiBS5cuISAgADKZDLa2tnj55ZdhY8OPChERVQ28WooMCgsLsXbtWixfvtzojt4sbIiIqCrhtxYBAFJTUxEdHY3MzEzI5XJotVqpIxEREVUIi5tqTgiBQ4cOYfv27dDpdHBxccGQIUPg5+cndTQiIqIKYXFTjRUUFCAmJgZnzpwBADRu3BhhYWGws7OTOBkREVHFsbipxm7evImzZ89CLpejZ8+eaNeuHa+GIiKiKo/FTTVWt25d9OnTBz4+PvD19ZU6DhERkVnwaqlqJD8/H2vWrEFGRoah7cknn2RhQ0REVoVHbqqJq1evYs2aNbh79y4yMzPx0ksv8RQUERFZJRY3Vk4Igb179yIuLg56vR5ubm7o378/CxsiIrJaLG6sWF5eHtavX4/z588DAJ544gkMGDAAarVa4mRERESWw+LGSmVmZiIyMhL37t2DjY0NevfujTZt2vCIDRERWT0WN1bKxcUFrq6uUKlUiIiIQK1ataSOREREVClY3FiR3Nxc2NraQqFQQKFQICIiAmq1GiqVSupoRERElYaXgluJ5ORkLFmyBDt37jS0OTk5sbAhIqJqh8VNFafX6xEfH4/ffvsNOTk5uHDhAm96SURE1RpPS1Vh9+7dw7p165CcnAwACAwMRN++faFUKiVORkREJB0WN1XUxYsXsW7dOuTm5kKpVKJfv35o1aqV1LGIiIgkx+KmCiooKMDq1atRWFgIT09PREREwN3dXepYREREjwUWN1WQra0t+vfvj+TkZPTu3ZunoYiIiP6FxU0Vcf78edjY2KBevXoAgObNm6N58+YSpyIiInr8sLh5zOl0OsTFxWHv3r1wcHDAxIkT4ejoKHUsIiKixxaLm8fY3bt3ER0djWvXrgEAmjVrBltbW4lTERERPd5Y3DymkpKSsH79ehQUFECtVmPgwIFo1qyZ1LGIqiQhBIqKiqDT6aSOUoJWq4WNjQ0KCgoey3zWhH1dOR6ln5VKJRQKxSNnYHHzmNHr9YiNjcX+/fsBAD4+Phg6dCjc3NwkTkZUNWk0GqSmpiIvL0/qKKUSQsDLywtXr17ljW0tjH1dOR6ln2UyGWrXrv3Iwy9Y3DxmZDIZcnNzAQDt2rVDz549zVLFElVHer0eycnJUCgU8PHxgUqleuy+1PR6PXJycuDo6Ai5nJPGWxL7unJUtJ+FEEhPT8e1a9fQsGHDR/ruY3HzmNDr9ZDL5ZDJZOjXrx9atGiBhg0bSh2LqErTaDTQ6/Xw8/ODvb291HFKpdfrodFoYGtryy9cC2NfV45H6WcPDw9cvnwZWq32kYobvrsSKyoqwubNm/HHH39ACAEAUKvVLGyIzIhfZERVg7mOrPLIjYQyMzMRHR2N1NRUAEBKSgrq1q0rcSoiIqKqjcWNRE6ePIkNGzZAo9HAzs4OgwYNYmFDRERkBjxWW8m0Wi02btyINWvWQKPRoE6dOpg4cSIaNWokdTQiIquQlJQELy8v3Lt3T+oo9C+nT59G7dq1DRfNWBKLm0q2Zs0aHDlyBADQqVMnjB49Gs7OzhKnIqLHzZgxYyCTySCTyaBUKlGvXj28+eabKCgoKLHsxo0bERISAicnJ9jb2+PJJ59EZGRkqdtds2YNnn76abi4uMDR0REtW7bE+++/j8zMTAu/osozc+ZMvPLKK3BycirxXHBwMOzs7JCWllbiOX9/fyxatKhE+3vvvYfAwECjtrS0NLzyyisICAiAWq2Gn58fBgwYgJ07d5rrZZRq9erVaNKkCWxtbdGiRQts3rz5oessXrwYTZs2hZ2dHRo3boxff/3V6HmtVov3338f9evXh62tLVq1aoWtW7caLePv72/4PP77Z/LkySX2J4TA0KFDoVAosH79ekN7s2bN8NRTT2HhwoUVe/EmYHFTyTp16gQnJyc899xz6N69Owc6ElGZevfujdTUVFy6dAlffPEFvv/+e8yZM8doma+//hphYWHo2LEjDhw4gH/++QfPPPMMJk6ciOnTpxst+84772D48OF48sknsWXLFpw8eRKff/45jh8/jt9++63SXpdGo7HYtlNSUrBx40aMGTOmxHMJCQnIz8/HkCFD8Msvv1R4H5cvX0ZQUBDi4uLw2Wef4cSJE9i6dSu6du1a6pe9uezduxfPPvssXnzxRRw7dgyDBg3CoEGDcPLkyTLX+e677zBz5ky89957OHXqFObOnYvJkydjw4YNhmXeffddfP/99/j6669x+vRpTJw4EYMHD8axY8cMyxw6dAipqamGn9jYWABAREREiX1++eWXZQ4MHjt2LL777jsUFRVVtBvKR1Qzd+/eFQDE3bt3zbrd3EKtqPvWRlH3rY3iTk6eoV2j0Yjk5GSjZbVarVn3Xd1oNBqxfv16odFopI5i1ayhn/Pz88Xp06dFfn6+oU2v14vcQq0kP3q9vkRGnU4nsrKyhE6nM2ofPXq0CAsLM2oLDw8XrVu3NjxOSUkRSqVSTJs2rcR2v/rqKwFA7N+/XwghxIEDBwQAsWjRolL7Kisrq8x+vHr1qnjmmWeEm5ubsLe3F0FBQYbtlpZz6tSpIiQkxPA4JCRETJ48WUydOlXUrFlTPP300+LZZ58Vw4YNM1pPo9GImjVril9++cXQN/PmzRP+/v7C1tZWtGzZUqxevbrMnEII8dlnn4m2bduW+tzo0aPFa6+9JjZt2iQaNWpU4vm6deuKL774okT7nDlzRKtWrQyP+/TpI3x9fUVOTk6JZR/Uj49q2LBhol+/fkZt7dq1ExMmTChznfbt24vp06cbtU2bNk107NjR8Njb21t88803RsuEh4eLkSNHlrndqVOnivr165f4TB87dkz4+vqKs2fPCgBi3bp1Rs8XFhYKtVotduzYUep2S/udLWbK9zcHFFtQeno6Vq9ejaysLLz00kuoVasWAMDGht1OJJV8rQ7NZm+TZN+n3w+Fvapiv/8nT57E3r17jS48iI6OhlarLXGEBgAmTJiAt99+G7///jvatWuHFStWwNHREZMmTSp1+66urqW25+TkICQkBL6+voiJiYGXlxeOHj0KvV5vUv5ffvkFL7/8Mvbs2QMAuHDhAiIiIgyTvQHAtm3bkJeXh8GDBwMA5s+fj+XLl2PJkiVo2LAh/v77bzz33HPw8PBASEhIqfvZvXs32rZtW6L93r17iI6ORmxsLNq2bYu7d+9i9+7d6Ny5s0mvIzMzE1u3bsVHH30EBweHEs+X1Y8AsGLFCkyYMOGB29+yZUuZmfbt24dp06YZtYWGhhqd+vmvwsLCEvcktLOzw8GDB6HVaqFUKstcJiEhodRtajQaLF++HNOmTTM6QpOXl4cRI0bg66+/Nnzf/ZdKpUJgYCB2796N7t27l5n7UT0W37KLFy/GZ599hrS0NLRq1Qpff/01goODy1x+9erVmDVrFi5fvoyGDRvik08+Qd++fSsx8YMJIXDs2DFs3rwZRUVFcHR0RGFhodSxiKiK2bhxIxwdHVFUVITCwkLI5XJ88803hufPnTsHFxcXeHt7l1hXpVIhICAA586dAwCcP38eAQEBUCqVJmVYuXIl0tPTcejQIdSoUQMA0KBBA5NfS8OGDfHpp58aHtevXx8ODg5Yt24dnn/+ecO+Bg4cCCcnJxQWFmLevHnYsWMH2rdvDwAICAhAQkICvv/++zKLmytXrpRa3ERFRaFhw4Zo2rQpFAoFnnnmGfz0008mFzcXLlyAEAJNmjQxaT0AGDhwINq1a/fAZXx9fct8Li0trUTRUKtWrVLHDxULDQ3F0qVLMWjQILRp0wZHjhzB0qVLodVqkZGRAW9vb4SGhmLhwoXo0qUL6tevj507d2Lt2rVl3hdq/fr1uHPnTolTf6+//jo6dOiAsLAwZGdnl5nJx8cHV65cKfN5c5C8uFm1ahWmTZuGJUuWoF27dli0aBFCQ0ORlJQET0/PEssXn3OcP38++vfvj5UrV2LQoEE4evQomjdvLsErMGYDHbZv2Ywzp08BuP/LOHjw4Ee+TwYRmYedUoHT74dKtm9TdO3aFd999x1yc3PxxRdfwMbGBkOGDKnQvsX/TRJqqsTERLRu3dpQ2FRUUFCQ0WMbGxsMGzYMK1aswPPPP4/c3Fz8+eefiIqKAnC/iMjLy0PPnj2N1tNoNGjdunWZ+8nPzy9xFAIAli1bhpEjRxoeP/fccwgJCcHXX39d6sDjslS0HwHAycnJpH2Zw6xZs5CWloannnoKQgjUqlULo0ePxqeffmoY8/nll19i3LhxaNKkCWQyGerXr4+xY8di2bJlpW7zp59+Qp8+feDj42Noi4mJQVxcnNE4nbLY2dlZ/F5vko9mXbhwIcaNG4exY8eiWbNmWLJkCezt7cvs1C+//BK9e/fGG2+8gaZNm+KDDz5AmzZtjP6akYqbLA8D1Gdw5vQpyGQydO3aFc899xwLG6LHiEwmg73KRpIfU2dfdXBwQIMGDdCqVSssW7YMBw4cwE8//WR4vlGjRrh79y5u3LhRYl2NRoOLFy8applo1KgRLl26BK1Wa1IGOzu7Bz4vl8tLfOGXto/STuGMHDkSO3fuxK1bt7B+/XrY2dmhd+/eAO6fDgOATZs2ITEx0fBz+vRpREdHl5nH3d0dWVlZRm2nT5/G/v378dZbb8Hd3R0qlQpPPfUU8vLyDMUUADg7O+Pu3bsltnnnzh24uLgAuH8ESiaT4ezZs2VmKEvxqcEH/ezevbvM9b28vHDz5k2jtps3b8LLy6vMdezs7LBs2TLk5eXh8uXLSElJgb+/P5ycnODh4QHg/i0P1q9fj9zcXFy5cgVnz56Fo6MjAgICSmzvypUr2LFjB1566SWj9ri4OFy8eBGurq5QqVRwd3cHAAwZMgRPP/200bKZmZmGfVuKpEduNBoNjhw5gpkzZxra5HI5evTogX379pW6jqnnHAsLC41OCRUfKtNqtSb/kj+IVluEOoo7cJUXwMHBAYMHD0adOnUsPyK8Gip+38z5/lFJ1tDPWq0WQgjo9XqTx4hUluLCoDjnv9v/2zZjxgxMnz4dzzzzDOzs7DB48GC89dZbWLBgARYsWGC03eIjPsOHD4der8czzzyDr776CosXL8arr75aIsedO3dKHS/SvHlzLF26FBkZGaUevXF3d8fJkyeNciYmJkKpVJZ4Pf99D5566in4+fkhKioKW7ZsMVw+rNfr0aRJE6jValy+fLnUU0dlvZ+BgYE4deqU0fNLly5Fly5d8NVXXyE3NxcODg6QyWSIjIzETz/9hBdffBHA/QLw8OHDJbZ99OhRNGrUCHq9Hq6urujVqxcWL16MKVOmlCjayupHAOjfvz+OHj1a6nPFfH19y3xtTz31FHbs2GH0/sXGxuKpp5566Oe7+OaxwP1TdP369QNg3I8qlQre3t7QaDRYs2YNIiIiSmx32bJl8PT0RJ8+fYyee/PNN/HCCy8AuP9e5+bmomPHjli4cCH69+9vtOzJkycRHh5eama9Xg8hRKn3ljLl3yJJi5uMjAzodLpSzyGWVRWbes5x/vz5mDt3bon27du3m/VGeoU64J8ib8ghEFGnJk6ePPnAy/Po0RVfikiWVZX72cbGBl5eXsjJybHo5cfm8N8J57RaLYqKiozGLoSGhuLNN9/EwoUL8corr8DV1RVz587Fu+++C5lMhuHDh0OpVGLz5s344IMPMGXKFDRt2hTZ2dlo2rQpXn31VUyfPh2XLl1C//794eXlheTkZPz888946qmnMHHixBK5+vXrh3nz5mHgwIGYPXs2vLy88M8//8DLywvBwcFo164dFixYgB9++AFPPvkk/vjjD5w4cQItW7Y0ZC8qKoJGoyl1HEZ4eDi+++47XLhwATExMUbLTJkyBdOmTUNeXh6eeuopZGdn48CBA3BycsKzzz5baj926tQJU6dORVZWFhQKBbRaLX777TfMnDmzxCzww4cPxxdffIEDBw6gadOmGDduHPr27YvZs2djwIAB0Ol0WLNmDfbt24ePP/7YkO3jjz9G7969ERwcjJkzZ+KJJ55AUVER4uPjDUfYylLacIt/e9Af3i+++CL69++PefPmoVevXli7di0OHz6MBQsWGLLNnTsXqampWLJkCYD7p/eOHDmCtm3b4s6dO1i8eDFOnDiBr7/+2rDO4cOHkZqaihYtWuDGjRv45JNPUFRUhIkTJxq9H3q9HsuWLcPw4cNLnFayt7dHnTp1SmR2d3dHzZo1DdtJSUnB9evX0a5du1I/DxqNBvn5+fj7779LHBww6VTWQ6+nsqDr168LAGLv3r1G7W+88YYIDg4udR2lUilWrlxp1LZ48WLh6elZ6vIFBQXi7t27hp+rV68KACIjI0NoNBqz/RQWFor0rGyxas16kZOTY9Zt88f4Jzc3V6xfv17k5uZKnsWaf6yhn7Ozs8WpU6dEbm6u0Ol0j+VPUVGRyMrKEkVFRUbto0aNEgMHDiyx/Lx584SHh4fIzs42tK1bt0507txZODg4CFtbWxEUFCSWLl1a6v5+//130aVLF+Hk5CQcHBxEy5Ytxdy5c8Xt27fLzHjp0iURHh4unJ2dhb29vWjbtq3Yt2+f4flZs2aJWrVqCRcXF/Haa6+JyZMni5CQEMPzISEh4tVXXy112ydPnhQARN26dUv0QVFRkfjiiy9E48aNhVKpFB4eHqJXr15i165dZWYtLCwUPj4+YvPmzUKn04k//vhDyOVycePGjVL7umnTpuK1114zPN6yZYvo2LGjcHNzM1y2Xtr+rl27JiZNmiTq1q0rVCqV8PX1FQMGDBA7d+606OclKipKNGrUSKhUKvHEE0+IDRs2lPjc/LvvT548KQIDA4WdnZ1wdnYWAwcOFKdPnzZaJy4uTjRt2lSo1WpRs2ZN8dxzz4mrV6+W2PeWLVsEAHHmzJlyfaYBiDVr1hg999FHH4levXqVuW5ubq44deqUyM7OLvH7nJGRUe5LwSUtbgoLC4VCoShxHXzxL3Vp/Pz8SsxDMHv2bNGyZcty7dNS89wIYR3zglQF7OfKYQ39/KA5Mx4XOl3p89xQxX3zzTeiV69eJdrZ15WjrH4uLCwUderUEQkJCWWua655biQdUKxSqRAUFGQ0XbVer8fOnTsNl/79V/v27UtMbx0bG1vm8kREVL1MmDABXbp04b2lHjMpKSl4++230bFjR4vvS/JLwadNm4bRo0ejbdu2CA4OxqJFi5Cbm4uxY8cCAEaNGgVfX1/Mnz8fADB16lSEhITg888/R79+/RAVFYXDhw/jhx9+kPJlEBHRY8LGxgbvvPOO1DHoPxo0aFChOZIqQvLiZvjw4UhPT8fs2bORlpaGwMBAbN261TBoOCUlxej+Sx06dMDKlSvx7rvv4u2330bDhg2xfv36x2KOGyIiIpKe5MUNcH9E/JQpU0p9Lj4+vkRbREREqTfrIiIiIpJ8Ej8iIksTjzCrLBFVHnP9rrK4ISKrVXwfJUtP9U5E5qH5v/mo/juBn6kei9NSRESWoFAo4Orqilu3bgG4P9GYqbdAsDS9Xg+NRoOCggKj8YVkfuzrylHRftbr9UhPT4e9vT1sbB6tPGFxQ0RWrfi+O8UFzuNGCIH8/HzY2dk9doWXtWFfV45H6We5XI46deo88vvD4oaIrJpMJoO3tzc8PT0fy/tkabVa/P333+jSpYvhNBpZBvu6cjxKP6tUKrMcVWNxQ0TVgkKheOTz+JagUChQVFQEW1tbfuFaGPu6cjwO/cyTjkRERGRVWNwQERGRVWFxQ0RERFal2o25KZ4gKDs72+zb1mq1yMvLQ3Z2Ns/nWhD7uXKwnysH+7nysK8rh6X6ufh7uzwT/VW74qb4LrF+fn4SJyEiIiJT3bt3Dy4uLg9cRiaq2bzker0eN27cgJOTk9nnOcjOzoafnx+uXr0KZ2dns26b/j/2c+VgP1cO9nPlYV9XDkv1sxAC9+7dg4+Pz0MvF692R27kcjlq165t0X04OzvzF6cSsJ8rB/u5crCfKw/7unJYop8fdsSmGAcUExERkVVhcUNERERWhcWNGanVasyZMwdqtVrqKFaN/Vw52M+Vg/1cedjXleNx6OdqN6CYiIiIrBuP3BAREZFVYXFDREREVoXFDREREVkVFjdERERkVVjcmGjx4sXw9/eHra0t2rVrh4MHDz5w+dWrV6NJkyawtbVFixYtsHnz5kpKWrWZ0s8//vgjOnfuDDc3N7i5uaFHjx4PfV/oPlM/z8WioqIgk8kwaNAgywa0Eqb28507dzB58mR4e3tDrVajUaNG/LejHEzt50WLFqFx48aws7ODn58fXn/9dRQUFFRS2qrp77//xoABA+Dj4wOZTIb169c/dJ34+Hi0adMGarUaDRo0QGRkpMVzQlC5RUVFCZVKJZYtWyZOnTolxo0bJ1xdXcXNmzdLXX7Pnj1CoVCITz/9VJw+fVq8++67QqlUihMnTlRy8qrF1H4eMWKEWLx4sTh27Jg4c+aMGDNmjHBxcRHXrl2r5ORVi6n9XCw5OVn4+vqKzp07i7CwsMoJW4WZ2s+FhYWibdu2om/fviIhIUEkJyeL+Ph4kZiYWMnJqxZT+3nFihVCrVaLFStWiOTkZLFt2zbh7e0tXn/99UpOXrVs3rxZvPPOO2Lt2rUCgFi3bt0Dl7906ZKwt7cX06ZNE6dPnxZff/21UCgUYuvWrRbNyeLGBMHBwWLy5MmGxzqdTvj4+Ij58+eXuvywYcNEv379jNratWsnJkyYYNGcVZ2p/fxfRUVFwsnJSfzyyy+WimgVKtLPRUVFokOHDmLp0qVi9OjRLG7KwdR+/u6770RAQIDQaDSVFdEqmNrPkydPFt26dTNqmzZtmujYsaNFc1qT8hQ3b775pnjiiSeM2oYPHy5CQ0MtmEwInpYqJ41GgyNHjqBHjx6GNrlcjh49emDfvn2lrrNv3z6j5QEgNDS0zOWpYv38X3l5edBqtahRo4alYlZ5Fe3n999/H56ennjxxRcrI2aVV5F+jomJQfv27TF58mTUqlULzZs3x7x586DT6SordpVTkX7u0KEDjhw5Yjh1denSJWzevBl9+/atlMzVhVTfg9XuxpkVlZGRAZ1Oh1q1ahm116pVC2fPni11nbS0tFKXT0tLs1jOqq4i/fxfb731Fnx8fEr8QtH/V5F+TkhIwE8//YTExMRKSGgdKtLPly5dQlxcHEaOHInNmzfjwoULmDRpErRaLebMmVMZsaucivTziBEjkJGRgU6dOkEIgaKiIkycOBFvv/12ZUSuNsr6HszOzkZ+fj7s7Owssl8euSGr8vHHHyMqKgrr1q2Dra2t1HGsxr179/D888/jxx9/hLu7u9RxrJper4enpyd++OEHBAUFYfjw4XjnnXewZMkSqaNZlfj4eMybNw/ffvstjh49irVr12LTpk344IMPpI5GZsAjN+Xk7u4OhUKBmzdvGrXfvHkTXl5epa7j5eVl0vJUsX4utmDBAnz88cfYsWMHWrZsacmYVZ6p/Xzx4kVcvnwZAwYMMLTp9XoAgI2NDZKSklC/fn3Lhq6CKvJ59vb2hlKphEKhMLQ1bdoUaWlp0Gg0UKlUFs1cFVWkn2fNmoXnn38eL730EgCgRYsWyM3Nxfjx4/HOO+9ALuff/uZQ1vegs7OzxY7aADxyU24qlQpBQUHYuXOnoU2v12Pnzp1o3759qeu0b9/eaHkAiI2NLXN5qlg/A8Cnn36KDz74AFu3bkXbtm0rI2qVZmo/N2nSBCdOnEBiYqLhZ+DAgejatSsSExPh5+dXmfGrjIp8njt27IgLFy4YikcAOHfuHLy9vVnYlKEi/ZyXl1eigCkuKAVvuWg2kn0PWnS4spWJiooSarVaREZGitOnT4vx48cLV1dXkZaWJoQQ4vnnnxczZswwLL9nzx5hY2MjFixYIM6cOSPmzJnDS8HLwdR+/vjjj4VKpRLR0dEiNTXV8HPv3j2pXkKVYGo//xevliofU/s5JSVFODk5iSlTpoikpCSxceNG4enpKT788EOpXkKVYGo/z5kzRzg5OYnff/9dXLp0SWzfvl3Ur19fDBs2TKqXUCXcu3dPHDt2TBw7dkwAEAsXLhTHjh0TV65cEUIIMWPGDPH8888bli++FPyNN94QZ86cEYsXL+al4I+jr7/+WtSpU0eoVCoRHBws9u/fb3guJCREjB492mj5P/74QzRq1EioVCrxxBNPiE2bNlVy4qrJlH6uW7euAFDiZ86cOZUfvIox9fP8byxuys/Uft67d69o166dUKvVIiAgQHz00UeiqKioklNXPab0s1arFe+9956oX7++sLW1FX5+fmLSpEkiKyur8oNXIbt27Sr139vivh09erQICQkpsU5gYKBQqVQiICBA/PzzzxbPKROCx9+IiIjIenDMDREREVkVFjdERERkVVjcEBERkVVhcUNERERWhcUNERERWRUWN0RERGRVWNwQERGRVWFxQ0RGIiMj4erqKnWMCpPJZFi/fv0DlxkzZgwGDRpUKXmIqPKxuCGyQmPGjIFMJivxc+HCBamjITIy0pBHLpejdu3aGDt2LG7dumWW7aempqJPnz4AgMuXL0MmkyExMdFomS+//BKRkZFm2V9Z3nvvPcPrVCgU8PPzw/jx45GZmWnSdliIEZmOdwUnslK9e/fGzz//bNTm4eEhURpjzs7OSEpKgl6vx/HjxzF27FjcuHED27Zte+RtP+zu8QDg4uLyyPspjyeeeAI7duyATqfDmTNn8MILL+Du3btYtWpVpeyfqLrikRsiK6VWq+Hl5WX0o1AosHDhQrRo0QIODg7w8/PDpEmTkJOTU+Z2jh8/jq5du8LJyQnOzs4ICgrC4cOHDc8nJCSgc+fOsLOzg5+fH1599VXk5uY+MJtMJoOXlxd8fHzQp08fvPrqq9ixYwfy8/Oh1+vx/vvvo3bt2lCr1QgMDMTWrVsN62o0GkyZMgXe3t6wtbVF3bp1MX/+fKNtF5+WqlevHgCgdevWkMlkePrppwEYHw354Ycf4OPjY3QXbgAICwvDCy+8YHj8559/ok2bNrC1tUVAQADmzp2LoqKiB75OGxsbeHl5wdfXFz169EBERARiY2MNz+t0Orz44ouoV68e7Ozs0LhxY3z55ZeG59977z388ssv+PPPPw1HgeLj4wEAV69exbBhw+Dq6ooaNWogLCwMly9ffmAeouqCxQ1RNSOXy/HVV1/h1KlT+OWXXxAXF4c333yzzOVHjhyJ2rVr49ChQzhy5AhmzJgBpVIJALh48SJ69+6NIUOG4J9//sGqVauQkJCAKVOmmJTJzs4Oer0eRUVF+PLLL/H5559jwYIF+OeffxAaGoqBAwfi/PnzAICvvvoKMTEx+OOPP5CUlIQVK1bA39+/1O0ePHgQALBjxw6kpqZi7dq1JZaJiIjA7du3sWvXLkNbZmYmtm7dipEjRwIAdu/ejVGjRmHq1Kk4ffo0vv/+e0RGRuKjjz4q92u8fPkytm3bBpVKZWjT6/WoXbs2Vq9ejdOnT2P27Nl4++238ccffwAApk+fjmHDhqF3795ITU1FamoqOnToAK1Wi9DQUDg5OWH37t3Ys2cPHB0d0bt3b2g0mnJnIrJaFr81JxFVutGjRwuFQiEcHBwMP0OHDi112dWrV4uaNWsaHv/888/CxcXF8NjJyUlERkaWuu6LL74oxo8fb9S2e/duIZfLRX5+fqnr/Hf7586dE40aNRJt27YVQgjh4+MjPvroI6N1nnzySTFp0iQhhBCvvPKK6Natm9Dr9aVuH4BYt26dEEKI5ORkAUAcO3bMaJn/3tE8LCxMvPDCC4bH33//vfDx8RE6nU4IIUT37t3FvHnzjLbx22+/CW9v71IzCCHEnDlzhFwuFw4ODsLW1tZw9+SFCxeWuY4QQkyePFkMGTKkzKzF+27cuLFRHxQWFgo7Ozuxbdu2B26fqDrgmBsiK9W1a1d89913hscODg4A7h/FmD9/Ps6ePYvs7GwUFRWhoKAAeXl5sLe3L7GdadOm4aWXXsJvv/1mOLVSv359APdPWf3zzz9YsWKFYXkhBPR6PZKTk9G0adNSs929exeOjo7Q6/UoKChAp06dsHTpUmRnZ+PGjRvo2LGj0fIdO3bE8ePHAdw/pdSzZ080btwYvXv3Rv/+/dGrV69H6quRI0di3Lhx+Pbbb6FWq7FixQo888wzkMvlhte5Z88eoyM1Op3ugf0GAI0bN0ZMTAwKCgqwfPlyJCYm4pVXXjFaZvHixVi2bBlSUlKQn58PjUaDwMDAB+Y9fvw4Lly4ACcnJ6P2goICXLx4sQI9QGRdWNwQWSkHBwc0aNDAqO3y5cvo378/Xn75ZXz00UeoUaMGEhIS8OKLL0Kj0ZT6Jf3ee+9hxIgR2LRpE7Zs2YI5c+YgKioKgwcPRk5ODiZMmIBXX321xHp16tQpM5uTkxOOHj0KuVwOb29v2NnZAQCys7Mf+rratGmD5ORkbNmyBTt27MCwYcPQo0cPREdHP3TdsgwYMABCCGzatAlPPvkkdu/ejS+++MLwfE5ODubOnYvw8PAS69ra2pa5XZVKZXgPPv74Y/Tr1w9z587FBx98AACIiorC9OnT8fnnn6N9+/ZwcnLCZ599hgMHDjwwb05ODoKCgoyKymKPy6BxIimxuCGqRo4cOQK9Xo/PP//ccFSieHzHgzRq1AiNGjXC66+/jmeffRY///wzBg8ejDZt2uD06dMliqiHkcvlpa7j7OwMHx8f7NmzByEhIYb2PXv2IDg42Gi54cOHY/jw4Rg6dCh69+6NzMxM1KhRw2h7xeNbdDrdA/PY2toiPDwcK1aswIULF9C4cWO0adPG8HybNm2QlJRk8uv8r3fffRfdunXDyy+/bHidHTp0wKRJkwzL/PfIi0qlKpG/TZs2WLVqFTw9PeHs7PxImYisEQcUE1UjDRo0gFarxddff41Lly7ht99+w5IlS8pcPj8/H1OmTEF8fDyuXLmCPXv24NChQ4bTTW+99Rb27t2LKVOmIDExEefPn8eff/5p8oDif3vjjTfwySefYNWqVUhKSsKMGTOQmJiIqVOnAgAWLlyI33//HWfPnsW5c+ewevVqeHl5lTrxoKenJ+zs7LB161bcvHkTd+/eLXO/I0eOxKZNm7Bs2TLDQOJis2fPxq+//oq5c+fi1KlTOHPmDKKiovDuu++a9Nrat2+Pli1bYt68eQCAhg0b4vDhw9i2bRvOnTuHWbNm4dChQ0br+Pv7459//kFSUhIyMjKg1WoxcuRIuLu7IywsDLt370ZycjLi4+Px6quv4tq1ayZlIrJKUg/6ISLzK20QarGFCxcKb29vYWdnJ0JDQ8Wvv/4qAIisrCwhhPGA38LCQvHMM88IPz8/oVKphI+Pj5gyZYrRYOGDBw+Knj17CkdHR+Hg4CBatmxZYkDwv/13QPF/6XQ68d577wlfX1+hVCpFq1atxJYtWwzP//DDDyIwMFA4ODgIZ2dn0b17d3H06FHD8/jXgGIhhPjxxx+Fn5+fkMvlIiQkpMz+0el0wtvbWwAQFy9eLJFr69atokOHDsLOzk44OzuL4OBg8cMPP5T5OubMmSNatWpVov33338XarVapKSkiIKCAjFmzBjh4uIiXF1dxcsvvyxmzJhhtN6tW7cM/QtA7Nq1SwghRGpqqhg1apRwd3cXarVaBAQEiHHjxom7d++WmYmoupAJIYS05RURERGR+fC0FBEREVkVFjdERERkVVjcEBERkVVhcUNERERWhcUNERERWRUWN0RERGRVWNwQERGRVWFxQ0RERFaFxQ0RERFZFRY3REREZFVY3BAREZFVYXFDREREVuX/Afpx7n2BTm0WAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical features to numeric\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Prepare the data\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with C=0.5: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxYquM2Yz1m0",
        "outputId": "de2edf63-2504-4fad-bb40-62b9ef66274e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with C=0.5: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train logistic regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Extract feature importance (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients,\n",
        "    'Abs_Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Step 6: Sort by absolute coefficient\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Step 7: Display top features\n",
        "print(\"Top Important Features in Logistic Regression:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcWIdCQv0h_k",
        "outputId": "ec42daea-793e-40fd-dae5-b0c130ad51ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Important Features in Logistic Regression:\n",
            "                 Feature  Coefficient\n",
            "21         worst texture    -1.335651\n",
            "10          radius error    -1.283117\n",
            "28        worst symmetry    -1.196087\n",
            "7    mean concave points    -1.130510\n",
            "13            area error    -0.944861\n",
            "26       worst concavity    -0.942150\n",
            "23            worst area    -0.882949\n",
            "20          worst radius    -0.881042\n",
            "6         mean concavity    -0.818323\n",
            "27  worst concave points    -0.766904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Calculate Cohen’s Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen’s Kappa Score:\", round(kappa, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MCxFxsE1ML_",
        "outputId": "fc3d5c45-1983-4a84-db95-f935a0e6b52e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen’s Kappa Score: 0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities\n",
        "y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Step 6: Compute Precision-Recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 7: Plot the Precision-Recall Curve\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Logistic Regression')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "yQYM-jOB1sUI",
        "outputId": "8274386f-b364-4579-e78b-5f0914d91c86"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVhdJREFUeJzt3XlYlOX+P/D3MAwzICAqmyDJYkoqamHyw91CUMyi0+IukksudEwyj5iKS0pWkpYLZrjk1xK18pgLihiWiVq4nGPu+wYIFqIgMDD37w8PU+MMOuAs0vN+XddcOvfczz3385ntzbPMyIQQAkREREQSYmPtCRARERFZGgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxDVWcOGDYOvr2+NlsnMzIRMJkNmZqZZ5lTXde/eHd27d9dev3jxImQyGVatWmW1OdGDmfo5vWrVKshkMly8eNEk4xEwY8YMyGQya0+D7sMAREaremOsuqhUKjRv3hyxsbHIy8uz9vQee1VhoupiY2ODhg0bonfv3sjKyrL29EwiLy8PEydORGBgIBwcHFCvXj0EBwfj/fffR2FhobWnZ3a+vr544YUXrD0No8ydOxebNm0y633c/55ha2sLb29vDBs2DNeuXTPrfRM9jK21J0B1z6xZs+Dn54fS0lLs3bsXS5cuxbZt23Ds2DE4ODhYbB7Lly+HRqOp0TJdu3bF3bt3YWdnZ6ZZPdyAAQMQGRmJyspKnD59GkuWLEGPHj3wyy+/ICgoyGrzelS//PILIiMjcefOHQwePBjBwcEAgF9//RUffPABfvzxR+zcudPKs/z7qe1zeu7cuXj11VcRFRWl0z5kyBD0798fSqXSZHP863vG/v37sWrVKuzduxfHjh2DSqUy2f08rqZOnYrJkydbexp0HwYgqrHevXujffv2AIARI0agUaNGSEpKwr///W8MGDDA4DLFxcWoV6+eSeehUChqvIyNjY3V33CfeeYZDB48WHu9S5cu6N27N5YuXYolS5ZYcWa1V1hYiJdffhlyuRyHDx9GYGCgzu1z5szB8uXLTXJf5ngu1WWmfk7L5XLI5XKTjQfov2e4urpi3rx52Lx5M15//XWT3teDCCFQWloKe3t7i90nANja2sLWlh+3jxvuAqNH9txzzwEALly4AODesTmOjo44d+4cIiMj4eTkhEGDBgEANBoNFixYgFatWkGlUsHDwwNvvvkm/vjjD71xt2/fjm7dusHJyQnOzs549tln8dVXX2lvN3QM0Lp16xAcHKxdJigoCAsXLtTeXt3xEhs2bEBwcDDs7e3h6uqKwYMH622ir1qva9euISoqCo6OjnBzc8PEiRNRWVlZ6/p16dIFAHDu3Dmd9sLCQrz99tvw8fGBUqlEs2bNMG/ePL2tXhqNBgsXLkRQUBBUKhXc3NzQq1cv/Prrr9o+K1euxHPPPQd3d3colUq0bNkSS5curfWc77ds2TJcu3YNSUlJeuEHADw8PDB16lTtdZlMhhkzZuj18/X1xbBhw7TXq3ah7NmzB2PHjoW7uzuaNGmCjRs3atsNzUUmk+HYsWPatpMnT+LVV19Fw4YNoVKp0L59e2zevPnRVrqWKioqMHv2bAQEBECpVMLX1xdTpkxBWVmZTj+NRoMZM2bAy8sLDg4O6NGjB44fP65XI0PP6TNnzuCVV16Bp6cnVCoVmjRpgv79++PWrVsA7tW/uLgYq1ev1u6eqhqzumOAHvZ6rInqnvPGPk7/+c9/0K1bN9jb26NJkyZ4//33sXLlSr15V+2S3LFjB9q3bw97e3ssW7YMgPGvr4e9p6jVasycORNPPvkkVCoVGjVqhM6dOyM9PV3bx9AxQMY+D6rWYe/evejQoQNUKhX8/f3x5Zdf1qDiZAgjKT2yqjexRo0aadsqKioQERGBzp074+OPP9buGnvzzTexatUqxMTE4J///CcuXLiARYsW4fDhw/j555+1W3VWrVqFN954A61atUJ8fDxcXFxw+PBhpKWlYeDAgQbnkZ6ejgEDBuD555/HvHnzAAAnTpzAzz//jPHjx1c7/6r5PPvss0hMTEReXh4WLlyIn3/+GYcPH4aLi4u2b2VlJSIiIhASEoKPP/4Yu3btwvz58xEQEIAxY8bUqn5Vb9gNGjTQtpWUlKBbt264du0a3nzzTTzxxBPYt28f4uPjkZOTgwULFmj7Dh8+HKtWrULv3r0xYsQIVFRU4KeffsL+/fu1f3UvXboUrVq1wosvvghbW1t8//33GDt2LDQaDcaNG1eref/V5s2bYW9vj1dfffWRxzJk7NixcHNzw/Tp01FcXIw+ffrA0dER69evR7du3XT6pqamolWrVmjdujUA4LfffkOnTp3g7e2NyZMno169eli/fj2ioqLwzTff4OWXXzbLnKszYsQIrF69Gq+++ireeecdHDhwAImJiThx4gS+++47bb/4+Hh8+OGH6Nu3LyIiInD06FFERESgtLT0geOXl5cjIiICZWVleOutt+Dp6Ylr165hy5YtKCwsRP369bFmzRqMGDECHTp0wKhRowAAAQEB1Y5Zm9fjgxh6zhv7OF27dg09evSATCZDfHw86tWrhy+++KLaXXanTp3CgAED8Oabb2LkyJFo0aKF0a8vY95TZsyYgcTERG09i4qK8Ouvv+LQoUPo2bNntTUw9nkAAGfPnsWrr76K4cOHIzo6GitWrMCwYcMQHByMVq1a1bj+9D+CyEgrV64UAMSuXbtEfn6+uHLlili3bp1o1KiRsLe3F1evXhVCCBEdHS0AiMmTJ+ss/9NPPwkAYu3atTrtaWlpOu2FhYXCyclJhISEiLt37+r01Wg02v9HR0eLpk2baq+PHz9eODs7i4qKimrX4YcffhAAxA8//CCEEKK8vFy4u7uL1q1b69zXli1bBAAxffp0nfsDIGbNmqUz5tNPPy2Cg4Orvc8qFy5cEADEzJkzRX5+vsjNzRU//fSTePbZZwUAsWHDBm3f2bNni3r16onTp0/rjDF58mQhl8vF5cuXhRBC7N69WwAQ//znP/Xu76+1Kikp0bs9IiJC+Pv767R169ZNdOvWTW/OK1eufOC6NWjQQLRt2/aBff4KgEhISNBrb9q0qYiOjtZer3rOde7cWe9xHTBggHB3d9dpz8nJETY2NjqP0fPPPy+CgoJEaWmptk2j0YiOHTuKJ5980ug5G6Np06aiT58+1d5+5MgRAUCMGDFCp33ixIkCgNi9e7cQQojc3Fxha2sroqKidPrNmDFDANCp0f3P6cOHD+s9nwypV6+ezjhVqmp+4cIFIYTxr0dDDL1nbNy4Ubi5uQmlUimuXLmi7Wvs4/TWW28JmUwmDh8+rG27efOmaNiwoc68hbj3eAAQaWlpOvMy9vVlzHtK27ZtH/iYCyFEQkKC+OvHrbHPg7+uw48//qhtu3HjhlAqleKdd9554P3Sg3EXGNVYWFgY3Nzc4OPjg/79+8PR0RHfffcdvL29dfrdv0Vkw4YNqF+/Pnr27ImCggLtJTg4GI6Ojvjhhx8A3Pur6/bt25g8ebLesQ0POpXUxcUFxcXFOpueH+bXX3/FjRs3MHbsWJ376tOnDwIDA7F161a9ZUaPHq1zvUuXLjh//rzR95mQkAA3Nzd4enqiS5cuOHHiBObPn6+z9WTDhg3o0qULGjRooFOrsLAwVFZW4scffwQAfPPNN5DJZEhISNC7n7/W6q/HPNy6dQsFBQXo1q0bzp8/r90t8iiKiorg5OT0yONUZ+TIkXrHpfTr1w83btzQ2fWzceNGaDQa9OvXDwDw+++/Y/fu3Xj99ddx+/ZtbR1v3ryJiIgInDlzxqJnI23btg0AEBcXp9P+zjvvAID2+ZaRkYGKigqMHTtWp99bb7310PuoX78+AGDHjh0oKSl55DnX9vX4V399z3j11VdRr149bN68GU2aNAFQs8cpLS0NoaGhaNeunXb8hg0banez38/Pzw8RERE6bca+vox5T3FxccFvv/2GM2fOGFULwPjnQZWWLVtqdxsCgJubG1q0aFGj9x3Sx11gVGOLFy9G8+bNYWtrCw8PD7Ro0QI2NrpZ2tbWVvvmVuXMmTO4desW3N3dDY5748YNAH/uUqvahWGssWPHYv369ejduze8vb0RHh6O119/Hb169ap2mUuXLgEAWrRooXdbYGAg9u7dq9NWdYzNXzVo0EDnGKb8/HydY4IcHR3h6OiovT5q1Ci89tprKC0txe7du/Hpp5/qHUN05swZ/Oc//9G7ryp/rZWXlxcaNmxY7ToCwM8//4yEhARkZWXpfSjeunVL+6FZW87Ozrh9+/YjjfEgfn5+em29evVC/fr1kZqaiueffx7Avd1f7dq1Q/PmzQHc23UghMC0adMwbdo0g2PfuHFDL7xXedhjWVOXLl2CjY0NmjVrptPu6ekJFxcX7fOx6t/7+zVs2FBnt5Ehfn5+iIuLQ1JSEtauXYsuXbrgxRdfxODBg2v1ONf29fhXVe8Zt27dwooVK/Djjz/q7LKqyeN06dIlhIaG6t1+f62qGHruGPv6MuY9ZdasWXjppZfQvHlztG7dGr169cKQIUPQpk2bauth7POgyhNPPKE3xv3vO1RzDEBUYx06dNAeW1IdpVKpF4o0Gg3c3d2xdu1ag8tU92ZkLHd3dxw5cgQ7duzA9u3bsX37dqxcuRJDhw7F6tWrH2nsKsacHfPss8/qvIElJCToHPD75JNPIiwsDADwwgsvQC6XY/LkyejRo4e2rhqNBj179sSkSZMM3kfVB7wxzp07h+effx6BgYFISkqCj48P7OzssG3bNnzyySc1/ioBQwIDA3HkyBGUl5c/0lcMVHcwuaGzdpRKJaKiovDdd99hyZIlyMvLw88//4y5c+dq+1St28SJE/W2AlSp7oMTePhjWVvm/lK8+fPnY9iwYfj3v/+NnTt34p///CcSExOxf/9+vT9MLOGv7xlRUVHo3LkzBg4ciFOnTsHR0fGRH6cHMfTcMfb1Zcx7SteuXXHu3Dltrb/44gt88sknSE5OxogRIx44N2OfB9W97wghjFqeDGMAIosJCAjArl270KlTpweehlp1MOaxY8dq/KZnZ2eHvn37om/fvtBoNBg7diyWLVuGadOmGRyradOmAO4dKFl1NluVU6dOaW+vibVr1+Lu3bva6/7+/g/s/95772H58uWYOnUq0tLSANyrwZ07d7RBqToBAQHYsWMHfv/992q3An3//fcoKyvD5s2bdf6SrNrlaAp9+/ZFVlYWvvnmm2q/CuGvGjRooPfFiOXl5cjJyanR/fbr1w+rV69GRkYGTpw4ASGEdvcX8GftFQrFQ2tpSE0fy4dp2rQpNBoNzpw5g6eeekrbnpeXh8LCQu3zrerfs2fP6mzBuHnzptF/9QcFBSEoKAhTp07Fvn370KlTJyQnJ+P9998HYPyH76O8Hg2Ry+VITExEjx49sGjRIkyePLlGj1PTpk1x9uxZvXZDbdUx9vUFGPee0rBhQ8TExCAmJgZ37txB165dMWPGjGoDkLHPAzIvHgNEFvP666+jsrISs2fP1rutoqJC+4EYHh4OJycnJCYm6p3x8qC/eG7evKlz3cbGRrsZ+v5TS6u0b98e7u7uSE5O1umzfft2nDhxAn369DFq3f6qU6dOCAsL014e9qHp4uKCN998Ezt27MCRI0cA3KtVVlYWduzYode/sLAQFRUVAIBXXnkFQgjMnDlTr19Vrar+evxr7W7duoWVK1fWeN2qM3r0aDRu3BjvvPMOTp8+rXf7jRs3tB+8wL0PoKrjLKp8/vnnNf46gbCwMDRs2BCpqalITU1Fhw4ddAKDu7s7unfvjmXLlhkMV/n5+Q8cv6aP5cNERkYCgM5ZfACQlJQEANrn2/PPPw9bW1u9rypYtGjRQ++jqKhI+/yoEhQUBBsbG53neL169Yz6du7avh4fpHv37ujQoQMWLFiA0tLSGj1OERERyMrK0r5WgHvHEFW3ZdkQY19fxryn3N/H0dERzZo1q/Y9BzD+eUDmxS1AZDHdunXDm2++icTERBw5cgTh4eFQKBQ4c+YMNmzYgIULF+LVV1+Fs7MzPvnkE4wYMQLPPvssBg4ciAYNGuDo0aMoKSmpdnfWiBEj8Pvvv+O5555DkyZNcOnSJXz22Wdo166dzl9Zf6VQKDBv3jzExMSgW7duGDBggPY0eF9fX0yYMMGcJdEaP348FixYgA8++ADr1q3Du+++i82bN+OFF17Qnu5aXFyM//73v9i4cSMuXrwIV1dX9OjRA0OGDMGnn36KM2fOoFevXtBoNPjpp5/Qo0cPxMbGIjw8XPtX7Jtvvok7d+5g+fLlcHd3r/EWl+o0aNAA3333HSIjI9GuXTudb4I+dOgQvv76a53jNkaMGIHRo0fjlVdeQc+ePXH06FHs2LEDrq6uNbpfhUKBf/zjH1i3bh2Ki4vx8ccf6/VZvHgxOnfujKCgIIwcORL+/v7Iy8tDVlYWrl69iqNHjz7ayt/n7NmzOmGvytNPP40+ffogOjoan3/+OQoLC9GtWzccPHgQq1evRlRUFHr06AHg3vcmjR8/HvPnz8eLL76IXr164ejRo9i+fTtcXV0fuPVm9+7diI2NxWuvvYbmzZujoqICa9asgVwuxyuvvKLtFxwcjF27diEpKQleXl7w8/NDSEiI3ni1fT0+zLvvvovXXnsNq1atwujRo41+nCZNmoT/+7//Q8+ePfHWW29pT4N/4okn8Pvvvxu1ZcvY15cx7yktW7ZE9+7dERwcjIYNG+LXX3/Fxo0bERsbW+39t23b1qjnAZmZ1c4/ozqn6pTWX3755YH9oqOjRb169aq9/fPPPxfBwcHC3t5eODk5iaCgIDFp0iRx/fp1nX6bN28WHTt2FPb29sLZ2Vl06NBBfP311zr389fT4Ddu3CjCw8OFu7u7sLOzE0888YR48803RU5OjrbP/acMV0lNTRVPP/20UCqVomHDhmLQoEHa0/oftl73n+JanapTyj/66CODtw8bNkzI5XJx9uxZIYQQt2/fFvHx8aJZs2bCzs5OuLq6io4dO4qPP/5YlJeXa5erqKgQH330kQgMDBR2dnbCzc1N9O7dW2RnZ+vUsk2bNkKlUglfX18xb948sWLFCr3Thmt7GnyV69eviwkTJojmzZsLlUolHBwcRHBwsJgzZ464deuWtl9lZaX417/+JVxdXYWDg4OIiIgQZ8+erfY0+Ac959LT0wUAIZPJdE6r/qtz586JoUOHCk9PT6FQKIS3t7d44YUXxMaNG41aL2NVnbJs6DJ8+HAhhBBqtVrMnDlT+Pn5CYVCIXx8fER8fLzO6d9C3Htcp02bJjw9PYW9vb147rnnxIkTJ0SjRo3E6NGjtf3uf06fP39evPHGGyIgIECoVCrRsGFD0aNHD7Fr1y6d8U+ePCm6du0q7O3tdU6tv/80+CoPez0a8qDHr7KyUgQEBIiAgADtaebGPk6HDx8WXbp0EUqlUjRp0kQkJiaKTz/9VAAQubm5Oo9HdaeoG/P6MuY95f333xcdOnQQLi4uwt7eXgQGBoo5c+bovEYNvUcY+zyobh3uf61SzcmE4FFURER1QWFhIRo0aID3338f7733nrWn81h5++23sWzZMty5c8fkP+VBf088BoiI6DH014Ovq1QdM9K9e3fLTuYxc39tbt68iTVr1qBz584MP2Q0HgNERPQYSk1NxapVqxAZGQlHR0fs3bsXX3/9NcLDw9GpUydrT8+qQkND0b17dzz11FPIy8tDSkoKioqKqv0OISJDGICIiB5Dbdq0ga2tLT788EMUFRVpD4w2dIC11ERGRmLjxo34/PPPIZPJ8MwzzyAlJQVdu3a19tSoDuExQERERCQ5PAaIiIiIJIcBiIiIiCSHxwAZoNFocP36dTg5OZn9N3uIiIjINIQQuH37Nry8vPR+j/J+DEAGXL9+HT4+PtaeBhEREdXClStXHvrDvwxABjg5OQG4V0BnZ2eTjq1Wq7Fz507tz0CQebDOlsE6WwbrbBmss2WYs85FRUXw8fHRfo4/CAOQAVW7vZydnc0SgBwcHODs7MwXmBmxzpbBOlsG62wZrLNlWKLOxhy+woOgiYiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHKsGoB+/PFH9O3bF15eXpDJZNi0adNDl8nMzMQzzzwDpVKJZs2aYdWqVXp9Fi9eDF9fX6hUKoSEhODgwYOmnzwRERHVWVYNQMXFxWjbti0WL15sVP8LFy6gT58+6NGjB44cOYK3334bI0aMwI4dO7R9UlNTERcXh4SEBBw6dAht27ZFREQEbty4Ya7VICIiojrGqj+G2rt3b/Tu3dvo/snJyfDz88P8+fMBAE899RT27t2LTz75BBEREQCApKQkjBw5EjExMdpltm7dihUrVmDy5MmmX4kaKCpV4/fbd/F7GXCt8C5sbdVWnc/fWUVFBetsAayzZbDOllGX6+zupIKdLY9qqYk69WvwWVlZCAsL02mLiIjA22+/DQAoLy9HdnY24uPjtbfb2NggLCwMWVlZ1Y5bVlaGsrIy7fWioiIA936xVq023Yvgy58v4OP0MwBsMfPQTyYbl6rDOlsG62wZrLNl1M06+zVywPZ/doLc5uG/gm5tVZ+rpvx8vX9sY9SpAJSbmwsPDw+dNg8PDxQVFeHu3bv4448/UFlZabDPyZMnqx03MTERM2fO1GvfuXMnHBwcTDN5AGeuy6CQMaETEZFpCAAVQoYLN0vw3ZbtcKhDn+rp6ekmH7OkpMTovnWoVOYTHx+PuLg47fWioiL4+PggPDwczs7OJrufSACJajXS09PRs2dPKBQKk41NutSss0WwzpbBOltGXaxzRaUGT83YBQDo2bMn6ts//vM2Z52r9uAYo04FIE9PT+Tl5em05eXlwdnZGfb29pDL5ZDL5Qb7eHp6VjuuUqmEUqnUa1coFGZ7EZhzbPoT62wZrLNlsM6WUZfqLLPRaP+vsK078wbMU+eajFen9seEhoYiIyNDpy09PR2hoaEAADs7OwQHB+v00Wg0yMjI0PYhIiIismoAunPnDo4cOYIjR44AuHea+5EjR3D58mUA93ZNDR06VNt/9OjROH/+PCZNmoSTJ09iyZIlWL9+PSZMmKDtExcXh+XLl2P16tU4ceIExowZg+LiYu1ZYURERERW3QX266+/okePHtrrVcfhREdHY9WqVcjJydGGIQDw8/PD1q1bMWHCBCxcuBBNmjTBF198oT0FHgD69euH/Px8TJ8+Hbm5uWjXrh3S0tL0DowmIiIi6bJqAOrevTuEENXebuhbnrt3747Dhw8/cNzY2FjExsY+6vSIiIjob6pOHQNEREREZAoMQERERCQ5DEBEREQkOQxAREREJDl16osQiYiIyHw0GoHySg3K1BqUVVaiTK3RXr/3byXKKjQor9Dc+9dQH+3tlXjS3QkDQ56w9moZxABERET0N5Dy8wXIAJT9L3yUqu/9W1bxv0BTUfnnv/8LMFWBpiq0lFdqHno/NdXlSVf4NDTd72qaCgMQERFRHSWTyaCQy6CuFPg044yJxwaUtjawk9tAqZD/718bKG3lsLO1gfJ/l6p2O7nN/9rv3b72wCWUqjUoKa806bxMhQGIiIiojpLbyPB+VGv8dKYAKoUcqv8FFKWtDVSKe0FE9b//K/9ym11Vm+2fbfeHG1sbGWQyWa3n9u8j11CqLjfh2poWAxAREVEd1u/ZJ9Dv2cfzOJvHGc8CIyIiIslhACIiIiLJYQAiIiIiyWEAIiIiIslhACIiIiLJYQAiIiIiyWEAIiIiIsnh9wARERGR2Wz49Qoc7OS4U1aJO2Vq3L6rRl6uDVr/vxIEeNS32rwYgIiIiMjkbG3u7WT6Yu8FA7fa4LvD1zGxFwMQERER/Y3ERwZi639yUE9pC0elLRxV9/7NPHUDv1z8AxUaYdX5MQARERGRyb3UzhsvtfPWa88vuotfLv5hhRnp4kHQREREJDkMQERERCQ5DEBEREQkOQxAREREJDkMQERERCQ5DEBEREQkOQxAREREJDkMQERERCQ5DEBEREQkOQxAREREJDkMQERERCQ5DEBEREQkOQxAREREJDlWD0CLFy+Gr68vVCoVQkJCcPDgwWr7qtVqzJo1CwEBAVCpVGjbti3S0tJ0+syYMQMymUznEhgYaO7VICIiojrEqgEoNTUVcXFxSEhIwKFDh9C2bVtERETgxo0bBvtPnToVy5Ytw2effYbjx49j9OjRePnll3H48GGdfq1atUJOTo72snfvXkusDhEREdURVg1ASUlJGDlyJGJiYtCyZUskJyfDwcEBK1asMNh/zZo1mDJlCiIjI+Hv748xY8YgMjIS8+fP1+lna2sLT09P7cXV1dUSq0NERER1hK217ri8vBzZ2dmIj4/XttnY2CAsLAxZWVkGlykrK4NKpdJps7e319vCc+bMGXh5eUGlUiE0NBSJiYl44oknqp1LWVkZysrKtNeLiooA3Nvlplara7xuD1I1nqnHJV2ss2WwzpbBOlsG62wZGo0GAFBZWWm2z1hjyIQQwqT3bqTr16/D29sb+/btQ2hoqLZ90qRJ2LNnDw4cOKC3zMCBA3H06FFs2rQJAQEByMjIwEsvvYTKykptgNm+fTvu3LmDFi1aICcnBzNnzsS1a9dw7NgxODk5GZzLjBkzMHPmTL32r776Cg4ODiZaYyIiIvr2og325NggzFuDvk9oTDp2SUkJBg4ciFu3bsHZ2fmBfa22Bag2Fi5ciJEjRyIwMBAymQwBAQGIiYnR2WXWu3dv7f/btGmDkJAQNG3aFOvXr8fw4cMNjhsfH4+4uDjt9aKiIvj4+CA8PPyhBawptVqN9PR09OzZEwqFwqRj059YZ8tgnS2DdbYM1tkysrccB3Kuws/XF5G9THuSUtUeHGNYLQC5urpCLpcjLy9Ppz0vLw+enp4Gl3Fzc8OmTZtQWlqKmzdvwsvLC5MnT4a/v3+19+Pi4oLmzZvj7Nmz1fZRKpVQKpV67QqFwmwvAnOOTX9inS2DdbYM1tkyWGfzsrG5d/ixXC43eZ1rMp7VDoK2s7NDcHAwMjIytG0ajQYZGRk6u8QMUalU8Pb2RkVFBb755hu89NJL1fa9c+cOzp07h8aNG5ts7kRERFS3WfUssLi4OCxfvhyrV6/GiRMnMGbMGBQXFyMmJgYAMHToUJ2DpA8cOIBvv/0W58+fx08//YRevXpBo9Fg0qRJ2j4TJ07Enj17cPHiRezbtw8vv/wy5HI5BgwYYPH1IyIioseTVY8B6tevH/Lz8zF9+nTk5uaiXbt2SEtLg4eHBwDg8uXL2k1lAFBaWoqpU6fi/PnzcHR0RGRkJNasWQMXFxdtn6tXr2LAgAG4efMm3Nzc0LlzZ+zfvx9ubm6WXj0iIiJ6TFn9IOjY2FjExsYavC0zM1Pnerdu3XD8+PEHjrdu3TpTTY2IiIj+pqz+UxhERERElsYARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREkmP1ALR48WL4+vpCpVIhJCQEBw8erLavWq3GrFmzEBAQAJVKhbZt2yItLe2RxiQiIiLpsWoASk1NRVxcHBISEnDo0CG0bdsWERERuHHjhsH+U6dOxbJly/DZZ5/h+PHjGD16NF5++WUcPny41mMSERGR9Fg1ACUlJWHkyJGIiYlBy5YtkZycDAcHB6xYscJg/zVr1mDKlCmIjIyEv78/xowZg8jISMyfP7/WYxIREZH02FrrjsvLy5GdnY34+Hhtm42NDcLCwpCVlWVwmbKyMqhUKp02e3t77N27t9ZjVo1bVlamvV5UVATg3i43tVpd85V7gKrxTD0u6WKdLYN1tgzW2TJYZ8vQaDQAgMrKSrN9xhrDagGooKAAlZWV8PDw0Gn38PDAyZMnDS4TERGBpKQkdO3aFQEBAcjIyMC3336LysrKWo8JAImJiZg5c6Ze+86dO+Hg4FDTVTNKenq6WcYlXayzZbDOlsE6WwbrbF6XLtsAsMGFixexbdt5k45dUlJidF+rBaDaWLhwIUaOHInAwEDIZDIEBAQgJibmkXdvxcfHIy4uTnu9qKgIPj4+CA8Ph7Oz86NOW4darUZ6ejp69uwJhUJh0rHpT6yzZbDOlsE6WwbrbBnZW44DOVfh5+uLyF6BJh27ag+OMawWgFxdXSGXy5GXl6fTnpeXB09PT4PLuLm5YdOmTSgtLcXNmzfh5eWFyZMnw9/fv9ZjAoBSqYRSqdRrVygUZnsRmHNs+hPrbBmss2WwzpbBOpuXjc29w4/lcrnJ61yT8ax2ELSdnR2Cg4ORkZGhbdNoNMjIyEBoaOgDl1WpVPD29kZFRQW++eYbvPTSS488JhEREUmHVXeBxcXFITo6Gu3bt0eHDh2wYMECFBcXIyYmBgAwdOhQeHt7IzExEQBw4MABXLt2De3atcO1a9cwY8YMaDQaTJo0yegxiYiIiKwagPr164f8/HxMnz4dubm5aNeuHdLS0rQHMV++fFm7qQwASktLMXXqVJw/fx6Ojo6IjIzEmjVr4OLiYvSYRERERFY/CDo2NhaxsbEGb8vMzNS53q1bNxw/fvyRxiQiIiKy+k9hEBEREVkaAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSY7VA9DixYvh6+sLlUqFkJAQHDx48IH9FyxYgBYtWsDe3h4+Pj6YMGECSktLtbfPmDEDMplM5xIYGGju1SAiIqI6xNaad56amoq4uDgkJycjJCQECxYsQEREBE6dOgV3d3e9/l999RUmT56MFStWoGPHjjh9+jSGDRsGmUyGpKQkbb9WrVph165d2uu2tlZdTSIiInrMWHULUFJSEkaOHImYmBi0bNkSycnJcHBwwIoVKwz237dvHzp16oSBAwfC19cX4eHhGDBggN5WI1tbW3h6emovrq6ullgdIiIiqiOsFoDKy8uRnZ2NsLCwPydjY4OwsDBkZWUZXKZjx47Izs7WBp7z589j27ZtiIyM1Ol35swZeHl5wd/fH4MGDcLly5fNtyJERERU51ht31BBQQEqKyvh4eGh0+7h4YGTJ08aXGbgwIEoKChA586dIYRARUUFRo8ejSlTpmj7hISEYNWqVWjRogVycnIwc+ZMdOnSBceOHYOTk5PBccvKylBWVqa9XlRUBABQq9VQq9WPuqo6qsYz9biki3W2DNbZMlhny2CdLUOj0QAAKisrzfYZa4w6dXBMZmYm5s6diyVLliAkJARnz57F+PHjMXv2bEybNg0A0Lt3b23/Nm3aICQkBE2bNsX69esxfPhwg+MmJiZi5syZeu07d+6Eg4ODWdYlPT3dLOOSLtbZMlhny2CdLYN1Nq9Ll20A2ODCxYvYtu28SccuKSkxuq/VApCrqyvkcjny8vJ02vPy8uDp6WlwmWnTpmHIkCEYMWIEACAoKAjFxcUYNWoU3nvvPdjY6O/Rc3FxQfPmzXH27Nlq5xIfH4+4uDjt9aKiIvj4+CA8PBzOzs61Wb1qqdVqpKeno2fPnlAoFCYdm/7EOlsG62wZrLNlsM6Wkb3lOJBzFX6+vojsZdqztKv24BjDagHIzs4OwcHByMjIQFRUFIB7m8UyMjIQGxtrcJmSkhK9kCOXywEAQgiDy9y5cwfnzp3DkCFDqp2LUqmEUqnUa1coFGZ7EZhzbPoT62wZrLNlsM6WwTqbV9XnuFwuN3mdazKeVXeBxcXFITo6Gu3bt0eHDh2wYMECFBcXIyYmBgAwdOhQeHt7IzExEQDQt29fJCUl4emnn9buAps2bRr69u2rDUITJ05E37590bRpU1y/fh0JCQmQy+UYMGCA1daTiIiIHi9WDUD9+vVDfn4+pk+fjtzcXLRr1w5paWnaA6MvX76ss8Vn6tSpkMlkmDp1Kq5duwY3Nzf07dsXc+bM0fa5evUqBgwYgJs3b8LNzQ2dO3fG/v374ebmZvH1IyIioseT1Q+Cjo2NrXaXV2Zmps51W1tbJCQkICEhodrx1q1bZ8rpERER0d+Q1X8Kg4iIiMjSarUFqLKyEqtWrUJGRgZu3LihPae/yu7du00yOSIiIiJzqFUAGj9+PFatWoU+ffqgdevWkMlkpp4XERERkdnUKgCtW7cO69ev1/sJCiIiIqK6oFbHANnZ2aFZs2amngsRERGRRdQqAL3zzjtYuHBhtV8+SERERPQ4q9UusL179+KHH37A9u3b0apVK71vXvz2229NMjkiIiIic6hVAHJxccHLL79s6rkQERERWUStAtDKlStNPQ8iIiIii3mkb4LOz8/HqVOnAAAtWrTgz00QERFRnVCrg6CLi4vxxhtvoHHjxujatSu6du0KLy8vDB8+HCUlJaaeIxEREZFJ1SoAxcXFYc+ePfj+++9RWFiIwsJC/Pvf/8aePXvwzjvvmHqORERERCZVq11g33zzDTZu3Iju3btr2yIjI2Fvb4/XX38dS5cuNdX8iIiIiEyuVluASkpK4OHhodfu7u7OXWBERET02KtVAAoNDUVCQgJKS0u1bXfv3sXMmTMRGhpqsskRERERmUOtdoEtXLgQERERaNKkCdq2bQsAOHr0KFQqFXbs2GHSCRIRERGZWq0CUOvWrXHmzBmsXbsWJ0+eBAAMGDAAgwYNgr29vUknSERERGRqtf4eIAcHB4wcOdKUcyEiIiKyCKMD0ObNm9G7d28oFAps3rz5gX1ffPHFR54YERERkbkYHYCioqKQm5sLd3d3REVFVdtPJpOhsrLSFHMjIiIiMgujA5BGozH4fyIiIqK6planwRtSWFhoqqGIiIiIzKpWAWjevHlITU3VXn/ttdfQsGFDeHt74+jRoyabHBEREZE51CoAJScnw8fHBwCQnp6OXbt2IS0tDb1798a7775r0gkSERERmVqtToPPzc3VBqAtW7bg9ddfR3h4OHx9fRESEmLSCRIRERGZWq22ADVo0ABXrlwBAKSlpSEsLAwAIITgGWBERET02KvVFqB//OMfGDhwIJ588kncvHkTvXv3BgAcPnwYzZo1M+kEiYiIiEytVgHok08+ga+vL65cuYIPP/wQjo6OAICcnByMHTvWpBMkIiIiMrVaBSCFQoGJEyfqtU+YMOGRJ0RERERkbvwpDCIiIpIc/hQGERERSQ5/CoOIiIgkx2Q/hUFERERUV9QqAP3zn//Ep59+qte+aNEivP322486JyIiIiKzqlUA+uabb9CpUye99o4dO2Ljxo01Gmvx4sXw9fWFSqVCSEgIDh48+MD+CxYsQIsWLWBvbw8fHx9MmDABpaWljzQmERERSUutAtDNmzdRv359vXZnZ2cUFBQYPU5qairi4uKQkJCAQ4cOoW3btoiIiMCNGzcM9v/qq68wefJkJCQk4MSJE0hJSUFqaiqmTJlS6zGJiIhIemoVgJo1a4a0tDS99u3bt8Pf39/ocZKSkjBy5EjExMSgZcuWSE5OhoODA1asWGGw/759+9CpUycMHDgQvr6+CA8Px4ABA3S28NR0TCIiIpKeWn0RYlxcHGJjY5Gfn4/nnnsOAJCRkYH58+djwYIFRo1RXl6O7OxsxMfHa9tsbGwQFhaGrKwsg8t07NgR//d//4eDBw+iQ4cOOH/+PLZt24YhQ4bUekwAKCsrQ1lZmfZ6UVERAECtVkOtVhu1PsaqGs/U45Iu1tkyWGfLYJ0tg3W2jKozySsrK832GWuMWgWgN954A2VlZZgzZw5mz54NAPD19cXSpUsxdOhQo8YoKChAZWUlPDw8dNo9PDxw8uRJg8sMHDgQBQUF6Ny5M4QQqKiowOjRo7W7wGozJgAkJiZi5syZeu07d+6Eg4ODUetTU+np6WYZl3SxzpbBOlsG62wZrLN5XbpsA8AGFy5exLZt5006dklJidF9axWAAGDMmDEYM2YM8vPzYW9vr/09MHPKzMzE3LlzsWTJEoSEhODs2bMYP348Zs+ejWnTptV63Pj4eMTFxWmvFxUVwcfHB+Hh4XB2djbF1LXUajXS09PRs2dPKBQKk45Nf2KdLYN1tgzW2TJYZ8vI3nIcyLkKP19fRPYKNOnYVXtwjFHrAFRRUYHMzEycO3cOAwcOBABcv34dzs7ORoUhV1dXyOVy5OXl6bTn5eXB09PT4DLTpk3DkCFDMGLECABAUFAQiouLMWrUKLz33nu1GhMAlEollEqlXrtCoTDbi8CcY9OfWGfLYJ0tg3W2DNbZvGxs7h1+LJfLTV7nmoxXq4OgL126hKCgILz00ksYN24c8vPzAQDz5s0z+COphtjZ2SE4OBgZGRnaNo1Gg4yMDISGhhpcpqSkRFu4KnK5HAAghKjVmERERCQ9tQpA48ePR/v27fHHH3/A3t5e2/7yyy/rhI+HiYuLw/Lly7F69WqcOHECY8aMQXFxMWJiYgAAQ4cO1TmguW/fvli6dCnWrVuHCxcuID09HdOmTUPfvn21QehhYxIRERHVahfYTz/9hH379sHOzk6n3dfXF9euXTN6nH79+iE/Px/Tp09Hbm4u2rVrh7S0NO1BzJcvX9bZ4jN16lTIZDJMnToV165dg5ubG/r27Ys5c+YYPSYRERFRrQKQRqMx+IvvV69ehZOTU43Gio2NRWxsrMHbMjMzda7b2toiISEBCQkJtR6TiIiIqFa7wMLDw3W+70cmk+HOnTtISEhAZGSkqeZGREREZBa12gL08ccfo1evXmjZsiVKS0sxcOBAnDlzBq6urvj6669NPUciIiIik6pVAPLx8cHRo0eRmpqKo0eP4s6dOxg+fDgGDRqkc1A0ERER0eOoxgFIrVYjMDAQW7ZswaBBgzBo0CBzzIuIiIjIbGp8DJBCoUBpaak55kJERERkEbU6CHrcuHGYN28eKioqTD0fIiIiIrOr1TFAv/zyCzIyMrBz504EBQWhXr16Ord/++23JpkcERERkTnUKgC5uLjglVdeMfVciIiIiCyiRgFIo9Hgo48+wunTp1FeXo7nnnsOM2bM4JlfREREVKfU6BigOXPmYMqUKXB0dIS3tzc+/fRTjBs3zlxzIyIiIjKLGgWgL7/8EkuWLMGOHTuwadMmfP/991i7di00Go255kdERERkcjUKQJcvX9b5qYuwsDDIZDJcv37d5BMjIiIiMpcaBaCKigqoVCqdNoVCAbVabdJJEREREZlTjQ6CFkJg2LBhUCqV2rbS0lKMHj1a51R4ngZPREREj7MaBaDo6Gi9tsGDB5tsMkRERESWUKMAtHLlSnPNg4iIiMhiavVTGERERER1GQMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSQ4DEBEREUkOAxARERFJDgMQERERSc5jEYAWL14MX19fqFQqhISE4ODBg9X27d69O2Qymd6lT58+2j7Dhg3Tu71Xr16WWBUiIiKqA2ytPYHU1FTExcUhOTkZISEhWLBgASIiInDq1Cm4u7vr9f/2229RXl6uvX7z5k20bdsWr732mk6/Xr16YeXKldrrSqXSfCtBREREdYrVtwAlJSVh5MiRiImJQcuWLZGcnAwHBwesWLHCYP+GDRvC09NTe0lPT4eDg4NeAFIqlTr9GjRoYInVISIiojrAqgGovLwc2dnZCAsL07bZ2NggLCwMWVlZRo2RkpKC/v37o169ejrtmZmZcHd3R4sWLTBmzBjcvHnTpHMnIiKiusuqu8AKCgpQWVkJDw8PnXYPDw+cPHnyocsfPHgQx44dQ0pKik57r1698I9//AN+fn44d+4cpkyZgt69eyMrKwtyuVxvnLKyMpSVlWmvFxUVAQDUajXUanVtVq1aVeOZelzSxTpbButsGayzZbDOlqHRaAAAlZWVZvuMNYbVjwF6FCkpKQgKCkKHDh102vv376/9f1BQENq0aYOAgABkZmbi+eef1xsnMTERM2fO1GvfuXMnHBwcTD9xAOnp6WYZl3SxzpbBOlsG62wZrLN5XbpsA8AGFy5exLZt5006dklJidF9rRqAXF1dIZfLkZeXp9Oel5cHT0/PBy5bXFyMdevWYdasWQ+9H39/f7i6uuLs2bMGA1B8fDzi4uK014uKiuDj44Pw8HA4OzsbuTbGUavVSE9PR8+ePaFQKEw6Nv2JdbYM1tkyWGfLYJ0tI3vLcSDnKvx8fRHZK9CkY1ftwTGGVQOQnZ0dgoODkZGRgaioKAD3No1lZGQgNjb2gctu2LABZWVlGDx48EPv5+rVq7h58yYaN25s8HalUmnwLDGFQmG2F4E5x6Y/sc6WwTpbButsGayzednY3Dv8WC6Xm7zONRnP6meBxcXFYfny5Vi9ejVOnDiBMWPGoLi4GDExMQCAoUOHIj4+Xm+5lJQUREVFoVGjRjrtd+7cwbvvvov9+/fj4sWLyMjIwEsvvYRmzZohIiLCIutEREREjzerHwPUr18/5OfnY/r06cjNzUW7du2QlpamPTD68uXL2rRY5dSpU9i7dy927typN55cLsd//vMfrF69GoWFhfDy8kJ4eDhmz57N7wIiIiIiAI9BAAKA2NjYand5ZWZm6rW1aNECQgiD/e3t7bFjxw5TTo+IiIj+Zqy+C4yIiIjI0hiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIchiAiIiISHIYgIiIiEhyGICIiIhIch6LALR48WL4+vpCpVIhJCQEBw8erLZv9+7dIZPJ9C59+vTR9hFCYPr06WjcuDHs7e0RFhaGM2fOWGJViIiIqA6wegBKTU1FXFwcEhIScOjQIbRt2xYRERG4ceOGwf7ffvstcnJytJdjx45BLpfjtdde0/b58MMP8emnnyI5ORkHDhxAvXr1EBERgdLSUkutFhERET3GrB6AkpKSMHLkSMTExKBly5ZITk6Gg4MDVqxYYbB/w4YN4enpqb2kp6fDwcFBG4CEEFiwYAGmTp2Kl156CW3atMGXX36J69evY9OmTRZcMyIiInpc2VrzzsvLy5GdnY34+Hhtm42NDcLCwpCVlWXUGCkpKejfvz/q1asHALhw4QJyc3MRFham7VO/fn2EhIQgKysL/fv31xujrKwMZWVl2utFRUUAALVaDbVaXat1q07VeKYel3SxzpbBOlsG62wZrLNlaDQaAEBlZaXZPmONYdUAVFBQgMrKSnh4eOi0e3h44OTJkw9d/uDBgzh27BhSUlK0bbm5udox7h+z6rb7JSYmYubMmXrtO3fuhIODw0PnURvp6elmGZd0sc6WwTpbButsGayzeV26bAPABhcuXsS2bedNOnZJSYnRfa0agB5VSkoKgoKC0KFDh0caJz4+HnFxcdrrRUVF8PHxQXh4OJydnR91mjrUajXS09PRs2dPKBQKk45Nf2KdLYN1tgzW2TJYZ8vI3nIcyLkKP19fRPYKNOnYVXtwjGHVAOTq6gq5XI68vDyd9ry8PHh6ej5w2eLiYqxbtw6zZs3Saa9aLi8vD40bN9YZs127dgbHUiqVUCqVeu0KhcJsLwJzjk1/Yp0tg3W2DNbZMlhn87KxuXf4sVwuN3mdazKeVQ+CtrOzQ3BwMDIyMrRtGo0GGRkZCA0NfeCyGzZsQFlZGQYPHqzT7ufnB09PT50xi4qKcODAgYeOSURERNJg9V1gcXFxiI6ORvv27dGhQwcsWLAAxcXFiImJAQAMHToU3t7eSExM1FkuJSUFUVFRaNSokU67TCbD22+/jffffx9PPvkk/Pz8MG3aNHh5eSEqKspSq0VERESPMasHoH79+iE/Px/Tp09Hbm4u2rVrh7S0NO1BzJcvX9ZuLqty6tQp7N27Fzt37jQ45qRJk1BcXIxRo0ahsLAQnTt3RlpaGlQqldnXh4iIiB5/Vg9AABAbG4vY2FiDt2VmZuq1tWjRAkKIaseTyWSYNWuW3vFBRERERMBj8EWIRERERJbGAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREksMARERERJLDAERERESSwwBEREREkmP1ALR48WL4+vpCpVIhJCQEBw8efGD/wsJCjBs3Do0bN4ZSqUTz5s2xbds27e0zZsyATCbTuQQGBpp7NYiIiKgOsbXmnaempiIuLg7JyckICQnBggULEBERgVOnTsHd3V2vf3l5OXr27Al3d3ds3LgR3t7euHTpElxcXHT6tWrVCrt27dJet7W16moSERHRY8aqySApKQkjR45ETEwMACA5ORlbt27FihUrMHnyZL3+K1aswO+//459+/ZBoVAAAHx9ffX62drawtPT06xzJyIiorrLarvAysvLkZ2djbCwsD8nY2ODsLAwZGVlGVxm8+bNCA0Nxbhx4+Dh4YHWrVtj7ty5qKys1Ol35swZeHl5wd/fH4MGDcLly5fNui5ERERUt1htC1BBQQEqKyvh4eGh0+7h4YGTJ08aXOb8+fPYvXs3Bg0ahG3btuHs2bMYO3Ys1Go1EhISAAAhISFYtWoVWrRogZycHMycORNdunTBsWPH4OTkZHDcsrIylJWVaa8XFRUBANRqNdRqtSlWV6tqPFOPS7pYZ8tgnS2DdbYM1tkyNBoNAKCystJsn7HGkAkhhEnv3UjXr1+Ht7c39u3bh9DQUG37pEmTsGfPHhw4cEBvmebNm6O0tBQXLlyAXC4HcG832kcffYScnByD91NYWIimTZsiKSkJw4cPN9hnxowZmDlzpl77V199BQcHh9qsHhERERnw7UUb7MmxQZi3Bn2f0Jh07JKSEgwcOBC3bt2Cs7PzA/tabQuQq6sr5HI58vLydNrz8vKqPX6ncePGUCgU2vADAE899RRyc3NRXl4OOzs7vWVcXFzQvHlznD17ttq5xMfHIy4uTnu9qKgIPj4+CA8Pf2gBa0qtViM9PR09e/bUHsdEpsc6WwbrbBmss2WwzpaRveU4kHMVfr6+iOxl2rO0q/bgGMNqAcjOzg7BwcHIyMhAVFQUgHubxTIyMhAbG2twmU6dOuGrr76CRqOBjc29w5dOnz6Nxo0bGww/AHDnzh2cO3cOQ4YMqXYuSqUSSqVSr12hUJjtRWDOselPrLNlsM6WwTpbButsXlWf33K53OR1rsl4Vv0eoLi4OCxfvhyrV6/GiRMnMGbMGBQXF2vPChs6dCji4+O1/ceMGYPff/8d48ePx+nTp7F161bMnTsX48aN0/aZOHEi9uzZg4sXL2Lfvn14+eWXIZfLMWDAAIuvHxERET2erHoafL9+/ZCfn4/p06cjNzcX7dq1Q1pamvbA6MuXL2uTIgD4+Phgx44dmDBhAtq0aQNvb2+MHz8e//rXv7R9rl69igEDBuDmzZtwc3ND586dsX//fri5uVl8/YiIiOjxZPVvCIyNja12l1dmZqZeW2hoKPbv31/teOvWrTPV1IiIiOhvyuo/hUFERERkaQxAREREJDlW3wVWVwkhUFFRofct1A+jVqtha2uL0tLSGi9LxrNWneVyOWxtbSGTySx2n0REVHMMQLVQXl6OnJwclJSU1HhZIQQ8PT1x5coVfkiakTXr7ODg8MCvZiAiIutjAKohjUaj/SZqLy8v2NnZ1egDVqPR4M6dO3B0dNQ5w41Myxp1FkKgvLwc+fn5uHDhAp588kk+xkREjykGoBoqLy+HRqOBj49PrX4mQ6PRoLy8HCqVih+OZmStOtvb20OhUODSpUva+ycioscPP4FrieGFqsPnBhHR44/v1ERERCQ5DEBEREQkOQxAEpOVlQW5XI4+ffro3Xbx4kXIZDLtpVGjRggPD8fhw4fNOqfMzEw888wzUCqVaNasGVatWvXQZdavX4927drBwcEBTZs2xUcffaTXZ/ny5WjVqhXs7e3RokULfPnll3p9FixYgBYtWsDe3h4+Pj6YMGECSktLDd7nBx98AJlMhrfffrumq0hERI8ZBiCJSUlJwVtvvYUff/wR169fN9hn165dyMnJwY4dO3Dnzh307t0bhYWFZpnPhQsX0KdPH/To0QNHjhzB22+/jREjRmDHjh3VLrN9+3YMGjQIo0ePxrFjx7BkyRJ88sknWLRokbbP0qVLMXv2bEyfPh2//fYbZs6ciXHjxuH777/X9vnqq68wefJkJCQk4MSJE0hJSUFqaiqmTJmid5+//PILli1bhjZt2pi2AEREZBUMQBJy584dpKamYsyYMejTp0+1W1oaNWoET09PtG/fHh9//DHy8vJw4MABs8wpOTkZfn5+mD9/Pp566inExsbi1VdfxSeffFLtMmvWrEFUVBRGjx4Nf39/9OnTB/Hx8Zg3bx6EEACAtWvXIjo6Gv369YO/vz/69++PUaNGYd68edpx9u3bh06dOmHgwIHw9fVFeHg4BgwYgIMHD+rc3507dzBo0CAsX74cDRo0MEsdiIjIshiATEAIgZLyCqMvd8sra9S/ukvVh72x1q9fj8DAQLRo0QKDBw/GihUrHjqGvb09gHun/xvy008/wdHR8YGXtWvXVjt+VlYWwsLCdNoiIiKQlZVV7TJlZWV6p5fb29vj6tWruHTp0gP7HDx4EGq1GgDQsWNHZGdnawPP+fPnsW3bNkRGRuosN27cOPTp00dvnkREVHfxe4BM4K66Ei2nV7/LxlyOz4qAg53xD2FKSgoGDx4MAOjVqxdu3bqFPXv2oHv37gb7FxYWYvbs2XB0dESHDh0M9mnfvj2OHDnywPv18PCo9rbc3Fy92z08PFBUVIS7d+9qA9hfRUREYMKECRg2bBh69OiBs2fPYv78+QCAnJwc7daclStX4vXXX0f79u2RnZ2NL774Amq1GgUFBWjcuDEGDhyIgoICdO7cWfvTJqNHj9bZBbZu3TocOnQIv/zyywPXkYiI6hYGIIk4deoUDh48iO+++w4AYGtri379+iElJUUvAHXs2BE2NjYoLi6Gv78/UlNTqw0x9vb2aNasmbmnr2PkyJE4d+4cXnjhBajVajg7O2P8+PGYMWOG9jt4pk6diitXrqBjx44QQsDDwwPR0dH48MMPtX0yMzMxd+5cLFmyBCEhITh79izGjx+P2bNnY9q0abhy5QrGjx+P9PR0fqEhEdHfDAOQCdgr5Dg+K8KovhqNBreLbsPJ2emRvzDPXiE3um9KSgoqKirg5eWlbRNCQKlUYtGiRahfv762PTU1FS1btkSjRo3g4uLywHF/+ukn9O7d+4F9li1bhkGDBhm8zdPTE3l5eTpteXl5cHZ2Nrj1BwBkMhnmzZuHuXPnIjc3F25ubsjIyAAA+Pv7A7gXzBYtWoSUlBTk5+ejcePG+Pzzz+Hk5AQ3NzcAwLRp0zBkyBCMGDECABAUFITi4mKMGjUK7733HrKzs3Hjxg0888wz2vuurKzEjz/+iEWLFqGsrAxyufGPARERPT4YgExAJpMZvStKo9Ggwk4OBztbi31jcEVFBb788kvMnz8f4eHhOrdFRUXh66+/xujRo7VtPj4+CAgIMGrsR90FFhoaim3btum0paenIzQ09KH3LZfL4e3tDQD4+uuvERoaqg03VRQKBZo0aQLg3u6sF154QVv3kpISvcegKtAIIfD888/jv//9r87tMTExCAwMxL/+9S+GHyKiOowBSAK2bNmCP/74A8OHD9fZ0gMAr7zyClJSUnQCUE086i6w0aNHY9GiRZg0aRLeeOMN7N69G+vXr8fWrVu1fRYtWoTvvvtOu5WnoKAAGzduRPfu3VFaWoqVK1diw4YN2LNnj3aZ06dPa49vunXrFpKSknDs2DGsXr1a26dv375ISkrC008/rd0FNm3aNPTt2xdyuRxOTk5o3bq1znzr1auHRo0a6bUTEVHdwgAkASkpKQgLC9MLP8C9APThhx/iP//5D5ydnS0+Nz8/P2zduhUTJkzAwoUL0aRJE3zxxReIiPhzl2JBQQHOnTuns9zq1asxceJECCEQGhqKzMxMnQO1KysrsXjxYkyYMAEKhQI9evTAvn374Ovrq+0zdepUyGQyTJ06FdeuXYObmxv69u2LOXPmmH29iYikylZuA4VMQG4js+o8ZKKm51JLQFFREerXr49bt27phYLS0lJcuHABfn5+tTowVqPRoKioCM7OzvzRTDOyZp0f9TlSl6jVau1XBygUCmtP52+LdbYM1tkyzFnnB31+34+fwERERCQ5DEBEREQkOQxAREREJDkMQERERCQ5DEBEREQkOQxAtcST56g6fG4QET3+GIBqqOqUvZKSEivPhB5XVc8NnkZLRPT44hch1pBcLoeLiwtu3LgBAHBwcIBMZvyXOWk0GpSXl6O0tJTfA2RG1qizEAIlJSW4ceMGXFxc+FMZRESPMQagWvD09AQAbQiqCSEE7t69C3t7+xoFJ6oZa9bZxcVF+xwhIqLHEwNQLchkMjRu3Bju7u5Qq9U1WlatVuPHH39E165duYvEjKxVZ4VCwS0/RER1AAPQI5DL5TX+sJPL5aioqIBKpWIAMiPWmYiIHoQHoRAREZHkMAARERGR5DAAERERkeTwGCADqr7IrqioyORjq9VqlJSUoKioiMemmBHrbBmss2WwzpbBOluGOetc9bltzBfSMgAZcPv2bQCAj4+PlWdCRERENXX79m3Ur1//gX1kgt/br0ej0eD69etwcnIy+XfIFBUVwcfHB1euXIGzs7NJx6Y/sc6WwTpbButsGayzZZizzkII3L59G15eXg/9ElxuATLAxsYGTZo0Met9ODs78wVmAayzZbDOlsE6WwbrbBnmqvPDtvxU4UHQREREJDkMQERERCQ5DEAWplQqkZCQAKVSae2p/K2xzpbBOlsG62wZrLNlPC515kHQREREJDncAkRERESSwwBEREREksMARERERJLDAERERESSwwBkBosXL4avry9UKhVCQkJw8ODBB/bfsGEDAgMDoVKpEBQUhG3btllopnVbTeq8fPlydOnSBQ0aNECDBg0QFhb20MeF7qnp87nKunXrIJPJEBUVZd4J/k3UtM6FhYUYN24cGjduDKVSiebNm/O9wwg1rfOCBQvQokUL2Nvbw8fHBxMmTEBpaamFZls3/fjjj+jbty+8vLwgk8mwadOmhy6TmZmJZ555BkqlEs2aNcOqVavMPk8IMql169YJOzs7sWLFCvHbb7+JkSNHChcXF5GXl2ew/88//yzkcrn48MMPxfHjx8XUqVOFQqEQ//3vfy0887qlpnUeOHCgWLx4sTh8+LA4ceKEGDZsmKhfv764evWqhWdet9S0zlUuXLggvL29RZcuXcRLL71kmcnWYTWtc1lZmWjfvr2IjIwUe/fuFRcuXBCZmZniyJEjFp553VLTOq9du1YolUqxdu1aceHCBbFjxw7RuHFjMWHCBAvPvG7Ztm2beO+998S3334rAIjvvvvugf3Pnz8vHBwcRFxcnDh+/Lj47LPPhFwuF2lpaWadJwOQiXXo0EGMGzdOe72yslJ4eXmJxMREg/1ff/110adPH522kJAQ8eabb5p1nnVdTet8v4qKCuHk5CRWr15trin+LdSmzhUVFaJjx47iiy++ENHR0QxARqhpnZcuXSr8/f1FeXm5pab4t1DTOo8bN04899xzOm1xcXGiU6dOZp3n34kxAWjSpEmiVatWOm39+vUTERERZpyZENwFZkLl5eXIzs5GWFiYts3GxgZhYWHIysoyuExWVpZOfwCIiIiotj/Vrs73KykpgVqtRsOGDc01zTqvtnWeNWsW3N3dMXz4cEtMs86rTZ03b96M0NBQjBs3Dh4eHmjdujXmzp2LyspKS027zqlNnTt27Ijs7GztbrLz589j27ZtiIyMtMicpcJan4P8MVQTKigoQGVlJTw8PHTaPTw8cPLkSYPL5ObmGuyfm5trtnnWdbWp8/3+9a9/wcvLS+9FR3+qTZ337t2LlJQUHDlyxAIz/HuoTZ3Pnz+P3bt3Y9CgQdi2bRvOnj2LsWPHQq1WIyEhwRLTrnNqU+eBAweioKAAnTt3hhACFRUVGD16NKZMmWKJKUtGdZ+DRUVFuHv3Luzt7c1yv9wCRJLzwQcfYN26dfjuu++gUqmsPZ2/jdu3b2PIkCFYvnw5XF1drT2dvzWNRgN3d3d8/vnnCA4ORr9+/fDee+8hOTnZ2lP7W8nMzMTcuXOxZMkSHDp0CN9++y22bt2K2bNnW3tqZALcAmRCrq6ukMvlyMvL02nPy8uDp6enwWU8PT1r1J9qV+cqH3/8MT744APs2rULbdq0Mec067ya1vncuXO4ePEi+vbtq23TaDQAAFtbW5w6dQoBAQHmnXQdVJvnc+PGjaFQKCCXy7VtTz31FHJzc1FeXg47Ozuzzrkuqk2dp02bhiFDhmDEiBEAgKCgIBQXF2PUqFF47733YGPDbQimUN3noLOzs9m2/gDcAmRSdnZ2CA4ORkZGhrZNo9EgIyMDoaGhBpcJDQ3V6Q8A6enp1fan2tUZAD788EPMnj0baWlpaN++vSWmWqfVtM6BgYH473//iyNHjmgvL774Inr06IEjR47Ax8fHktOvM2rzfO7UqRPOnj2rDZgAcPr0aTRu3Jjhpxq1qXNJSYleyKkKnYI/o2kyVvscNOsh1hK0bt06oVQqxapVq8Tx48fFqFGjhIuLi8jNzRVCCDFkyBAxefJkbf+ff/5Z2Nraio8//licOHFCJCQk8DR4I9S0zh988IGws7MTGzduFDk5OdrL7du3rbUKdUJN63w/ngVmnJrW+fLly8LJyUnExsaKU6dOiS1btgh3d3fx/vvvW2sV6oSa1jkhIUE4OTmJr7/+Wpw/f17s3LlTBAQEiNdff91aq1An3L59Wxw+fFgcPnxYABBJSUni8OHD4tKlS0IIISZPniyGDBmi7V91Gvy7774rTpw4IRYvXszT4Ouqzz77TDzxxBPCzs5OdOjQQezfv197W7du3UR0dLRO//Xr14vmzZsLOzs70apVK7F161YLz7huqkmdmzZtKgDoXRISEiw/8Tqmps/nv2IAMl5N67xv3z4REhIilEql8Pf3F3PmzBEVFRUWnnXdU5M6q9VqMWPGDBEQECBUKpXw8fERY8eOFX/88YflJ16H/PDDDwbfb6tqGx0dLbp166a3TLt27YSdnZ3w9/cXK1euNPs8ZUJwOx4RERFJC48BIiIiIslhACIiIiLJYQAiIiIiyWEAIiIiIslhACIiIiLJYQAiIiIiyWEAIiIiIslhACIiMpJMJsOmTZsAABcvXoRMJsORI0esOiciqh0GICKqE4YNGwaZTAaZTAaFQgE/Pz9MmjQJpaWl1p4aEdVB/DV4IqozevXqhZUrV0KtViM7OxvR0dGQyWSYN2+etadGRHUMtwARUZ2hVCrh6ekJHx8fREVFISwsDOnp6QDu/bJ3YmIi/Pz8YG9vj7Zt22Ljxo06y//222944YUX4OzsDCcnJ3Tp0gXnzp0DAPzyyy/o2bMnXF1dUb9+fXTr1g2HDh2y+DoSkWUwABFRnXTs2DHs27cPdnZ2AIDExER8+eWXSE5Oxm+//YYJEyZg8ODB2LNnDwDg2rVr6Nq1K5RKJXbv3o3s7Gy88cYbqKioAADcvn0b0dHR2Lt3L/bv348nn3wSkZGRuH37ttXWkYjMh7vAiKjO2LJlCxwdHVFRUYGysjLY2Nhg0aJFKCsrw9y5c7Fr1y6EhoYCAPz9/bF3714sW7YM3bp1w+LFi1G/fn2sW7cOCoUCANC8eXPt2M8995zOfX3++edwcXHBnj178MILL1huJYnIIhiAiKjO6NGjB5YuXYri4mJ88sknsLW1xSuvvILffvsNJSUl6Nmzp07/8vJyPP300wCAI0eOoEuXLtrwc7+8vDxMnToVmZmZuHHjBiorK1FSUoLLly+bfb2IyPIYgIiozqhXrx6aNWsGAFixYgXatm2LlJQUtG7dGgCwdetWeHt76yyjVCoBAPb29g8cOzo6Gjdv3sTChQvRtGlTKJVKhIaGory83AxrQkTWxgBERHWSjY0NpkyZgri4OJw+fRpKpRKXL19Gt27dDPZv06YNVq9eDbVabXAr0M8//4wlS5YgMjISAHDlyhUUFBSYdR2IyHp4EDQR1VmvvfYa5HI5li1bhokTJ2LChAlYvXo1zp07h0OHDuGzzz7D6tWrAQCxsbEoKipC//798euvv+LMmTNYs2YNTp06BQB48sknsWbNGpw4cQIHDhzAoEGDHrrViIjqLm4BIqI6y9bWFrGxsfjwww9x4cIFuLm5ITExEefPn4eLiwueeeYZTJkyBQDQqFEj7N69G++++y66desGuVyOdu3aoVOnTgCAlJQUjBo1Cs888wx8fHwwd+5cTJw40ZqrR0RmJBNCCGtPgoiIiMiSuAuMiIiIJIcBiIiIiCSHAYiIiIgkhwGIiIiIJIcBiIiIiCSHAYiIiIgkhwGIiIiIJIcBiIiIiCSHAYiIiIgkhwGIiIiIJIcBiIiIiCSHAYiIiIgk5/8DG394Pie7Y8AAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "# Step 5: Train and evaluate each solver\n",
        "for solver in solvers:\n",
        "    print(f\"\\nTraining with solver: {solver}\")\n",
        "    model = LogisticRegression(solver=solver, max_iter=5000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = accuracy\n",
        "    print(f\"Accuracy with '{solver}': {round(accuracy, 4)}\")\n",
        "\n",
        "# Step 6: Summary\n",
        "print(\"\\n✅ Comparison of Solvers by Accuracy:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver:10}: {round(acc, 4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99uTQDb42FSZ",
        "outputId": "ea04fd50-ffec-449a-a701-bfbc531f8dda"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with solver: liblinear\n",
            "Accuracy with 'liblinear': 0.9737\n",
            "\n",
            "Training with solver: saga\n",
            "Accuracy with 'saga': 0.9737\n",
            "\n",
            "Training with solver: lbfgs\n",
            "Accuracy with 'lbfgs': 0.9737\n",
            "\n",
            "✅ Comparison of Solvers by Accuracy:\n",
            "liblinear : 0.9737\n",
            "saga      : 0.9737\n",
            "lbfgs     : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Compute Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 7: Output result\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", round(mcc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPB8nGIm2d7O",
        "outputId": "f02b699b-1002-4419-d6fc-c9e3ab9bd9a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train on raw (unscaled) data\n",
        "model_raw = LogisticRegression(solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 4: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train on standardized data\n",
        "model_scaled = LogisticRegression(solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print comparison\n",
        "print(\"Accuracy on Raw (Unscaled) Data     :\", round(accuracy_raw, 4))\n",
        "print(\"Accuracy on Standardized Data       :\", round(accuracy_scaled, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FaCLDu52-FK",
        "outputId": "7c758206-3fcf-423f-c6ec-d8bef4fee68e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw (Unscaled) Data     : 0.9561\n",
            "Accuracy on Standardized Data       : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Define Logistic Regression and parameter grid for C\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Step 5: Use GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Best C and performance\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Output results\n",
        "print(\"Best Regularization Strength (C):\", best_C)\n",
        "print(\"Accuracy on Test Set with Best C:\", round(accuracy, 4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDqqfDuT3ddl",
        "outputId": "bd8a4c66-9e2e-4e41-c04f-63919f96ab49"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Regularization Strength (C): 0.1\n",
            "Accuracy on Test Set with Best C: 0.9912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Save model and scaler using joblib\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"✅ Model and scaler saved.\")\n",
        "\n",
        "# Step 6: Load model and scaler\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "print(\"✅ Model and scaler loaded.\")\n",
        "\n",
        "# Step 7: Make prediction on test set\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)\n",
        "y_pred = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Step 8: Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with loaded model:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g8DNhOh30Gu",
        "outputId": "692ef708-7703-4041-a2b3-8b3a16f5dda2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and scaler saved.\n",
            "✅ Model and scaler loaded.\n",
            "Accuracy with loaded model: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGKqT6zt4WO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}